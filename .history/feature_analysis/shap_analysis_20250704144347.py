#!/usr/bin/env python3
"""
SHAPå¯è§†åŒ–è§£é‡Šåˆ†æ
ç”¨äºè§£é‡Šæ¨¡å‹å¯¹è¯¯æŠ¥æ ·æœ¬çš„é¢„æµ‹ç»“æœ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import shap
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SHAPAnalyzer:
    def __init__(self):
        self.white_data = None
        self.black_data = None
        self.misclassified_data = None
        self.feature_names = None
        self.model = None
        self.scaler = None
        self.explainer = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        try:
            # åŠ è½½æ•°æ®
            self.white_data = pd.read_excel('../data/good250623_features.xlsx')
            self.black_data = pd.read_excel('../data/bad250623_features.xlsx')
            self.misclassified_data = pd.read_excel('../data/good2bad_features.xlsx')
            
            # è·å–ç‰¹å¾åç§°
            self.feature_names = [col for col in self.white_data.columns if col != 'FILENAME']
            
            print(f"âœ… ç™½æ ·æœ¬: {self.white_data.shape}")
            print(f"âœ… é»‘æ ·æœ¬: {self.black_data.shape}")
            print(f"âœ… è¯¯æŠ¥æ ·æœ¬: {self.misclassified_data.shape}")
            print(f"âœ… ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("\nğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # æå–ç‰¹å¾
        white_features = self.white_data[self.feature_names].fillna(0)
        black_features = self.black_data[self.feature_names].fillna(0)
        
        # åˆå¹¶æ•°æ®
        X = pd.concat([white_features, black_features], ignore_index=True)
        y = np.concatenate([np.zeros(len(white_features)), np.ones(len(black_features))])
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å½¢çŠ¶: {X_scaled.shape}")
        print(f"âœ… æ ‡ç­¾åˆ†å¸ƒ: è‰¯æ€§={np.sum(y==0)}, æ¶æ„={np.sum(y==1)}")
        
        return X_scaled, y
    
    def train_model(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        print(f"âœ… è®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
        print(f"âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        joblib.dump(self.model, 'random_forest_model.pkl')
        joblib.dump(self.scaler, 'feature_scaler.pkl')
        print("âœ… æ¨¡å‹å·²ä¿å­˜")
        
        return X_train, X_test, y_train, y_test
    
    def create_shap_explainer(self, X_train):
        """åˆ›å»ºSHAPè§£é‡Šå™¨"""
        print("\nğŸ” åˆ›å»ºSHAPè§£é‡Šå™¨...")
        
        # ä½¿ç”¨TreeExplainer for RandomForest
        self.explainer = shap.TreeExplainer(self.model)
        
        # è®¡ç®—SHAPå€¼ (ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å­é›†ä»¥æé«˜é€Ÿåº¦)
        sample_size = min(1000, len(X_train))
        sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
        X_sample = X_train[sample_indices]
        
        print(f"âœ… ä½¿ç”¨ {sample_size} ä¸ªæ ·æœ¬è®¡ç®—SHAPå€¼...")
        
        return X_sample
    
    def analyze_all_samples(self):
        """åˆ†æè¯¯æŠ¥æ ·æœ¬ã€ç™½æ ·æœ¬å’Œé»‘æ ·æœ¬çš„SHAPå€¼è¿›è¡Œå¯¹æ¯”"""
        print("\nğŸ¯ åˆ†ææ‰€æœ‰æ ·æœ¬ç±»å‹...")

        # 1. å‡†å¤‡è¯¯æŠ¥æ ·æœ¬æ•°æ®
        misc_features = self.misclassified_data[self.feature_names].fillna(0)
        misc_scaled = self.scaler.transform(misc_features)

        # 2. å‡†å¤‡ç™½æ ·æœ¬æ•°æ®ï¼ˆéšæœºé‡‡æ ·ï¼Œé¿å…è®¡ç®—é‡è¿‡å¤§ï¼‰
        white_features = self.white_data[self.feature_names].fillna(0)
        white_sample_size = min(200, len(white_features))  # æœ€å¤š200ä¸ªæ ·æœ¬
        white_sample_idx = np.random.choice(len(white_features), white_sample_size, replace=False)
        white_sample = white_features.iloc[white_sample_idx]
        white_scaled = self.scaler.transform(white_sample)

        # 3. å‡†å¤‡é»‘æ ·æœ¬æ•°æ®ï¼ˆéšæœºé‡‡æ ·ï¼‰
        black_features = self.black_data[self.feature_names].fillna(0)
        black_sample_size = min(200, len(black_features))  # æœ€å¤š200ä¸ªæ ·æœ¬
        black_sample_idx = np.random.choice(len(black_features), black_sample_size, replace=False)
        black_sample = black_features.iloc[black_sample_idx]
        black_scaled = self.scaler.transform(black_sample)

        print(f"âœ… æ ·æœ¬æ•°é‡:")
        print(f"  - è¯¯æŠ¥æ ·æœ¬: {len(misc_scaled)}")
        print(f"  - ç™½æ ·æœ¬: {len(white_scaled)}")
        print(f"  - é»‘æ ·æœ¬: {len(black_scaled)}")

        # é¢„æµ‹æ‰€æœ‰æ ·æœ¬
        misc_pred = self.model.predict(misc_scaled)
        misc_prob = self.model.predict_proba(misc_scaled)
        white_pred = self.model.predict(white_scaled)
        white_prob = self.model.predict_proba(white_scaled)
        black_pred = self.model.predict(black_scaled)
        black_prob = self.model.predict_proba(black_scaled)

        print(f"\nğŸ“Š é¢„æµ‹ç»“æœ:")
        print(f"  è¯¯æŠ¥æ ·æœ¬ - é¢„æµ‹ä¸ºæ¶æ„: {np.sum(misc_pred == 1)}/{len(misc_pred)} ({np.sum(misc_pred == 1)/len(misc_pred)*100:.1f}%)")
        print(f"  ç™½æ ·æœ¬   - é¢„æµ‹ä¸ºæ¶æ„: {np.sum(white_pred == 1)}/{len(white_pred)} ({np.sum(white_pred == 1)/len(white_pred)*100:.1f}%)")
        print(f"  é»‘æ ·æœ¬   - é¢„æµ‹ä¸ºæ¶æ„: {np.sum(black_pred == 1)}/{len(black_pred)} ({np.sum(black_pred == 1)/len(black_pred)*100:.1f}%)")

        # è®¡ç®—SHAPå€¼
        print("\nğŸ” è®¡ç®—SHAPå€¼...")
        print("  - è®¡ç®—è¯¯æŠ¥æ ·æœ¬SHAPå€¼...")
        misc_shap = self.explainer.shap_values(misc_scaled)
        print("  - è®¡ç®—ç™½æ ·æœ¬SHAPå€¼...")
        white_shap = self.explainer.shap_values(white_scaled)
        print("  - è®¡ç®—é»‘æ ·æœ¬SHAPå€¼...")
        black_shap = self.explainer.shap_values(black_scaled)

        # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œå–æ¶æ„ç±»çš„SHAPå€¼
        if isinstance(misc_shap, list):
            misc_shap_malicious = misc_shap[1]
            white_shap_malicious = white_shap[1]
            black_shap_malicious = black_shap[1]
        else:
            misc_shap_malicious = misc_shap
            white_shap_malicious = white_shap
            black_shap_malicious = black_shap

        return {
            'misc': {'scaled': misc_scaled, 'shap': misc_shap_malicious, 'pred': misc_pred, 'prob': misc_prob},
            'white': {'scaled': white_scaled, 'shap': white_shap_malicious, 'pred': white_pred, 'prob': white_prob},
            'black': {'scaled': black_scaled, 'shap': black_shap_malicious, 'pred': black_pred, 'prob': black_prob}
        }
    
    def plot_shap_comparison(self, all_results):
        """ç»˜åˆ¶SHAPå¯¹æ¯”åˆ†æå›¾"""
        print("\nğŸ“ˆ ç»˜åˆ¶SHAPå¯¹æ¯”åˆ†æå›¾...")

        misc_data = all_results['misc']
        white_data = all_results['white']
        black_data = all_results['black']

        # 1. è¯¯æŠ¥æ ·æœ¬SHAPæ‘˜è¦å›¾
        plt.figure(figsize=(12, 8))
        misc_df = pd.DataFrame(misc_data['scaled'], columns=self.feature_names)
        shap.summary_plot(misc_data['shap'], misc_df, max_display=20, show=False)
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ‘˜è¦ - è¯¯æŠ¥æ ·æœ¬', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_summary_misclassified.png', dpi=300, bbox_inches='tight')
        print("âœ… è¯¯æŠ¥æ ·æœ¬SHAPæ‘˜è¦å›¾å·²ä¿å­˜åˆ°: shap_summary_misclassified.png")
        plt.show()

        # 2. ç™½æ ·æœ¬SHAPæ‘˜è¦å›¾
        plt.figure(figsize=(12, 8))
        white_df = pd.DataFrame(white_data['scaled'], columns=self.feature_names)
        shap.summary_plot(white_data['shap'], white_df, max_display=20, show=False)
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ‘˜è¦ - ç™½æ ·æœ¬', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_summary_white.png', dpi=300, bbox_inches='tight')
        print("âœ… ç™½æ ·æœ¬SHAPæ‘˜è¦å›¾å·²ä¿å­˜åˆ°: shap_summary_white.png")
        plt.show()

        # 3. é»‘æ ·æœ¬SHAPæ‘˜è¦å›¾
        plt.figure(figsize=(12, 8))
        black_df = pd.DataFrame(black_data['scaled'], columns=self.feature_names)
        shap.summary_plot(black_data['shap'], black_df, max_display=20, show=False)
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ‘˜è¦ - é»‘æ ·æœ¬', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_summary_black.png', dpi=300, bbox_inches='tight')
        print("âœ… é»‘æ ·æœ¬SHAPæ‘˜è¦å›¾å·²ä¿å­˜åˆ°: shap_summary_black.png")
        plt.show()
    
    def plot_shap_waterfall(self, misc_scaled, shap_values, sample_idx=0):
        """ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„SHAPç€‘å¸ƒå›¾"""
        print(f"\nğŸŒŠ ç»˜åˆ¶æ ·æœ¬ {sample_idx} çš„SHAPç€‘å¸ƒå›¾...")
        
        # åˆ›å»ºç‰¹å¾åç§°DataFrame
        feature_df = pd.DataFrame(misc_scaled, columns=self.feature_names)
        
        # åˆ›å»ºExplanationå¯¹è±¡
        explanation = shap.Explanation(
            values=shap_values[sample_idx],
            base_values=self.explainer.expected_value[1] if isinstance(self.explainer.expected_value, list) else self.explainer.expected_value,
            data=feature_df.iloc[sample_idx].values,
            feature_names=self.feature_names
        )
        
        plt.figure(figsize=(12, 8))
        shap.waterfall_plot(explanation, max_display=15, show=False)
        plt.title(f'SHAPç€‘å¸ƒå›¾ - è¯¯æŠ¥æ ·æœ¬ {sample_idx}', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'shap_waterfall_sample_{sample_idx}.png', dpi=300, bbox_inches='tight')
        print(f"âœ… SHAPç€‘å¸ƒå›¾å·²ä¿å­˜åˆ°: shap_waterfall_sample_{sample_idx}.png")
        plt.show()
    
    def analyze_shap_comparison(self, all_results):
        """å¯¹æ¯”åˆ†æä¸åŒæ ·æœ¬ç±»å‹çš„SHAPå€¼"""
        print("\nğŸ” å¯¹æ¯”åˆ†æSHAPå€¼...")

        misc_shap = all_results['misc']['shap']
        white_shap = all_results['white']['shap']
        black_shap = all_results['black']['shap']

        # è®¡ç®—å„ç±»æ ·æœ¬çš„ç‰¹å¾é‡è¦æ€§
        misc_importance = np.abs(misc_shap).mean(axis=0)
        white_importance = np.abs(white_shap).mean(axis=0)
        black_importance = np.abs(black_shap).mean(axis=0)

        # è®¡ç®—å¹³å‡SHAPå€¼ï¼ˆå¸¦ç¬¦å·ï¼‰
        misc_mean_shap = misc_shap.mean(axis=0)
        white_mean_shap = white_shap.mean(axis=0)
        black_mean_shap = black_shap.mean(axis=0)

        # åˆ›å»ºå¯¹æ¯”DataFrame
        comparison_df = pd.DataFrame({
            'feature': self.feature_names,
            'misc_importance': misc_importance,
            'white_importance': white_importance,
            'black_importance': black_importance,
            'misc_mean_shap': misc_mean_shap,
            'white_mean_shap': white_mean_shap,
            'black_mean_shap': black_mean_shap,
        })

        # è®¡ç®—è¯¯æŠ¥æ ·æœ¬ä¸é»‘æ ·æœ¬çš„ç›¸ä¼¼åº¦
        comparison_df['misc_black_similarity'] = 1 - np.abs(comparison_df['misc_mean_shap'] - comparison_df['black_mean_shap']) / (np.abs(comparison_df['misc_mean_shap']) + np.abs(comparison_df['black_mean_shap']) + 1e-8)

        # æŒ‰è¯¯æŠ¥æ ·æœ¬é‡è¦æ€§æ’åº
        comparison_df = comparison_df.sort_values('misc_importance', ascending=False)

        # ä¿å­˜ç»“æœ
        comparison_df.to_excel('shap_comparison_analysis.xlsx', index=False)
        print("âœ… SHAPå¯¹æ¯”åˆ†æå·²ä¿å­˜åˆ°: shap_comparison_analysis.xlsx")

        # æ˜¾ç¤ºå…³é”®å‘ç°
        print("\nğŸ“‹ å…³é”®å‘ç°:")
        print("=" * 80)

        # 1. è¯¯æŠ¥æ ·æœ¬ä¸­æœ€é‡è¦çš„ç‰¹å¾
        print("ï¿½ è¯¯æŠ¥æ ·æœ¬ä¸­æœ€é‡è¦çš„å‰10ä¸ªç‰¹å¾:")
        for idx, row in comparison_df.head(10).iterrows():
            print(f"  {row['feature']:25} | è¯¯æŠ¥é‡è¦æ€§:{row['misc_importance']:.4f} | é»‘æ ·æœ¬é‡è¦æ€§:{row['black_importance']:.4f}")

        # 2. è¯¯æŠ¥æ ·æœ¬ä¸é»‘æ ·æœ¬SHAPå€¼ç›¸ä¼¼çš„ç‰¹å¾
        similar_features = comparison_df[comparison_df['misc_black_similarity'] > 0.7].head(10)
        print(f"\nğŸ¯ è¯¯æŠ¥æ ·æœ¬ä¸é»‘æ ·æœ¬SHAPå€¼ç›¸ä¼¼çš„ç‰¹å¾ (ç›¸ä¼¼åº¦>0.7):")
        for idx, row in similar_features.iterrows():
            print(f"  {row['feature']:25} | ç›¸ä¼¼åº¦:{row['misc_black_similarity']:.3f} | è¯¯æŠ¥SHAP:{row['misc_mean_shap']:.4f} | é»‘æ ·æœ¬SHAP:{row['black_mean_shap']:.4f}")

        # 3. å¯èƒ½å¯¼è‡´è¯¯æŠ¥çš„å…³é”®ç‰¹å¾
        problematic_features = comparison_df[
            (comparison_df['misc_importance'] > comparison_df['misc_importance'].quantile(0.8)) &
            (comparison_df['misc_mean_shap'] * comparison_df['black_mean_shap'] > 0) &  # åŒå·
            (np.abs(comparison_df['misc_mean_shap'] - comparison_df['black_mean_shap']) < 0.1)  # æ•°å€¼æ¥è¿‘
        ]

        print(f"\nâš ï¸  å¯èƒ½å¯¼è‡´è¯¯æŠ¥çš„å…³é”®ç‰¹å¾ (é«˜é‡è¦æ€§ä¸”ä¸é»‘æ ·æœ¬ç›¸ä¼¼):")
        for idx, row in problematic_features.head(10).iterrows():
            print(f"  {row['feature']:25} | è¯¯æŠ¥SHAP:{row['misc_mean_shap']:.4f} | é»‘æ ·æœ¬SHAP:{row['black_mean_shap']:.4f} | å·®å¼‚:{abs(row['misc_mean_shap']-row['black_mean_shap']):.4f}")

        return comparison_df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ SHAPå¯è§†åŒ–è§£é‡Šåˆ†æ")
    print("=" * 60)
    print("ğŸ“‹ åŠŸèƒ½: è§£é‡Šæ¨¡å‹å¯¹è¯¯æŠ¥æ ·æœ¬çš„é¢„æµ‹ç»“æœ")
    print("ğŸ¤– æ¨¡å‹: éšæœºæ£®æ—åˆ†ç±»å™¨")
    print()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = SHAPAnalyzer()
    
    # åŠ è½½æ•°æ®
    if not analyzer.load_data():
        return
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y = analyzer.prepare_training_data()
    
    # è®­ç»ƒæ¨¡å‹
    X_train, X_test, y_train, y_test = analyzer.train_model(X, y)
    
    # åˆ›å»ºSHAPè§£é‡Šå™¨
    X_sample = analyzer.create_shap_explainer(X_train)
    
    # åˆ†æè¯¯æŠ¥æ ·æœ¬
    misc_scaled, shap_values, predictions, probabilities = analyzer.analyze_misclassified_samples()
    
    # ç»˜åˆ¶SHAPå›¾è¡¨
    analyzer.plot_shap_summary(misc_scaled, shap_values)
    
    # ç»˜åˆ¶ç€‘å¸ƒå›¾ (å‰3ä¸ªæ ·æœ¬)
    for i in range(min(3, len(misc_scaled))):
        analyzer.plot_shap_waterfall(misc_scaled, shap_values, i)
    
    # åˆ†ææœ€é‡è¦çš„ç‰¹å¾
    importance_df = analyzer.analyze_top_features(shap_values)
    
    print("\nğŸ‰ SHAPåˆ†æå®Œæˆ!")
    print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - random_forest_model.pkl: è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("  - feature_scaler.pkl: ç‰¹å¾æ ‡å‡†åŒ–å™¨")
    print("  - shap_feature_importance.xlsx: SHAPç‰¹å¾é‡è¦æ€§")
    print("  - shap_summary_plot.png: SHAPæ‘˜è¦å›¾")
    print("  - shap_bar_plot.png: SHAPæ¡å½¢å›¾")
    print("  - shap_waterfall_sample_*.png: SHAPç€‘å¸ƒå›¾")

if __name__ == "__main__":
    main()
