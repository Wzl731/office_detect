#!/usr/bin/env python3
"""
ç¬¬ä¸€æ­¥ï¼šæ•°æ®æ¢ç´¢å’ŒåŸºç¡€ç»Ÿè®¡åˆ†æ
åˆ†æé»‘ã€ç™½ã€è¯¯æŠ¥æ ·æœ¬çš„åŸºæœ¬ç‰¹å¾åˆ†å¸ƒ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class DataExplorer:
    def __init__(self):
        self.white_data = None
        self.black_data = None
        self.misc_data = None
        self.feature_names = None
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        
        self.white_data = pd.read_excel('../data/good250623_features.xlsx')
        self.black_data = pd.read_excel('../data/bad250623_features.xlsx')
        self.misc_data = pd.read_excel('../data/good2bad_features.xlsx')
        
        self.feature_names = [col for col in self.white_data.columns if col != 'FILENAME']
        
        print(f"âœ… ç™½æ ·æœ¬: {self.white_data.shape}")
        print(f"âœ… é»‘æ ·æœ¬: {self.black_data.shape}")
        print(f"âœ… è¯¯æŠ¥æ ·æœ¬: {self.misc_data.shape}")
        print(f"âœ… ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
        
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“ˆ åŸºç¡€ç»Ÿè®¡åˆ†æ...")
        
        stats_summary = []
        
        for feature in self.feature_names:
            white_vals = self.white_data[feature].fillna(0)
            black_vals = self.black_data[feature].fillna(0)
            misc_vals = self.misc_data[feature].fillna(0)
            
            stats_summary.append({
                'feature': feature,
                'white_mean': white_vals.mean(),
                'white_std': white_vals.std(),
                'white_median': white_vals.median(),
                'black_mean': black_vals.mean(),
                'black_std': black_vals.std(),
                'black_median': black_vals.median(),
                'misc_mean': misc_vals.mean(),
                'misc_std': misc_vals.std(),
                'misc_median': misc_vals.median(),
                'white_nonzero_ratio': (white_vals > 0).mean(),
                'black_nonzero_ratio': (black_vals > 0).mean(),
                'misc_nonzero_ratio': (misc_vals > 0).mean(),
            })
        
        stats_df = pd.DataFrame(stats_summary)
        stats_df.to_excel('data_statistics_summary.xlsx', index=False)
        print("âœ… åŸºç¡€ç»Ÿè®¡å·²ä¿å­˜åˆ°: data_statistics_summary.xlsx")
        
        return stats_df
    
    def feature_distribution_analysis(self):
        """ç‰¹å¾åˆ†å¸ƒåˆ†æ"""
        print("\nğŸ“Š ç‰¹å¾åˆ†å¸ƒåˆ†æ...")
        
        # é€‰æ‹©æœ€æœ‰åŒºåˆ†åº¦çš„ç‰¹å¾è¿›è¡Œå¯è§†åŒ–
        discriminative_features = []
        
        for feature in self.feature_names:
            white_vals = self.white_data[feature].fillna(0)
            black_vals = self.black_data[feature].fillna(0)
            misc_vals = self.misc_data[feature].fillna(0)
            
            # è®¡ç®—æ–¹å·®æ¯”å’Œå‡å€¼å·®å¼‚
            if white_vals.std() > 0 and black_vals.std() > 0:
                mean_diff = abs(white_vals.mean() - black_vals.mean())
                pooled_std = np.sqrt((white_vals.var() + black_vals.var()) / 2)
                effect_size = mean_diff / (pooled_std + 1e-8)
                
                discriminative_features.append({
                    'feature': feature,
                    'effect_size': effect_size,
                    'misc_closer_to_black': abs(misc_vals.mean() - black_vals.mean()) < abs(misc_vals.mean() - white_vals.mean())
                })
        
        # æ’åºå¹¶é€‰æ‹©å‰12ä¸ªæœ€æœ‰åŒºåˆ†åº¦çš„ç‰¹å¾
        discriminative_df = pd.DataFrame(discriminative_features)
        discriminative_df = discriminative_df.sort_values('effect_size', ascending=False)
        top_features = discriminative_df.head(12)['feature'].tolist()
        
        # ç»˜åˆ¶åˆ†å¸ƒå›¾
        fig, axes = plt.subplots(3, 4, figsize=(20, 15))
        axes = axes.flatten()
        
        for i, feature in enumerate(top_features):
            ax = axes[i]
            
            white_vals = self.white_data[feature].fillna(0)
            black_vals = self.black_data[feature].fillna(0)
            misc_vals = self.misc_data[feature].fillna(0)
            
            # ç»˜åˆ¶ç›´æ–¹å›¾ - ä½¿ç”¨é¢‘æ¬¡è€Œä¸æ˜¯å¯†åº¦ï¼Œä¾¿äºç†è§£
            # ä¸ºäº†ä¾¿äºæ¯”è¾ƒï¼Œå¯¹æ ·æœ¬æ•°é‡è¿›è¡Œå½’ä¸€åŒ–æ˜¾ç¤º
            white_weights = np.ones_like(white_vals) / len(white_vals) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            black_weights = np.ones_like(black_vals) / len(black_vals) * 100
            misc_weights = np.ones_like(misc_vals) / len(misc_vals) * 100

            ax.hist(white_vals, bins=30, alpha=0.6, label=f'ç™½æ ·æœ¬(n={len(white_vals)})',
                   color='blue', weights=white_weights)
            ax.hist(black_vals, bins=30, alpha=0.6, label=f'é»‘æ ·æœ¬(n={len(black_vals)})',
                   color='red', weights=black_weights)
            ax.hist(misc_vals, bins=20, alpha=0.8, label=f'è¯¯æŠ¥æ ·æœ¬(n={len(misc_vals)})',
                   color='orange', weights=misc_weights)

            ax.set_title(f'{feature}')
            ax.set_xlabel('ç‰¹å¾å€¼')
            ax.set_ylabel('ç™¾åˆ†æ¯” (%)')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.suptitle('å…³é”®ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('feature_distributions_detailed.png', dpi=300, bbox_inches='tight')
        print("âœ… ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: feature_distributions_detailed.png")
        plt.show()
        
        return discriminative_df
    
    def correlation_analysis(self):
        """ç‰¹å¾ç›¸å…³æ€§åˆ†æ"""
        print("\nğŸ”— ç‰¹å¾ç›¸å…³æ€§åˆ†æ...")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®è¿›è¡Œç›¸å…³æ€§åˆ†æ
        all_data = pd.concat([
            self.white_data[self.feature_names].assign(sample_type='white'),
            self.black_data[self.feature_names].assign(sample_type='black'),
            self.misc_data[self.feature_names].assign(sample_type='misc')
        ], ignore_index=True).fillna(0)
        
        # è®¡ç®—ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ
        feature_corr = all_data[self.feature_names].corr()
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
        high_corr_pairs = []
        for i in range(len(feature_corr.columns)):
            for j in range(i+1, len(feature_corr.columns)):
                corr_val = feature_corr.iloc[i, j]
                if abs(corr_val) > 0.7:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                    high_corr_pairs.append({
                        'feature1': feature_corr.columns[i],
                        'feature2': feature_corr.columns[j],
                        'correlation': corr_val
                    })
        
        if high_corr_pairs:
            high_corr_df = pd.DataFrame(high_corr_pairs)
            high_corr_df = high_corr_df.sort_values('correlation', key=abs, ascending=False)
            high_corr_df.to_excel('high_correlation_features.xlsx', index=False)
            print(f"âœ… å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾ï¼Œå·²ä¿å­˜åˆ°: high_correlation_features.xlsx")
            
            # æ˜¾ç¤ºå‰10å¯¹
            print("\nğŸ”— é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r| > 0.7):")
            for _, row in high_corr_df.head(10).iterrows():
                print(f"  {row['feature1']} <-> {row['feature2']}: r = {row['correlation']:.3f}")
        else:
            print("âœ… æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")
        
        return feature_corr, high_corr_pairs
    
    def missing_value_analysis(self):
        """ç¼ºå¤±å€¼åˆ†æ"""
        print("\nâ“ ç¼ºå¤±å€¼åˆ†æ...")
        
        missing_summary = []
        
        for feature in self.feature_names:
            white_missing = self.white_data[feature].isna().sum()
            black_missing = self.black_data[feature].isna().sum()
            misc_missing = self.misc_data[feature].isna().sum()
            
            if white_missing > 0 or black_missing > 0 or misc_missing > 0:
                missing_summary.append({
                    'feature': feature,
                    'white_missing': white_missing,
                    'white_missing_pct': white_missing / len(self.white_data) * 100,
                    'black_missing': black_missing,
                    'black_missing_pct': black_missing / len(self.black_data) * 100,
                    'misc_missing': misc_missing,
                    'misc_missing_pct': misc_missing / len(self.misc_data) * 100,
                })
        
        if missing_summary:
            missing_df = pd.DataFrame(missing_summary)
            missing_df.to_excel('missing_values_analysis.xlsx', index=False)
            print(f"âœ… å‘ç° {len(missing_summary)} ä¸ªç‰¹å¾æœ‰ç¼ºå¤±å€¼ï¼Œå·²ä¿å­˜åˆ°: missing_values_analysis.xlsx")
        else:
            print("âœ… æ‰€æœ‰ç‰¹å¾éƒ½æ²¡æœ‰ç¼ºå¤±å€¼")
        
        return missing_summary

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ•°æ®æ¢ç´¢å’ŒåŸºç¡€ç»Ÿè®¡åˆ†æ")
    print("=" * 60)
    
    explorer = DataExplorer()
    
    # 1. åŠ è½½æ•°æ®
    explorer.load_data()
    
    # 2. åŸºç¡€ç»Ÿè®¡åˆ†æ
    stats_df = explorer.basic_statistics()
    
    # 3. ç‰¹å¾åˆ†å¸ƒåˆ†æ
    discriminative_df = explorer.feature_distribution_analysis()
    
    # 4. ç›¸å…³æ€§åˆ†æ
    feature_corr, high_corr_pairs = explorer.correlation_analysis()
    
    # 5. ç¼ºå¤±å€¼åˆ†æ
    missing_summary = explorer.missing_value_analysis()
    
    print("\nğŸ‰ æ•°æ®æ¢ç´¢å®Œæˆ!")
    print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - data_statistics_summary.xlsx: åŸºç¡€ç»Ÿè®¡æ‘˜è¦")
    print("  - feature_distributions_detailed.png: ç‰¹å¾åˆ†å¸ƒå›¾")
    print("  - high_correlation_features.xlsx: é«˜ç›¸å…³æ€§ç‰¹å¾")
    print("  - missing_values_analysis.xlsx: ç¼ºå¤±å€¼åˆ†æ")

if __name__ == "__main__":
    main()
