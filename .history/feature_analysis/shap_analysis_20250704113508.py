#!/usr/bin/env python3
"""
SHAPå¯è§†åŒ–è§£é‡Šåˆ†æ
ç”¨äºè§£é‡Šæ¨¡å‹å¯¹è¯¯æŠ¥æ ·æœ¬çš„é¢„æµ‹ç»“æœ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import shap
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SHAPAnalyzer:
    def __init__(self):
        self.white_data = None
        self.black_data = None
        self.misclassified_data = None
        self.feature_names = None
        self.model = None
        self.scaler = None
        self.explainer = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        try:
            # åŠ è½½æ•°æ®
            self.white_data = pd.read_excel('../data/good250623_features.xlsx')
            self.black_data = pd.read_excel('../data/bad250623_features.xlsx')
            self.misclassified_data = pd.read_excel('../data/good2bad_features.xlsx')
            
            # è·å–ç‰¹å¾åç§°
            self.feature_names = [col for col in self.white_data.columns if col != 'FILENAME']
            
            print(f"âœ… ç™½æ ·æœ¬: {self.white_data.shape}")
            print(f"âœ… é»‘æ ·æœ¬: {self.black_data.shape}")
            print(f"âœ… è¯¯æŠ¥æ ·æœ¬: {self.misclassified_data.shape}")
            print(f"âœ… ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("\nğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # æå–ç‰¹å¾
        white_features = self.white_data[self.feature_names].fillna(0)
        black_features = self.black_data[self.feature_names].fillna(0)
        
        # åˆå¹¶æ•°æ®
        X = pd.concat([white_features, black_features], ignore_index=True)
        y = np.concatenate([np.zeros(len(white_features)), np.ones(len(black_features))])
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å½¢çŠ¶: {X_scaled.shape}")
        print(f"âœ… æ ‡ç­¾åˆ†å¸ƒ: è‰¯æ€§={np.sum(y==0)}, æ¶æ„={np.sum(y==1)}")
        
        return X_scaled, y
    
    def train_model(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        print(f"âœ… è®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
        print(f"âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        joblib.dump(self.model, 'random_forest_model.pkl')
        joblib.dump(self.scaler, 'feature_scaler.pkl')
        print("âœ… æ¨¡å‹å·²ä¿å­˜")
        
        return X_train, X_test, y_train, y_test
    
    def create_shap_explainer(self, X_train):
        """åˆ›å»ºSHAPè§£é‡Šå™¨"""
        print("\nğŸ” åˆ›å»ºSHAPè§£é‡Šå™¨...")
        
        # ä½¿ç”¨TreeExplainer for RandomForest
        self.explainer = shap.TreeExplainer(self.model)
        
        # è®¡ç®—SHAPå€¼ (ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å­é›†ä»¥æé«˜é€Ÿåº¦)
        sample_size = min(1000, len(X_train))
        sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
        X_sample = X_train[sample_indices]
        
        print(f"âœ… ä½¿ç”¨ {sample_size} ä¸ªæ ·æœ¬è®¡ç®—SHAPå€¼...")
        
        return X_sample
    
    def analyze_misclassified_samples(self):
        """åˆ†æè¯¯æŠ¥æ ·æœ¬çš„SHAPå€¼"""
        print("\nğŸ¯ åˆ†æè¯¯æŠ¥æ ·æœ¬...")
        
        # å‡†å¤‡è¯¯æŠ¥æ ·æœ¬æ•°æ®
        misc_features = self.misclassified_data[self.feature_names].fillna(0)
        misc_scaled = self.scaler.transform(misc_features)
        
        # é¢„æµ‹è¯¯æŠ¥æ ·æœ¬
        predictions = self.model.predict(misc_scaled)
        probabilities = self.model.predict_proba(misc_scaled)
        
        print(f"âœ… è¯¯æŠ¥æ ·æœ¬é¢„æµ‹ç»“æœ:")
        print(f"  - é¢„æµ‹ä¸ºæ¶æ„: {np.sum(predictions == 1)}")
        print(f"  - é¢„æµ‹ä¸ºè‰¯æ€§: {np.sum(predictions == 0)}")
        print(f"  - å¹³å‡æ¶æ„æ¦‚ç‡: {probabilities[:, 1].mean():.4f}")
        
        # è®¡ç®—è¯¯æŠ¥æ ·æœ¬çš„SHAPå€¼
        print("ğŸ” è®¡ç®—è¯¯æŠ¥æ ·æœ¬SHAPå€¼...")
        shap_values = self.explainer.shap_values(misc_scaled)
        
        # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œå–æ¶æ„ç±»çš„SHAPå€¼
        if isinstance(shap_values, list):
            shap_values_malicious = shap_values[1]  # æ¶æ„ç±»
        else:
            shap_values_malicious = shap_values
        
        return misc_scaled, shap_values_malicious, predictions, probabilities
    
    def plot_shap_summary(self, misc_scaled, shap_values):
        """ç»˜åˆ¶SHAPæ‘˜è¦å›¾"""
        print("\nğŸ“ˆ ç»˜åˆ¶SHAPæ‘˜è¦å›¾...")
        
        # åˆ›å»ºç‰¹å¾åç§°DataFrame
        feature_df = pd.DataFrame(misc_scaled, columns=self.feature_names)
        
        # 1. SHAPæ‘˜è¦å›¾
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, feature_df, max_display=20, show=False)
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ‘˜è¦ (è¯¯æŠ¥æ ·æœ¬)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_summary_plot.png', dpi=300, bbox_inches='tight')
        print("âœ… SHAPæ‘˜è¦å›¾å·²ä¿å­˜åˆ°: shap_summary_plot.png")
        plt.show()
        
        # 2. SHAPæ¡å½¢å›¾
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, feature_df, plot_type="bar", max_display=20, show=False)
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾ (è¯¯æŠ¥æ ·æœ¬)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_bar_plot.png', dpi=300, bbox_inches='tight')
        print("âœ… SHAPæ¡å½¢å›¾å·²ä¿å­˜åˆ°: shap_bar_plot.png")
        plt.show()
    
    def plot_shap_waterfall(self, misc_scaled, shap_values, sample_idx=0):
        """ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„SHAPç€‘å¸ƒå›¾"""
        print(f"\nğŸŒŠ ç»˜åˆ¶æ ·æœ¬ {sample_idx} çš„SHAPç€‘å¸ƒå›¾...")
        
        # åˆ›å»ºç‰¹å¾åç§°DataFrame
        feature_df = pd.DataFrame(misc_scaled, columns=self.feature_names)
        
        # åˆ›å»ºExplanationå¯¹è±¡
        explanation = shap.Explanation(
            values=shap_values[sample_idx],
            base_values=self.explainer.expected_value[1] if isinstance(self.explainer.expected_value, list) else self.explainer.expected_value,
            data=feature_df.iloc[sample_idx].values,
            feature_names=self.feature_names
        )
        
        plt.figure(figsize=(12, 8))
        shap.waterfall_plot(explanation, max_display=15, show=False)
        plt.title(f'SHAPç€‘å¸ƒå›¾ - è¯¯æŠ¥æ ·æœ¬ {sample_idx}', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'shap_waterfall_sample_{sample_idx}.png', dpi=300, bbox_inches='tight')
        print(f"âœ… SHAPç€‘å¸ƒå›¾å·²ä¿å­˜åˆ°: shap_waterfall_sample_{sample_idx}.png")
        plt.show()
    
    def analyze_top_features(self, shap_values):
        """åˆ†ææœ€é‡è¦çš„ç‰¹å¾"""
        print("\nğŸ” åˆ†ææœ€é‡è¦çš„ç‰¹å¾...")
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        feature_importance = np.abs(shap_values).mean(axis=0)
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        # ä¿å­˜ç»“æœ
        importance_df.to_excel('shap_feature_importance.xlsx', index=False)
        print("âœ… SHAPç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ°: shap_feature_importance.xlsx")
        
        # æ˜¾ç¤ºå‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        print("\nğŸ“‹ SHAPé‡è¦æ€§æœ€é«˜çš„å‰20ä¸ªç‰¹å¾:")
        print("-" * 50)
        for i, row in importance_df.head(20).iterrows():
            print(f"{row['feature']:25} | {row['importance']:.6f}")
        
        return importance_df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ SHAPå¯è§†åŒ–è§£é‡Šåˆ†æ")
    print("=" * 60)
    print("ğŸ“‹ åŠŸèƒ½: è§£é‡Šæ¨¡å‹å¯¹è¯¯æŠ¥æ ·æœ¬çš„é¢„æµ‹ç»“æœ")
    print("ğŸ¤– æ¨¡å‹: éšæœºæ£®æ—åˆ†ç±»å™¨")
    print()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = SHAPAnalyzer()
    
    # åŠ è½½æ•°æ®
    if not analyzer.load_data():
        return
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y = analyzer.prepare_training_data()
    
    # è®­ç»ƒæ¨¡å‹
    X_train, X_test, y_train, y_test = analyzer.train_model(X, y)
    
    # åˆ›å»ºSHAPè§£é‡Šå™¨
    X_sample = analyzer.create_shap_explainer(X_train)
    
    # åˆ†æè¯¯æŠ¥æ ·æœ¬
    misc_scaled, shap_values, predictions, probabilities = analyzer.analyze_misclassified_samples()
    
    # ç»˜åˆ¶SHAPå›¾è¡¨
    analyzer.plot_shap_summary(misc_scaled, shap_values)
    
    # ç»˜åˆ¶ç€‘å¸ƒå›¾ (å‰3ä¸ªæ ·æœ¬)
    for i in range(min(3, len(misc_scaled))):
        analyzer.plot_shap_waterfall(misc_scaled, shap_values, i)
    
    # åˆ†ææœ€é‡è¦çš„ç‰¹å¾
    importance_df = analyzer.analyze_top_features(shap_values)
    
    print("\nğŸ‰ SHAPåˆ†æå®Œæˆ!")
    print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - random_forest_model.pkl: è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("  - feature_scaler.pkl: ç‰¹å¾æ ‡å‡†åŒ–å™¨")
    print("  - shap_feature_importance.xlsx: SHAPç‰¹å¾é‡è¦æ€§")
    print("  - shap_summary_plot.png: SHAPæ‘˜è¦å›¾")
    print("  - shap_bar_plot.png: SHAPæ¡å½¢å›¾")
    print("  - shap_waterfall_sample_*.png: SHAPç€‘å¸ƒå›¾")

if __name__ == "__main__":
    main()
