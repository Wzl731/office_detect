#!/usr/bin/env python3
"""
è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜
è®­ç»ƒåä¿å­˜åˆ°modelsæ–‡ä»¶å¤¹
"""

import numpy as np
import pandas as pd
import pickle
import os
import argparse
from pathlib import Path
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score



class VBAMalwareModelTrainer:
    def __init__(self, dataset_file='ds_date/combined_dataset.csv', models_dir=None):
        """åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨"""
        self.models = {}
        self.scalers = {}
        self.feature_columns = None
        self.dataset_file = dataset_file

        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ¨¡å‹ç›®å½•
        if models_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.models_dir = Path(f"models_{timestamp}")
        else:
            self.models_dir = Path(models_dir)

        # åŠ¨æ€åˆ†æçš„æ•°æ®é›†é…ç½®ï¼ˆå°†åœ¨load_datasetä¸­è®¾ç½®ï¼‰
        self.benign_samples_cnt = 0
        self.malicious_samples_cnt = 0
        self.total_samples_cnt = 0
        
        # æ¨¡å‹é…ç½® 
        # TODO ä¼˜åŒ–å‚æ•°ä»¥æé«˜æ€§èƒ½ 
        self.model_configs = {
            'RandomForest': {
                'model': RandomForestClassifier(
                    n_estimators=100,
                    random_state=42,
                    n_jobs=-1
                ),
                'need_scaling': False
            },
            'MLP': {
                'model': MLPClassifier(
                    hidden_layer_sizes=(150,),
                    max_iter=2000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                    alpha=1e-4,
                    learning_rate='adaptive',
                    early_stopping=True,  # æ—©åœ
                    validation_fraction=0.1,
                    random_state=42
                ),
                'need_scaling': True
            },
            'KNN': {
                'model': KNeighborsClassifier(
                    n_neighbors=3,
                    weights='distance',  # ä½¿ç”¨è·ç¦»æƒé‡
                    n_jobs=-1
                ),
                'need_scaling': True
            },
            'SVM': {
                'model': SVC(
                    kernel='rbf',  # ä½¿ç”¨RBFæ ¸
                    C=1.0,
                    gamma='scale',
                    random_state=42,
                    probability=True
                ),
                'need_scaling': True
            }
        }
    
    def load_dataset(self):
        """åŠ¨æ€åŠ è½½å’Œåˆ†ææ•°æ®é›†"""
        print("ğŸ“Š åŠ è½½æ•°æ®é›†...")

        # åŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒ CSV å’Œ Excel æ ¼å¼ï¼‰
        try:
            if self.dataset_file.endswith('.csv'):
                self.dataset = pd.read_csv(self.dataset_file)
                print(f"  âœ… CSVæ•°æ®é›†åŠ è½½æˆåŠŸ: {len(self.dataset)} æ¡è®°å½•")
            else:
                self.dataset = pd.read_excel(self.dataset_file)
                print(f"  âœ… Excelæ•°æ®é›†åŠ è½½æˆåŠŸ: {len(self.dataset)} æ¡è®°å½•")
        except Exception as e:
            print(f"  âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            return False

        # åŠ¨æ€åˆ†ææ•°æ®é›†ç»“æ„
        if 'label' not in self.dataset.columns:
            print("  âŒ æ•°æ®é›†ä¸­æœªæ‰¾åˆ°'label'åˆ—")
            return False

        # ç»Ÿè®¡æ ·æœ¬æ•°é‡
        label_counts = self.dataset['label'].value_counts()
        self.benign_samples_cnt = label_counts.get(0, 0)
        self.malicious_samples_cnt = label_counts.get(1, 0)
        self.total_samples_cnt = len(self.dataset)

        # è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ–‡ä»¶åå’Œæ ‡ç­¾åˆ—ï¼‰
        exclude_columns = ['label']
        if 'filename' in self.dataset.columns:
            exclude_columns.append('filename')
        elif self.dataset.columns[0].lower() in ['file', 'name', 'filename']:
            exclude_columns.append(self.dataset.columns[0])

        self.feature_columns = [col for col in self.dataset.columns if col not in exclude_columns]

        print(f"  ğŸ“‹ ç‰¹å¾ç»´åº¦: {len(self.feature_columns)}")
        print(f"  ğŸ“‹ è‰¯æ€§æ ·æœ¬: {self.benign_samples_cnt}")
        print(f"  ğŸ“‹ æ¶æ„æ ·æœ¬: {self.malicious_samples_cnt}")
        print(f"  ğŸ“‹ æ€»æ ·æœ¬æ•°: {self.total_samples_cnt}")

        # éªŒè¯æ•°æ®é›†
        if self.benign_samples_cnt == 0 or self.malicious_samples_cnt == 0:
            print("  âš ï¸  è­¦å‘Š: æ•°æ®é›†ä¸­ç¼ºå°‘è‰¯æ€§æˆ–æ¶æ„æ ·æœ¬")

        return True
    
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X = self.dataset[self.feature_columns].values
        y = self.dataset['label'].values

        print(f"  ğŸ“Š ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
        print(f"  ğŸ“Š æ ‡ç­¾æ•°ç»„å½¢çŠ¶: {y.shape}")

        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"  ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(self.X_train)}")
        print(f"  ğŸ“Š æµ‹è¯•é›†å¤§å°: {len(self.X_test)}")
        print(f"  ğŸ“Š è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: è‰¯æ€§={sum(self.y_train==0)}, æ¶æ„={sum(self.y_train==1)}")
        print(f"  ğŸ“Š æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: è‰¯æ€§={sum(self.y_test==0)}, æ¶æ„={sum(self.y_test==1)}")

        return True
    
    def train_models(self):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        for model_name, config in self.model_configs.items():
            print(f"\n   è®­ç»ƒ {model_name} æ¨¡å‹...")
            
            try:
                model = config['model']
                
                # å‡†å¤‡è®­ç»ƒæ•°æ®
                X_train = self.X_train.copy()
                X_test = self.X_test.copy()
                
                # å¦‚æœéœ€è¦æ ‡å‡†åŒ–
                if config['need_scaling']:
                    scaler = StandardScaler()
                    X_train = scaler.fit_transform(X_train)
                    X_test = scaler.transform(X_test)
                    self.scalers[model_name] = scaler
                    print(f"    âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
                
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, self.y_train)
                
                # è¯„ä¼°æ¨¡å‹
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(self.y_test, y_pred)
                
                print(f"    âœ… {model_name} è®­ç»ƒå®Œæˆ")
                print(f"    ğŸ“ˆ æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
                
                # ä¿å­˜æ¨¡å‹
                self.models[model_name] = model
                
            except Exception as e:
                print(f"    âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
    
    def save_models(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {self.models_dir}")

        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„modelsç›®å½•
        self.models_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for model_name, model in self.models.items():
            try:
                # ä¿å­˜æ¨¡å‹
                model_path = self.models_dir / f'{model_name.lower()}_model.pkl'
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                print(f"  âœ… {model_name} æ¨¡å‹å·²ä¿å­˜: {model_path}")

                # ä¿å­˜å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
                if model_name in self.scalers:
                    scaler_path = self.models_dir / f'{model_name.lower()}_scaler.pkl'
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scalers[model_name], f)
                    print(f"  âœ… {model_name} æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")

            except Exception as e:
                print(f"  âŒ ä¿å­˜ {model_name} å¤±è´¥: {e}")

        # ä¿å­˜ç‰¹å¾åˆ—å
        try:
            feature_path = self.models_dir / 'feature_columns.pkl'
            with open(feature_path, 'wb') as f:
                pickle.dump(self.feature_columns, f)
            print(f"  âœ… ç‰¹å¾åˆ—åå·²ä¿å­˜: {feature_path}")
        except Exception as e:
            print(f"  âŒ ä¿å­˜ç‰¹å¾åˆ—åå¤±è´¥: {e}")
    
    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„æ¨¡å‹è¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ¨¡å‹è¯„ä¼°æŠ¥å‘Š...")

        report_path = self.models_dir / 'model_evaluation_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("VBAæ¶æ„å®æ£€æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æ•°æ®é›†ä¿¡æ¯:\n")
            f.write(f"  - æ€»æ ·æœ¬æ•°: {len(self.dataset1)}\n")
            f.write(f"  - è‰¯æ€§æ ·æœ¬: {self.DS1_BENIGN_SAMPLES_CNT}\n")
            f.write(f"  - æ¶æ„æ ·æœ¬: {self.DS1_MAL_SAMPLES_CNT}\n")
            f.write(f"  - ç‰¹å¾ç»´åº¦: {len(self.feature_columns)}\n\n")
            
            for model_name, model in self.models.items():
                f.write(f"{model_name} æ¨¡å‹è¯„ä¼°:\n")
                f.write("-" * 30 + "\n")
                
                try:
                    # å‡†å¤‡æµ‹è¯•æ•°æ®
                    X_test = self.X_test.copy()
                    if model_name in self.scalers:
                        X_test = self.scalers[model_name].transform(X_test)
                    
                    # é¢„æµ‹
                    y_pred = model.predict(X_test)
                    accuracy = accuracy_score(self.y_test, y_pred)
                    
                    f.write(f"å‡†ç¡®ç‡: {accuracy:.4f}\n")
                    f.write(f"åˆ†ç±»æŠ¥å‘Š:\n{classification_report(self.y_test, y_pred)}\n")
                    f.write(f"æ··æ·†çŸ©é˜µ:\n{confusion_matrix(self.y_test, y_pred)}\n\n")
                    
                except Exception as e:
                    f.write(f"è¯„ä¼°å¤±è´¥: {e}\n\n")
        
        print(f"  âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("ğŸ¯ VBAæ¶æ„å®æ£€æµ‹æ¨¡å‹è®­ç»ƒå™¨")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®é›†
        if not self.load_dataset():
            return False
        
        # å‡†å¤‡æ•°æ®
        if not self.prepare_data():
            return False
        
        # è®­ç»ƒæ¨¡å‹
        self.train_models()
        
        # ä¿å­˜æ¨¡å‹
        self.save_models()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_detailed_report()
        
        print("\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: ./{self.models_dir}/")
        print(f"ğŸ“‹ æ¨¡å‹ç›®å½•: {self.models_dir.absolute()}")

        return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VBAæ¶æ„å®æ£€æµ‹æ¨¡å‹è®­ç»ƒå™¨')

    # æ·»åŠ å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # ç‰¹å¾æå–å‘½ä»¤
    extract_parser = subparsers.add_parser('extract', help='ä»åŸå§‹æ–‡ä»¶æå–ç‰¹å¾')
    extract_parser.add_argument('--benign-folder', '-b', required=True, help='è‰¯æ€§æ ·æœ¬æ–‡ä»¶å¤¹è·¯å¾„')
    extract_parser.add_argument('--malicious-folder', '-m', required=True, help='æ¶æ„æ ·æœ¬æ–‡ä»¶å¤¹è·¯å¾„')
    extract_parser.add_argument('--output', '-o', default='processed_dataset.xls', help='è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
    extract_parser.add_argument('--feature-extractor', '-f', default='feature222',
                               choices=['original', 'feature222', 'feature_11111'],
                               help='ç‰¹å¾æå–æ¨¡å—é€‰æ‹©')

    # è®­ç»ƒå‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒæ¨¡å‹')
    train_parser.add_argument('--dataset', '-d', default='ds_date/combined_dataset.csv', help='è®­ç»ƒæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
    train_parser.add_argument('--models-dir', default='models', help='æ¨¡å‹ä¿å­˜ç›®å½•')

    # å®Œæ•´æµç¨‹å‘½ä»¤
    full_parser = subparsers.add_parser('full', help='å®Œæ•´æµç¨‹ï¼šç‰¹å¾æå– + è®­ç»ƒ')
    full_parser.add_argument('--benign-folder', '-b', required=True, help='è‰¯æ€§æ ·æœ¬æ–‡ä»¶å¤¹è·¯å¾„')
    full_parser.add_argument('--malicious-folder', '-m', required=True, help='æ¶æ„æ ·æœ¬æ–‡ä»¶å¤¹è·¯å¾„')
    full_parser.add_argument('--feature-extractor', '-f', default='feature222',
                            choices=['original', 'feature222', 'feature_11111'],
                            help='ç‰¹å¾æå–æ¨¡å—é€‰æ‹©')
    full_parser.add_argument('--models-dir', default='models', help='æ¨¡å‹ä¿å­˜ç›®å½•')

    args = parser.parse_args()

    if args.command == 'extract':
        # ç‰¹å¾æå–
        print("ğŸ”„ å¼€å§‹ç‰¹å¾æå–...")
        processor = DatasetProcessor(args.feature_extractor)
        dataset = processor.process_datasets(args.benign_folder, args.malicious_folder, args.output)

        if dataset is not None:
            print("âœ… ç‰¹å¾æå–å®Œæˆï¼")
        else:
            print("âŒ ç‰¹å¾æå–å¤±è´¥ï¼")

    elif args.command == 'train':
        # æ¨¡å‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        trainer = VBAMalwareModelTrainer(args.dataset, args.models_dir)
        success = trainer.run()

        if success:
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        else:
            print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼")

    elif args.command == 'full':
        # å®Œæ•´æµç¨‹
        print("ğŸ¯ å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹...")

        # 1. ç‰¹å¾æå–
        print("\nğŸ“Š æ­¥éª¤1: ç‰¹å¾æå–")
        processor = DatasetProcessor(args.feature_extractor)
        dataset_file = 'temp_dataset.xls'
        dataset = processor.process_datasets(args.benign_folder, args.malicious_folder, dataset_file)

        if dataset is None:
            print("âŒ ç‰¹å¾æå–å¤±è´¥ï¼")
            return

        # 2. æ¨¡å‹è®­ç»ƒ
        print("\nğŸš€ æ­¥éª¤2: æ¨¡å‹è®­ç»ƒ")
        trainer = VBAMalwareModelTrainer(dataset_file)
        success = trainer.run()

        if success:
            print("âœ… å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
        else:
            print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼")

    else:
        parser.print_help()

if __name__ == "__main__":
    main()