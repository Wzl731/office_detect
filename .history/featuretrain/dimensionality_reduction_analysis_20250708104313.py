#!/usr/bin/env python3
"""
é™ç»´å¯è§†åŒ–åˆ†æè„šæœ¬
ä½¿ç”¨PCAå’Œt-SNEå¯¹train.csvå’Œdataæ–‡ä»¶å¤¹æ•°æ®è¿›è¡Œé™ç»´å¯è§†åŒ–
å½©è‰²æ ‡æ³¨ä¸åŒæ•°æ®é›†ï¼Œç›´è§‚å±•ç¤ºæ•°æ®åˆ†å¸ƒå’Œèšç±»æƒ…å†µ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from pathlib import Path
import warnings
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

class DimensionalityReductionAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–é™ç»´åˆ†æå™¨"""
        self.output_dir = Path('featuretrain')
        self.output_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.train_white = None
        self.train_black = None
        self.good250623 = None
        self.bad250623 = None
        self.feature_columns = None
        
        # é™ç»´ç»“æœå­˜å‚¨
        self.pca_results = {}
        self.tsne_results = {}
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("ğŸ“Š åŠ è½½æ•°æ®è¿›è¡Œé™ç»´åˆ†æ...")
        
        try:
            # åŠ è½½train.csv
            train_data = pd.read_csv('train.csv')
            self.train_white = train_data.iloc[:2939].copy()
            self.train_black = train_data.iloc[2939:].copy()
            
            # åŠ è½½dataæ–‡ä»¶å¤¹æ•°æ®
            self.good250623 = pd.read_csv('data/good250623_features.csv')
            self.bad250623 = pd.read_csv('data/bad250623_features.csv')
            
            # è·å–ç‰¹å¾åˆ—
            self.feature_columns = train_data.columns[1:].tolist()
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
            print(f"   - trainç™½æ ·æœ¬: {len(self.train_white)}")
            print(f"   - trainé»‘æ ·æœ¬: {len(self.train_black)}")
            print(f"   - good250623: {len(self.good250623)}")
            print(f"   - bad250623: {len(self.bad250623)}")
            print(f"   - ç‰¹å¾ç»´åº¦: {len(self.feature_columns)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def prepare_combined_data(self):
        """å‡†å¤‡åˆå¹¶æ•°æ®ç”¨äºé™ç»´åˆ†æ"""
        print(f"\nğŸ”§ å‡†å¤‡é™ç»´æ•°æ® (ä½¿ç”¨å…¨éƒ¨æ•°æ®)...")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸è¿›è¡Œé‡‡æ ·
        train_white_sample = self.train_white
        train_black_sample = self.train_black
        good250623_sample = self.good250623
        bad250623_sample = self.bad250623
        
        # æå–ç‰¹å¾æ•°æ®
        train_white_features = train_white_sample[self.feature_columns].values
        train_black_features = train_black_sample[self.feature_columns].values
        good250623_features = good250623_sample[self.feature_columns].values
        bad250623_features = bad250623_sample[self.feature_columns].values
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_features = np.vstack([
            train_white_features,
            train_black_features,
            good250623_features,
            bad250623_features
        ])
        
        # åˆ›å»ºæ ‡ç­¾
        labels = (
            ['train_white'] * len(train_white_features) +
            ['train_black'] * len(train_black_features) +
            ['good250623'] * len(good250623_features) +
            ['bad250623'] * len(bad250623_features)
        )
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„
        color_map = {
            'train_white': '#1f77b4',    # è“è‰²
            'train_black': '#ff7f0e',    # æ©™è‰²
            'good250623': '#2ca02c',     # ç»¿è‰²
            'bad250623': '#d62728'       # çº¢è‰²
        }
        
        colors = [color_map[label] for label in labels]
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ (å…¨éƒ¨æ•°æ®):")
        print(f"   - æ€»æ ·æœ¬æ•°: {len(all_features)}")
        print(f"   - train_white: {len(train_white_features)}")
        print(f"   - train_black: {len(train_black_features)}")
        print(f"   - good250623: {len(good250623_features)}")
        print(f"   - bad250623: {len(bad250623_features)}")
        
        return all_features, labels, colors, color_map
    
    def perform_pca_analysis(self, features, labels, colors, color_map):
        """æ‰§è¡ŒPCAé™ç»´åˆ†æ"""
        print("\nğŸ” æ‰§è¡ŒPCAé™ç»´åˆ†æ...")
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # PCAé™ç»´åˆ°2D
        pca_2d = PCA(n_components=2, random_state=42)
        pca_2d_result = pca_2d.fit_transform(features_scaled)
        
        # PCAé™ç»´åˆ°3D
        pca_3d = PCA(n_components=3, random_state=42)
        pca_3d_result = pca_3d.fit_transform(features_scaled)
        
        # ä¿å­˜ç»“æœ
        self.pca_results = {
            '2d': pca_2d_result,
            '3d': pca_3d_result,
            'explained_variance_2d': pca_2d.explained_variance_ratio_,
            'explained_variance_3d': pca_3d.explained_variance_ratio_,
            'labels': labels,
            'colors': colors
        }
        
        print(f"âœ… PCAåˆ†æå®Œæˆ:")
        print(f"   - PC1è§£é‡Šæ–¹å·®: {pca_2d.explained_variance_ratio_[0]:.3f}")
        print(f"   - PC2è§£é‡Šæ–¹å·®: {pca_2d.explained_variance_ratio_[1]:.3f}")
        print(f"   - æ€»è§£é‡Šæ–¹å·®: {sum(pca_2d.explained_variance_ratio_):.3f}")
        
        return pca_2d_result, pca_3d_result
    
    def perform_tsne_analysis(self, features, labels, colors):
        """æ‰§è¡Œt-SNEé™ç»´åˆ†æ"""
        print("\nğŸ” æ‰§è¡Œt-SNEé™ç»´åˆ†æ...")
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # t-SNEé™ç»´åˆ°2D
        tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
        tsne_2d_result = tsne_2d.fit_transform(features_scaled)
        
        # ä¿å­˜ç»“æœ
        self.tsne_results = {
            '2d': tsne_2d_result,
            'labels': labels,
            'colors': colors
        }
        
        print(f"âœ… t-SNEåˆ†æå®Œæˆ")
        
        return tsne_2d_result
    
    def create_pca_visualizations(self, color_map):
        """åˆ›å»ºPCAå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“ˆ ç”ŸæˆPCAå¯è§†åŒ–å›¾è¡¨...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. PCA 2Dæ•£ç‚¹å›¾
        pca_2d = self.pca_results['2d']
        labels = self.pca_results['labels']
        
        for dataset in color_map.keys():
            mask = np.array(labels) == dataset
            axes[0,0].scatter(pca_2d[mask, 0], pca_2d[mask, 1], 
                            c=color_map[dataset], label=dataset, alpha=0.6, s=20)
        
        axes[0,0].set_xlabel(f'PC1 ({self.pca_results["explained_variance_2d"][0]:.1%} variance)')
        axes[0,0].set_ylabel(f'PC2 ({self.pca_results["explained_variance_2d"][1]:.1%} variance)')
        axes[0,0].set_title('PCA 2DæŠ•å½± - æ•°æ®é›†åˆ†å¸ƒå¯¹æ¯”')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. PCAè§£é‡Šæ–¹å·®å›¾
        pca_3d_var = self.pca_results['explained_variance_3d']
        cumsum_var = np.cumsum(pca_3d_var)
        
        axes[0,1].bar(range(1, len(pca_3d_var)+1), pca_3d_var, alpha=0.7, color='skyblue', label='å•ç‹¬è§£é‡Šæ–¹å·®')
        axes[0,1].plot(range(1, len(cumsum_var)+1), cumsum_var, 'ro-', label='ç´¯ç§¯è§£é‡Šæ–¹å·®')
        axes[0,1].set_xlabel('ä¸»æˆåˆ†')
        axes[0,1].set_ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        axes[0,1].set_title('PCAè§£é‡Šæ–¹å·®åˆ†æ')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ç™½æ ·æœ¬å¯¹æ¯” (train_white vs good250623)
        white_mask = np.array(labels) == 'train_white'
        good_mask = np.array(labels) == 'good250623'
        
        axes[1,0].scatter(pca_2d[white_mask, 0], pca_2d[white_mask, 1], 
                         c='blue', label='train_white', alpha=0.6, s=20)
        axes[1,0].scatter(pca_2d[good_mask, 0], pca_2d[good_mask, 1], 
                         c='green', label='good250623', alpha=0.6, s=20)
        axes[1,0].set_xlabel(f'PC1 ({self.pca_results["explained_variance_2d"][0]:.1%} variance)')
        axes[1,0].set_ylabel(f'PC2 ({self.pca_results["explained_variance_2d"][1]:.1%} variance)')
        axes[1,0].set_title('PCA - ç™½æ ·æœ¬å¯¹æ¯” (ç‰¹å¾åç§»å¯è§†åŒ–)')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. é»‘æ ·æœ¬å¯¹æ¯” (train_black vs bad250623)
        black_mask = np.array(labels) == 'train_black'
        bad_mask = np.array(labels) == 'bad250623'
        
        axes[1,1].scatter(pca_2d[black_mask, 0], pca_2d[black_mask, 1], 
                         c='orange', label='train_black', alpha=0.6, s=20)
        axes[1,1].scatter(pca_2d[bad_mask, 0], pca_2d[bad_mask, 1], 
                         c='red', label='bad250623', alpha=0.6, s=20)
        axes[1,1].set_xlabel(f'PC1 ({self.pca_results["explained_variance_2d"][0]:.1%} variance)')
        axes[1,1].set_ylabel(f'PC2 ({self.pca_results["explained_variance_2d"][1]:.1%} variance)')
        axes[1,1].set_title('PCA - é»‘æ ·æœ¬å¯¹æ¯” (ç‰¹å¾åç§»å¯è§†åŒ–)')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_file = self.output_dir / 'pca_analysis.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… PCAå›¾å·²ä¿å­˜: {output_file}")
    
    def create_tsne_visualizations(self, color_map):
        """åˆ›å»ºt-SNEå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆt-SNEå¯è§†åŒ–å›¾è¡¨...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        tsne_2d = self.tsne_results['2d']
        labels = self.tsne_results['labels']
        
        # 1. t-SNE 2Dæ•£ç‚¹å›¾ - å…¨éƒ¨æ•°æ®
        for dataset in color_map.keys():
            mask = np.array(labels) == dataset
            axes[0,0].scatter(tsne_2d[mask, 0], tsne_2d[mask, 1], 
                            c=color_map[dataset], label=dataset, alpha=0.6, s=20)
        
        axes[0,0].set_xlabel('t-SNE 1')
        axes[0,0].set_ylabel('t-SNE 2')
        axes[0,0].set_title('t-SNE 2DæŠ•å½± - æ•°æ®é›†èšç±»åˆ†æ')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. t-SNEå¯†åº¦å›¾
        axes[0,1].hexbin(tsne_2d[:, 0], tsne_2d[:, 1], gridsize=30, cmap='Blues', alpha=0.7)
        axes[0,1].set_xlabel('t-SNE 1')
        axes[0,1].set_ylabel('t-SNE 2')
        axes[0,1].set_title('t-SNE å¯†åº¦åˆ†å¸ƒ')
        
        # 3. ç™½æ ·æœ¬å¯¹æ¯”
        white_mask = np.array(labels) == 'train_white'
        good_mask = np.array(labels) == 'good250623'
        
        axes[1,0].scatter(tsne_2d[white_mask, 0], tsne_2d[white_mask, 1], 
                         c='blue', label='train_white', alpha=0.6, s=20)
        axes[1,0].scatter(tsne_2d[good_mask, 0], tsne_2d[good_mask, 1], 
                         c='green', label='good250623', alpha=0.6, s=20)
        axes[1,0].set_xlabel('t-SNE 1')
        axes[1,0].set_ylabel('t-SNE 2')
        axes[1,0].set_title('t-SNE - ç™½æ ·æœ¬èšç±»å¯¹æ¯”')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. é»‘æ ·æœ¬å¯¹æ¯”
        black_mask = np.array(labels) == 'train_black'
        bad_mask = np.array(labels) == 'bad250623'
        
        axes[1,1].scatter(tsne_2d[black_mask, 0], tsne_2d[black_mask, 1], 
                         c='orange', label='train_black', alpha=0.6, s=20)
        axes[1,1].scatter(tsne_2d[bad_mask, 0], tsne_2d[bad_mask, 1], 
                         c='red', label='bad250623', alpha=0.6, s=20)
        axes[1,1].set_xlabel('t-SNE 1')
        axes[1,1].set_ylabel('t-SNE 2')
        axes[1,1].set_title('t-SNE - é»‘æ ·æœ¬èšç±»å¯¹æ¯”')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_file = self.output_dir / 'tsne_analysis.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… t-SNEå›¾å·²ä¿å­˜: {output_file}")

    def create_3d_pca_visualization(self, color_map):
        """åˆ›å»º3D PCAå¯è§†åŒ–"""
        print("\nğŸ“ˆ ç”Ÿæˆ3D PCAå¯è§†åŒ–...")

        from mpl_toolkits.mplot3d import Axes3D

        fig = plt.figure(figsize=(15, 5))

        pca_3d = self.pca_results['3d']
        labels = self.pca_results['labels']

        # 1. å…¨éƒ¨æ•°æ®3Dè§†å›¾
        ax1 = fig.add_subplot(131, projection='3d')
        for dataset in color_map.keys():
            mask = np.array(labels) == dataset
            ax1.scatter(pca_3d[mask, 0], pca_3d[mask, 1], pca_3d[mask, 2],
                       c=color_map[dataset], label=dataset, alpha=0.6, s=15)

        ax1.set_xlabel(f'PC1 ({self.pca_results["explained_variance_3d"][0]:.1%})')
        ax1.set_ylabel(f'PC2 ({self.pca_results["explained_variance_3d"][1]:.1%})')
        ax1.set_zlabel(f'PC3 ({self.pca_results["explained_variance_3d"][2]:.1%})')
        ax1.set_title('3D PCA - å…¨éƒ¨æ•°æ®')
        ax1.legend()

        # 2. ç™½æ ·æœ¬3Då¯¹æ¯”
        ax2 = fig.add_subplot(132, projection='3d')
        white_mask = np.array(labels) == 'train_white'
        good_mask = np.array(labels) == 'good250623'

        ax2.scatter(pca_3d[white_mask, 0], pca_3d[white_mask, 1], pca_3d[white_mask, 2],
                   c='blue', label='train_white', alpha=0.6, s=15)
        ax2.scatter(pca_3d[good_mask, 0], pca_3d[good_mask, 1], pca_3d[good_mask, 2],
                   c='green', label='good250623', alpha=0.6, s=15)

        ax2.set_xlabel(f'PC1 ({self.pca_results["explained_variance_3d"][0]:.1%})')
        ax2.set_ylabel(f'PC2 ({self.pca_results["explained_variance_3d"][1]:.1%})')
        ax2.set_zlabel(f'PC3 ({self.pca_results["explained_variance_3d"][2]:.1%})')
        ax2.set_title('3D PCA - ç™½æ ·æœ¬å¯¹æ¯”')
        ax2.legend()

        # 3. é»‘æ ·æœ¬3Då¯¹æ¯”
        ax3 = fig.add_subplot(133, projection='3d')
        black_mask = np.array(labels) == 'train_black'
        bad_mask = np.array(labels) == 'bad250623'

        ax3.scatter(pca_3d[black_mask, 0], pca_3d[black_mask, 1], pca_3d[black_mask, 2],
                   c='orange', label='train_black', alpha=0.6, s=15)
        ax3.scatter(pca_3d[bad_mask, 0], pca_3d[bad_mask, 1], pca_3d[bad_mask, 2],
                   c='red', label='bad250623', alpha=0.6, s=15)

        ax3.set_xlabel(f'PC1 ({self.pca_results["explained_variance_3d"][0]:.1%})')
        ax3.set_ylabel(f'PC2 ({self.pca_results["explained_variance_3d"][1]:.1%})')
        ax3.set_zlabel(f'PC3 ({self.pca_results["explained_variance_3d"][2]:.1%})')
        ax3.set_title('3D PCA - é»‘æ ·æœ¬å¯¹æ¯”')
        ax3.legend()

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        output_file = self.output_dir / 'pca_3d_analysis.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… 3D PCAå›¾å·²ä¿å­˜: {output_file}")

    def calculate_separation_metrics(self):
        """è®¡ç®—æ•°æ®é›†åˆ†ç¦»åº¦æŒ‡æ ‡"""
        print("\nğŸ“Š è®¡ç®—æ•°æ®é›†åˆ†ç¦»åº¦æŒ‡æ ‡...")

        from sklearn.metrics import silhouette_score
        from scipy.spatial.distance import pdist, squareform

        # PCAç»“æœåˆ†æ
        pca_2d = self.pca_results['2d']
        labels = self.pca_results['labels']

        # è½¬æ¢æ ‡ç­¾ä¸ºæ•°å€¼
        label_map = {'train_white': 0, 'train_black': 1, 'good250623': 2, 'bad250623': 3}
        numeric_labels = [label_map[label] for label in labels]

        # è®¡ç®—è½®å»“ç³»æ•°
        pca_silhouette = silhouette_score(pca_2d, numeric_labels)

        # t-SNEç»“æœåˆ†æ
        tsne_2d = self.tsne_results['2d']
        tsne_silhouette = silhouette_score(tsne_2d, numeric_labels)

        # è®¡ç®—ç»„å†…å’Œç»„é—´è·ç¦»
        def calculate_group_distances(data, labels):
            unique_labels = list(set(labels))
            group_stats = {}

            for label in unique_labels:
                mask = np.array(labels) == label
                group_data = data[mask]

                if len(group_data) > 1:
                    # ç»„å†…å¹³å‡è·ç¦»
                    intra_distances = pdist(group_data)
                    intra_mean = np.mean(intra_distances)

                    group_stats[label] = {
                        'intra_distance': intra_mean,
                        'center': np.mean(group_data, axis=0),
                        'size': len(group_data)
                    }

            # è®¡ç®—ç»„é—´è·ç¦»
            inter_distances = {}
            centers = {label: stats['center'] for label, stats in group_stats.items()}

            for i, label1 in enumerate(unique_labels):
                for label2 in unique_labels[i+1:]:
                    if label1 in centers and label2 in centers:
                        dist = np.linalg.norm(centers[label1] - centers[label2])
                        inter_distances[f'{label1}_vs_{label2}'] = dist

            return group_stats, inter_distances

        pca_group_stats, pca_inter_dist = calculate_group_distances(pca_2d, labels)
        tsne_group_stats, tsne_inter_dist = calculate_group_distances(tsne_2d, labels)

        # ä¿å­˜åˆ†ç¦»åº¦åˆ†æç»“æœ
        separation_file = self.output_dir / 'separation_analysis.txt'
        with open(separation_file, 'w', encoding='utf-8') as f:
            f.write("æ•°æ®é›†åˆ†ç¦»åº¦åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            f.write("è½®å»“ç³»æ•° (Silhouette Score):\n")
            f.write(f"  PCA: {pca_silhouette:.4f}\n")
            f.write(f"  t-SNE: {tsne_silhouette:.4f}\n\n")

            f.write("PCAç»„é—´è·ç¦»:\n")
            for pair, dist in pca_inter_dist.items():
                f.write(f"  {pair}: {dist:.4f}\n")

            f.write("\nt-SNEç»„é—´è·ç¦»:\n")
            for pair, dist in tsne_inter_dist.items():
                f.write(f"  {pair}: {dist:.4f}\n")

            f.write("\nå…³é”®å¯¹æ¯”:\n")
            white_comparison = pca_inter_dist.get('train_white_vs_good250623', 0)
            black_comparison = pca_inter_dist.get('train_black_vs_bad250623', 0)
            f.write(f"  ç™½æ ·æœ¬åˆ†ç¦»åº¦ (PCA): {white_comparison:.4f}\n")
            f.write(f"  é»‘æ ·æœ¬åˆ†ç¦»åº¦ (PCA): {black_comparison:.4f}\n")

        print(f"âœ… åˆ†ç¦»åº¦åˆ†æå·²ä¿å­˜: {separation_file}")
        print(f"   - PCAè½®å»“ç³»æ•°: {pca_silhouette:.4f}")
        print(f"   - t-SNEè½®å»“ç³»æ•°: {tsne_silhouette:.4f}")

        return pca_silhouette, tsne_silhouette

    def generate_dimensionality_report(self):
        """ç”Ÿæˆé™ç»´åˆ†æç»¼åˆæŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆé™ç»´åˆ†ææŠ¥å‘Š...")

        report_file = self.output_dir / 'dimensionality_reduction_report.md'

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# é™ç»´å¯è§†åŒ–åˆ†ææŠ¥å‘Š\n\n")

            f.write("## åˆ†ææ¦‚è¿°\n\n")
            f.write("æœ¬æŠ¥å‘Šä½¿ç”¨PCAå’Œt-SNEé™ç»´æŠ€æœ¯å¯¹train.csvå’Œdataæ–‡ä»¶å¤¹æ•°æ®è¿›è¡Œå¯è§†åŒ–åˆ†æï¼Œ")
            f.write("é€šè¿‡å½©è‰²æ ‡æ³¨ä¸åŒæ•°æ®é›†ï¼Œç›´è§‚å±•ç¤ºæ•°æ®åˆ†å¸ƒã€èšç±»æƒ…å†µå’Œç‰¹å¾åç§»ã€‚\n\n")

            f.write("## æ•°æ®é›†æ ‡æ³¨\n\n")
            f.write("- ğŸ”µ **train_white**: train.csvä¸­çš„è‰¯æ€§æ ·æœ¬ (è“è‰²)\n")
            f.write("- ğŸŸ  **train_black**: train.csvä¸­çš„æ¶æ„æ ·æœ¬ (æ©™è‰²)\n")
            f.write("- ğŸŸ¢ **good250623**: good250623_features.csvè‰¯æ€§æ ·æœ¬ (ç»¿è‰²)\n")
            f.write("- ğŸ”´ **bad250623**: bad250623_features.csvæ¶æ„æ ·æœ¬ (çº¢è‰²)\n\n")

            f.write("## PCAåˆ†æç»“æœ\n\n")
            if self.pca_results:
                var_2d = self.pca_results['explained_variance_2d']
                var_3d = self.pca_results['explained_variance_3d']
                f.write(f"- **PC1è§£é‡Šæ–¹å·®**: {var_2d[0]:.1%}\n")
                f.write(f"- **PC2è§£é‡Šæ–¹å·®**: {var_2d[1]:.1%}\n")
                f.write(f"- **å‰2ä¸ªä¸»æˆåˆ†æ€»è§£é‡Šæ–¹å·®**: {sum(var_2d):.1%}\n")
                f.write(f"- **å‰3ä¸ªä¸»æˆåˆ†æ€»è§£é‡Šæ–¹å·®**: {sum(var_3d):.1%}\n\n")

            f.write("## ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶\n\n")
            f.write("### PCAå¯è§†åŒ–\n")
            f.write("- `pca_analysis.png`: 2D PCAåˆ†æå›¾ (4ä¸ªå­å›¾)\n")
            f.write("  - å…¨éƒ¨æ•°æ®é›†2DæŠ•å½±\n")
            f.write("  - PCAè§£é‡Šæ–¹å·®åˆ†æ\n")
            f.write("  - ç™½æ ·æœ¬å¯¹æ¯” (ç‰¹å¾åç§»å¯è§†åŒ–)\n")
            f.write("  - é»‘æ ·æœ¬å¯¹æ¯” (ç‰¹å¾åç§»å¯è§†åŒ–)\n")
            f.write("- `pca_3d_analysis.png`: 3D PCAåˆ†æå›¾\n\n")

            f.write("### t-SNEå¯è§†åŒ–\n")
            f.write("- `tsne_analysis.png`: t-SNEèšç±»åˆ†æå›¾ (4ä¸ªå­å›¾)\n")
            f.write("  - å…¨éƒ¨æ•°æ®é›†èšç±»åˆ†æ\n")
            f.write("  - t-SNEå¯†åº¦åˆ†å¸ƒ\n")
            f.write("  - ç™½æ ·æœ¬èšç±»å¯¹æ¯”\n")
            f.write("  - é»‘æ ·æœ¬èšç±»å¯¹æ¯”\n\n")

            f.write("### åˆ†ææ•°æ®\n")
            f.write("- `separation_analysis.txt`: æ•°æ®é›†åˆ†ç¦»åº¦åˆ†æ\n")
            f.write("- `dimensionality_reduction_report.md`: æœ¬æŠ¥å‘Š\n\n")

            f.write("## å¦‚ä½•è§£è¯»å›¾è¡¨\n\n")
            f.write("### PCAå›¾è§£è¯»\n")
            f.write("- **èšé›†ç¨‹åº¦**: åŒè‰²ç‚¹èšé›†è¡¨ç¤ºæ•°æ®ä¸€è‡´æ€§å¥½\n")
            f.write("- **åˆ†ç¦»ç¨‹åº¦**: ä¸åŒè‰²ç‚¹åˆ†ç¦»è¡¨ç¤ºæ•°æ®é›†å·®å¼‚å¤§\n")
            f.write("- **é‡å åŒºåŸŸ**: é‡å è¡¨ç¤ºç‰¹å¾ç›¸ä¼¼ï¼Œåˆ†ç¦»è¡¨ç¤ºç‰¹å¾åç§»\n\n")

            f.write("### t-SNEå›¾è§£è¯»\n")
            f.write("- **èšç±»ç»“æ„**: t-SNEæ›´å¥½åœ°ä¿æŒå±€éƒ¨ç»“æ„\n")
            f.write("- **å¯†åº¦åˆ†å¸ƒ**: æ˜¾ç¤ºæ•°æ®çš„èšé›†æ¨¡å¼\n")
            f.write("- **å¼‚å¸¸ç‚¹**: è¿œç¦»ä¸»è¦èšç±»çš„ç‚¹å¯èƒ½æ˜¯å¼‚å¸¸æ ·æœ¬\n\n")

            f.write("## ç‰¹å¾åç§»è¯Šæ–­\n\n")
            f.write("é€šè¿‡é™ç»´å¯è§†åŒ–å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°:\n")
            f.write("1. **ç™½æ ·æœ¬åç§»**: train_whiteå’Œgood250623çš„åˆ†å¸ƒå·®å¼‚\n")
            f.write("2. **é»‘æ ·æœ¬åç§»**: train_blackå’Œbad250623çš„åˆ†å¸ƒå·®å¼‚\n")
            f.write("3. **èšç±»è´¨é‡**: åŒç±»æ ·æœ¬çš„èšé›†ç¨‹åº¦\n")
            f.write("4. **æ•°æ®è´¨é‡**: å¼‚å¸¸ç‚¹å’Œå™ªå£°çš„åˆ†å¸ƒ\n")

        print(f"âœ… é™ç»´åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„é™ç»´åˆ†æ"""
        print("ğŸ¯ é™ç»´å¯è§†åŒ–åˆ†æ (ä½¿ç”¨å…¨éƒ¨æ•°æ®)")
        print("=" * 60)

        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            return False

        # 2. å‡†å¤‡åˆå¹¶æ•°æ®
        features, labels, colors, color_map = self.prepare_combined_data()

        # 3. PCAåˆ†æ
        pca_2d, pca_3d = self.perform_pca_analysis(features, labels, colors, color_map)

        # 4. t-SNEåˆ†æ
        tsne_2d = self.perform_tsne_analysis(features, labels, colors)

        # 5. åˆ›å»ºå¯è§†åŒ–
        self.create_pca_visualizations(color_map)
        self.create_3d_pca_visualization(color_map)
        self.create_tsne_visualizations(color_map)

        # 6. è®¡ç®—åˆ†ç¦»åº¦æŒ‡æ ‡
        self.calculate_separation_metrics()

        # 7. ç”ŸæˆæŠ¥å‘Š
        self.generate_dimensionality_report()

        print(f"\nğŸ‰ é™ç»´åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° featuretrain/ æ–‡ä»¶å¤¹")
        print(f"ğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
        print(f"   - pca_analysis.png (2D PCAåˆ†æ)")
        print(f"   - pca_3d_analysis.png (3D PCAåˆ†æ)")
        print(f"   - tsne_analysis.png (t-SNEèšç±»åˆ†æ)")

        return True

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DimensionalityReductionAnalyzer()
    success = analyzer.run_analysis()  # ä½¿ç”¨å…¨éƒ¨æ•°æ®

    if success:
        print("\nâœ… é™ç»´å¯è§†åŒ–åˆ†æå®Œæˆï¼")
        print("ğŸ” è¯·æŸ¥çœ‹ç”Ÿæˆçš„PNGæ–‡ä»¶æ¥è§‚å¯Ÿæ•°æ®åˆ†å¸ƒå’Œç‰¹å¾åç§»æƒ…å†µ")
    else:
        print("\nâŒ é™ç»´åˆ†æå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
