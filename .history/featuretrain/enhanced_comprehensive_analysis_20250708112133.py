#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆç»¼åˆç‰¹å¾åˆ†æ
æ•´åˆKSæ£€éªŒã€ç‰¹å¾æ¼‚ç§»ã€SHAPåˆ†æç­‰æ‰€æœ‰æ•°æ®æº
æä¾›æ›´å…¨é¢çš„ç‰¹å¾æ´å¯Ÿå’Œæ¨¡å‹ä¼˜åŒ–å»ºè®®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

class EnhancedComprehensiveAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆç»¼åˆåˆ†æå™¨"""
        self.output_dir = Path('featuretrain')

        # æ•°æ®å­˜å‚¨
        self.drift_data = {}
        self.misclassification_data = {}
        self.shap_data = {}
        self.feature_insights = {}

    def load_all_data_sources(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®æº"""
        print("ğŸ“Š åŠ è½½æ‰€æœ‰åˆ†ææ•°æ®æº...")

        try:
            # 1. åŠ è½½ç‰¹å¾æ¼‚ç§»æ•°æ®
            print("  ğŸ“– åŠ è½½ç‰¹å¾æ¼‚ç§»æ•°æ®...")
            self.drift_data['white_drift'] = pd.read_csv('featuretrain/ks_analysis_white_comparison.csv')
            self.drift_data['black_drift'] = pd.read_csv('featuretrain/ks_analysis_black_comparison.csv')

            # 2. åŠ è½½è¯¯æŠ¥åˆ†ææ•°æ®
            print("  ğŸ“– åŠ è½½è¯¯æŠ¥åˆ†ææ•°æ®...")
            try:
                self.misclassification_data['comprehensive'] = pd.read_excel('feature_analysis/comprehensive_ks_analysis.xlsx')
            except:
                print("    âš ï¸  comprehensive_ks_analysis.xlsxè¯»å–å¤±è´¥")

            try:
                self.misclassification_data['problematic'] = pd.read_excel('feature_analysis/problematic_features.xlsx')
            except:
                print("    âš ï¸  problematic_features.xlsxè¯»å–å¤±è´¥")

            # 3. åŠ è½½SHAPåˆ†ææ•°æ®
            print("  ğŸ“– åŠ è½½SHAPåˆ†ææ•°æ®...")
            try:
                self.shap_data['pattern_analysis'] = pd.read_excel('feature_analysis/shap_pattern_analysis.xlsx')
                print("    âœ… SHAPæ¨¡å¼åˆ†ææ•°æ®åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  SHAPæ¨¡å¼åˆ†ææ•°æ®è¯»å–å¤±è´¥: {e}")

            try:
                self.shap_data['problematic_features'] = pd.read_excel('feature_analysis/shap_problematic_features.xlsx')
                print("    âœ… SHAPé—®é¢˜ç‰¹å¾æ•°æ®åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  SHAPé—®é¢˜ç‰¹å¾æ•°æ®è¯»å–å¤±è´¥: {e}")

            # 4. å°è¯•åŠ è½½å…¶ä»–å¯èƒ½çš„SHAPç›¸å…³æ–‡ä»¶
            try:
                self.shap_data['integrated_results'] = pd.read_excel('feature_analysis/integrated_analysis_results.xlsx')
                print("    âœ… æ•´åˆåˆ†æç»“æœåŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  æ•´åˆåˆ†æç»“æœè¯»å–å¤±è´¥: {e}")

            print("âœ… æ•°æ®åŠ è½½å®Œæˆ")
            return True

        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False

    def analyze_shap_insights(self):
        """åˆ†æSHAPæ´å¯Ÿ"""
        print("\nğŸ” åˆ†æSHAPç‰¹å¾é‡è¦æ€§æ´å¯Ÿ...")

        shap_insights = {}

        # åˆ†æSHAPæ¨¡å¼æ•°æ®
        if 'pattern_analysis' in self.shap_data and self.shap_data['pattern_analysis'] is not None:
            pattern_data = self.shap_data['pattern_analysis']
            print(f"    ğŸ“‹ SHAPæ¨¡å¼åˆ†ææ•°æ®: {len(pattern_data)} è¡Œ")
            print(f"    ğŸ“‹ åˆ—å: {list(pattern_data.columns)}")

            # å¦‚æœæœ‰SHAPå€¼ç›¸å…³åˆ—ï¼Œè¿›è¡Œåˆ†æ
            shap_columns = [col for col in pattern_data.columns if 'shap' in col.lower()]
            if shap_columns:
                print(f"    ğŸ“Š å‘ç°SHAPç›¸å…³åˆ—: {shap_columns}")
                shap_insights['pattern_analysis'] = pattern_data

        # åˆ†æSHAPé—®é¢˜ç‰¹å¾
        if 'problematic_features' in self.shap_data and self.shap_data['problematic_features'] is not None:
            prob_data = self.shap_data['problematic_features']
            print(f"    ğŸ“‹ SHAPé—®é¢˜ç‰¹å¾æ•°æ®: {len(prob_data)} è¡Œ")
            print(f"    ğŸ“‹ åˆ—å: {list(prob_data.columns)}")
            shap_insights['problematic_features'] = prob_data

        # åˆ†ææ•´åˆç»“æœ
        if 'integrated_results' in self.shap_data and self.shap_data['integrated_results'] is not None:
            integrated_data = self.shap_data['integrated_results']
            print(f"    ğŸ“‹ æ•´åˆåˆ†æç»“æœ: {len(integrated_data)} è¡Œ")
            print(f"    ğŸ“‹ åˆ—å: {list(integrated_data.columns)}")
            shap_insights['integrated_results'] = integrated_data

        return shap_insights

    def create_enhanced_feature_importance(self, shap_insights):
        """åˆ›å»ºå¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆç»“åˆSHAPï¼‰"""
        print("\nâ­ åˆ›å»ºå¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æ...")

        # åŸºç¡€é‡è¦æ€§åˆ†æï¼ˆæ¥è‡ªä¹‹å‰çš„åˆ†æï¼‰
        white_drift = self.drift_data['white_drift']
        black_drift = self.drift_data['black_drift']

        enhanced_importance = []

        for feature in white_drift['feature']:
            white_row = white_drift[white_drift['feature'] == feature].iloc[0]
            black_row = black_drift[black_drift['feature'] == feature].iloc[0]

            # åŸºç¡€æŒ‡æ ‡
            white_mean = white_row['train_white_mean']
            black_mean = black_row['train_black_mean']
            white_std = white_row['train_white_std']
            black_std = black_row['train_black_std']

            # åˆ†ç¦»åº¦å’Œç¨³å®šæ€§
            mean_diff = abs(white_mean - black_mean)
            pooled_std = np.sqrt((white_std**2 + black_std**2) / 2)
            separation_score = mean_diff / pooled_std if pooled_std > 0 else 0
            stability_score = 1 / (1 + white_row['ks_statistic'] + black_row['ks_statistic'])
            basic_importance = separation_score * stability_score

            # åˆå§‹åŒ–SHAPç›¸å…³æŒ‡æ ‡
            shap_importance = 0
            shap_problematic = False
            shap_pattern_score = 0

            # æ•´åˆSHAPåˆ†æç»“æœ
            if 'pattern_analysis' in shap_insights:
                pattern_data = shap_insights['pattern_analysis']
                if 'feature' in pattern_data.columns:
                    feature_shap = pattern_data[pattern_data['feature'] == feature]
                    if not feature_shap.empty:
                        # æŸ¥æ‰¾SHAPå€¼ç›¸å…³åˆ—
                        shap_cols = [col for col in pattern_data.columns if 'shap' in col.lower() and 'value' in col.lower()]
                        if shap_cols:
                            shap_importance = abs(feature_shap[shap_cols[0]].iloc[0])

                        # æŸ¥æ‰¾æ¨¡å¼åˆ†æ•°
                        pattern_cols = [col for col in pattern_data.columns if 'pattern' in col.lower() or 'score' in col.lower()]
                        if pattern_cols:
                            shap_pattern_score = feature_shap[pattern_cols[0]].iloc[0] if not pd.isna(feature_shap[pattern_cols[0]].iloc[0]) else 0

            # æ£€æŸ¥æ˜¯å¦ä¸ºSHAPé—®é¢˜ç‰¹å¾
            if 'problematic_features' in shap_insights:
                prob_data = shap_insights['problematic_features']
                if 'feature' in prob_data.columns:
                    shap_problematic = feature in prob_data['feature'].values

            # ç»¼åˆé‡è¦æ€§åˆ†æ•°ï¼ˆç»“åˆåŸºç¡€åˆ†æå’ŒSHAPï¼‰
            if shap_importance > 0:
                # å¦‚æœæœ‰SHAPæ•°æ®ï¼Œç»“åˆä½¿ç”¨
                combined_importance = (basic_importance * 0.6 + shap_importance * 0.4)
            else:
                # å¦‚æœæ²¡æœ‰SHAPæ•°æ®ï¼Œä½¿ç”¨åŸºç¡€é‡è¦æ€§
                combined_importance = basic_importance

            enhanced_importance.append({
                'feature': feature,
                'basic_importance': basic_importance,
                'shap_importance': shap_importance,
                'combined_importance': combined_importance,
                'separation_score': separation_score,
                'stability_score': stability_score,
                'white_drift_ks': white_row['ks_statistic'],
                'black_drift_ks': black_row['ks_statistic'],
                'shap_problematic': shap_problematic,
                'shap_pattern_score': shap_pattern_score,
                'white_mean': white_mean,
                'black_mean': black_mean,
                'mean_difference': mean_diff
            })

        enhanced_df = pd.DataFrame(enhanced_importance)
        enhanced_df = enhanced_df.sort_values('combined_importance', ascending=False)

        # ä¿å­˜å¢å¼ºç‰ˆé‡è¦æ€§åˆ†æ
        enhanced_file = self.output_dir / 'enhanced_feature_importance_with_shap.csv'
        enhanced_df.to_csv(enhanced_file, index=False, encoding='utf-8')

        print(f"âœ… å¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æå·²ä¿å­˜: {enhanced_file}")
        print(f"   - æœ‰SHAPæ•°æ®çš„ç‰¹å¾: {len(enhanced_df[enhanced_df['shap_importance'] > 0])}")
        print(f"   - SHAPæ ‡è®°çš„é—®é¢˜ç‰¹å¾: {enhanced_df['shap_problematic'].sum()}")

        return enhanced_df

    def generate_enhanced_recommendations(self, enhanced_df, shap_insights):
        """ç”Ÿæˆå¢å¼ºç‰ˆä¼˜åŒ–å»ºè®®ï¼ˆç»“åˆSHAPåˆ†æï¼‰"""
        print("\nğŸ’¡ ç”Ÿæˆå¢å¼ºç‰ˆæ¨¡å‹ä¼˜åŒ–å»ºè®®...")

        enhanced_recommendations = {
            'shap_high_risk_features': [],
            'shap_important_features': [],
            'features_to_remove_enhanced': [],
            'features_to_engineer_enhanced': [],
            'features_to_keep_enhanced': [],
            'shap_specific_insights': [],
            'preprocessing_suggestions_enhanced': [],
            'model_suggestions_enhanced': []
        }

        # 1. SHAPé«˜é£é™©ç‰¹å¾
        shap_high_risk = enhanced_df[enhanced_df['shap_problematic'] == True]
        enhanced_recommendations['shap_high_risk_features'] = shap_high_risk['feature'].tolist()

        # 2. SHAPé‡è¦ç‰¹å¾
        shap_important = enhanced_df[enhanced_df['shap_importance'] > enhanced_df['shap_importance'].quantile(0.8)]
        enhanced_recommendations['shap_important_features'] = shap_important['feature'].tolist()

        # 3. å¢å¼ºç‰ˆç§»é™¤å»ºè®®ï¼ˆç»“åˆSHAPï¼‰
        # é«˜æ¼‚ç§» + ä½ç»¼åˆé‡è¦æ€§ + SHAPé—®é¢˜ç‰¹å¾
        remove_candidates = enhanced_df[
            ((enhanced_df['white_drift_ks'] > 0.4) | (enhanced_df['black_drift_ks'] > 0.4)) &
            (enhanced_df['combined_importance'] < enhanced_df['combined_importance'].quantile(0.3))
        ]
        enhanced_recommendations['features_to_remove_enhanced'] = remove_candidates['feature'].tolist()

        # 4. å¢å¼ºç‰ˆå·¥ç¨‹å»ºè®®
        # ä¸­ç­‰æ¼‚ç§» + é«˜SHAPé‡è¦æ€§
        engineer_candidates = enhanced_df[
            (((enhanced_df['white_drift_ks'] > 0.2) & (enhanced_df['white_drift_ks'] < 0.4)) |
             ((enhanced_df['black_drift_ks'] > 0.2) & (enhanced_df['black_drift_ks'] < 0.4))) &
            (enhanced_df['shap_importance'] > enhanced_df['shap_importance'].quantile(0.6))
        ]
        enhanced_recommendations['features_to_engineer_enhanced'] = engineer_candidates['feature'].tolist()

        # 5. å¢å¼ºç‰ˆä¿ç•™å»ºè®®
        # ä½æ¼‚ç§» + é«˜ç»¼åˆé‡è¦æ€§ + éSHAPé—®é¢˜ç‰¹å¾
        keep_candidates = enhanced_df[
            (enhanced_df['white_drift_ks'] < 0.2) &
            (enhanced_df['black_drift_ks'] < 0.2) &
            (enhanced_df['combined_importance'] > enhanced_df['combined_importance'].quantile(0.7)) &
            (enhanced_df['shap_problematic'] == False)
        ]
        enhanced_recommendations['features_to_keep_enhanced'] = keep_candidates['feature'].tolist()

        # 6. SHAPç‰¹å®šæ´å¯Ÿ
        if shap_insights:
            enhanced_recommendations['shap_specific_insights'] = [
                f"å‘ç° {len(shap_high_risk)} ä¸ªSHAPæ ‡è®°çš„é«˜é£é™©ç‰¹å¾",
                f"å‘ç° {len(shap_important)} ä¸ªSHAPé«˜é‡è¦æ€§ç‰¹å¾",
                "SHAPåˆ†ææä¾›äº†ç‰¹å¾è´¡çŒ®çš„å¯è§£é‡Šæ€§è§†è§’",
                "å»ºè®®é‡ç‚¹å…³æ³¨SHAPå€¼å¼‚å¸¸çš„ç‰¹å¾"
            ]

        # 7. å¢å¼ºç‰ˆé¢„å¤„ç†å»ºè®®
        enhanced_recommendations['preprocessing_suggestions_enhanced'] = [
            "åŸºäºSHAPåˆ†æç»“æœè¿›è¡Œç‰¹å¾é‡è¦æ€§åŠ æƒ",
            "å¯¹SHAPæ ‡è®°çš„é—®é¢˜ç‰¹å¾è¿›è¡Œç‰¹æ®Šå¤„ç†",
            "ä½¿ç”¨SHAPå€¼ä½œä¸ºç‰¹å¾é€‰æ‹©çš„å‚è€ƒæŒ‡æ ‡",
            "ç»“åˆSHAPå¯è§£é‡Šæ€§ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ç­–ç•¥"
        ]

        # 8. å¢å¼ºç‰ˆæ¨¡å‹å»ºè®®
        enhanced_recommendations['model_suggestions_enhanced'] = [
            "é›†æˆSHAPå¯è§£é‡Šæ€§åˆ°æ¨¡å‹ç›‘æ§ä¸­",
            "ä½¿ç”¨SHAPå€¼æ£€æµ‹æ¨¡å‹å†³ç­–çš„å¼‚å¸¸æ¨¡å¼",
            "åŸºäºSHAPåˆ†æå»ºç«‹ç‰¹å¾é‡è¦æ€§åŠ¨æ€è°ƒæ•´æœºåˆ¶",
            "ç»“åˆSHAPå’Œç»Ÿè®¡åˆ†æè¿›è¡Œç‰¹å¾é€‰æ‹©"
        ]

        return enhanced_recommendations

    def create_enhanced_visualization(self, enhanced_df, shap_insights):
        """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–ï¼ˆåŒ…å«SHAPåˆ†æï¼‰"""
        print("\nğŸ“ˆ ç”Ÿæˆå¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨...")

        fig, axes = plt.subplots(2, 3, figsize=(20, 12))

        # 1. åŸºç¡€é‡è¦æ€§ vs SHAPé‡è¦æ€§å¯¹æ¯”
        valid_shap = enhanced_df[enhanced_df['shap_importance'] > 0]
        if len(valid_shap) > 0:
            axes[0,0].scatter(valid_shap['basic_importance'], valid_shap['shap_importance'],
                             alpha=0.6, color='blue', s=50)
            axes[0,0].set_xlabel('åŸºç¡€é‡è¦æ€§åˆ†æ•°')
            axes[0,0].set_ylabel('SHAPé‡è¦æ€§åˆ†æ•°')
            axes[0,0].set_title('åŸºç¡€é‡è¦æ€§ vs SHAPé‡è¦æ€§')
            axes[0,0].grid(True, alpha=0.3)

            # æ·»åŠ å¯¹è§’çº¿å‚è€ƒ
            max_val = max(valid_shap['basic_importance'].max(), valid_shap['shap_importance'].max())
            axes[0,0].plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='å®Œå…¨ä¸€è‡´çº¿')
            axes[0,0].legend()
        else:
            axes[0,0].text(0.5, 0.5, 'No SHAP Data Available', ha='center', va='center',
                          transform=axes[0,0].transAxes, fontsize=14)
            axes[0,0].set_title('åŸºç¡€é‡è¦æ€§ vs SHAPé‡è¦æ€§')

        # 2. ç»¼åˆé‡è¦æ€§æ’åï¼ˆå‰15ä¸ªç‰¹å¾ï¼‰
        top_features = enhanced_df.head(15)
        y_pos = range(len(top_features))

        # åˆ†åˆ«æ˜¾ç¤ºåŸºç¡€é‡è¦æ€§å’ŒSHAPé‡è¦æ€§
        axes[0,1].barh(y_pos, top_features['basic_importance'], alpha=0.7,
                      color='lightblue', label='åŸºç¡€é‡è¦æ€§')
        axes[0,1].barh(y_pos, top_features['shap_importance'], alpha=0.7,
                      color='orange', label='SHAPé‡è¦æ€§')
        axes[0,1].set_yticks(y_pos)
        axes[0,1].set_yticklabels(top_features['feature'], fontsize=9)
        axes[0,1].set_xlabel('é‡è¦æ€§åˆ†æ•°')
        axes[0,1].set_title('å‰15ä¸ªç‰¹å¾çš„é‡è¦æ€§å¯¹æ¯”')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        axes[0,1].invert_yaxis()

        # 3. SHAPé—®é¢˜ç‰¹å¾æ ‡è¯†
        problem_features = enhanced_df[enhanced_df['shap_problematic'] == True]
        normal_features = enhanced_df[enhanced_df['shap_problematic'] == False]

        axes[0,2].scatter(normal_features['combined_importance'],
                         normal_features['white_drift_ks'],
                         alpha=0.6, color='green', label='æ­£å¸¸ç‰¹å¾', s=30)
        if len(problem_features) > 0:
            axes[0,2].scatter(problem_features['combined_importance'],
                             problem_features['white_drift_ks'],
                             alpha=0.8, color='red', label='SHAPé—®é¢˜ç‰¹å¾', s=50, marker='x')
        axes[0,2].set_xlabel('ç»¼åˆé‡è¦æ€§åˆ†æ•°')
        axes[0,2].set_ylabel('ç™½æ ·æœ¬æ¼‚ç§»KS')
        axes[0,2].set_title('SHAPé—®é¢˜ç‰¹å¾è¯†åˆ«')
        axes[0,2].legend()
        axes[0,2].grid(True, alpha=0.3)

        # 4. ç‰¹å¾åˆ†ç±»é¥¼å›¾ï¼ˆåŸºäºSHAPå’Œæ¼‚ç§»ï¼‰
        categories = {'ç¨³å®šé‡è¦': 0, 'SHAPé‡è¦ä½†æ¼‚ç§»': 0, 'SHAPé—®é¢˜': 0, 'ä½é‡è¦æ€§': 0}

        for _, row in enhanced_df.iterrows():
            if row['shap_problematic']:
                categories['SHAPé—®é¢˜'] += 1
            elif row['shap_importance'] > enhanced_df['shap_importance'].quantile(0.7) and \
                 (row['white_drift_ks'] > 0.2 or row['black_drift_ks'] > 0.2):
                categories['SHAPé‡è¦ä½†æ¼‚ç§»'] += 1
            elif row['combined_importance'] > enhanced_df['combined_importance'].quantile(0.7) and \
                 row['white_drift_ks'] < 0.2 and row['black_drift_ks'] < 0.2:
                categories['ç¨³å®šé‡è¦'] += 1
            else:
                categories['ä½é‡è¦æ€§'] += 1

        colors = ['green', 'orange', 'red', 'gray']
        axes[1,0].pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%',
                     colors=colors, startangle=90)
        axes[1,0].set_title('ç‰¹å¾åˆ†ç±»åˆ†å¸ƒï¼ˆåŸºäºSHAP+æ¼‚ç§»ï¼‰')

        # 5. æ¼‚ç§» vs ç»¼åˆé‡è¦æ€§æ•£ç‚¹å›¾
        axes[1,1].scatter(enhanced_df['combined_importance'],
                         enhanced_df['white_drift_ks'],
                         alpha=0.6, color='blue', label='ç™½æ ·æœ¬æ¼‚ç§»', s=30)
        axes[1,1].scatter(enhanced_df['combined_importance'],
                         enhanced_df['black_drift_ks'],
                         alpha=0.6, color='red', label='é»‘æ ·æœ¬æ¼‚ç§»', s=30)
        axes[1,1].set_xlabel('ç»¼åˆé‡è¦æ€§åˆ†æ•°')
        axes[1,1].set_ylabel('KSæ¼‚ç§»ç»Ÿè®¡é‡')
        axes[1,1].set_title('ç»¼åˆé‡è¦æ€§ vs ç‰¹å¾æ¼‚ç§»')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)

        # æ·»åŠ è±¡é™åˆ†å‰²çº¿
        axes[1,1].axhline(y=0.2, color='orange', linestyle='--', alpha=0.7)
        axes[1,1].axvline(x=enhanced_df['combined_importance'].median(),
                         color='green', linestyle='--', alpha=0.7)

        # 6. SHAPé‡è¦æ€§åˆ†å¸ƒ
        shap_values = enhanced_df[enhanced_df['shap_importance'] > 0]['shap_importance']
        if len(shap_values) > 0:
            axes[1,2].hist(shap_values, bins=20, alpha=0.7, color='purple', edgecolor='black')
            axes[1,2].set_xlabel('SHAPé‡è¦æ€§åˆ†æ•°')
            axes[1,2].set_ylabel('ç‰¹å¾æ•°é‡')
            axes[1,2].set_title('SHAPé‡è¦æ€§åˆ†å¸ƒ')
            axes[1,2].grid(True, alpha=0.3)
        else:
            axes[1,2].text(0.5, 0.5, 'No SHAP Data Available', ha='center', va='center',
                          transform=axes[1,2].transAxes, fontsize=14)
            axes[1,2].set_title('SHAPé‡è¦æ€§åˆ†å¸ƒ')

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        output_file = self.output_dir / 'enhanced_comprehensive_analysis_with_shap.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        print(f"âœ… å¢å¼ºç‰ˆç»¼åˆåˆ†æå›¾å·²ä¿å­˜: {output_file}")

    def generate_enhanced_report(self, enhanced_df, shap_insights, enhanced_recommendations):
        """ç”Ÿæˆå¢å¼ºç‰ˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆå¢å¼ºç‰ˆç»¼åˆåˆ†ææŠ¥å‘Š...")

        report_file = self.output_dir / 'enhanced_comprehensive_analysis_report.md'

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# å¢å¼ºç‰ˆç»¼åˆç‰¹å¾åˆ†ææŠ¥å‘Šï¼ˆæ•´åˆSHAPåˆ†æï¼‰\n\n")

            f.write("## ğŸ¯ åˆ†æç›®æ ‡\n\n")
            f.write("æœ¬æŠ¥å‘Šæ•´åˆäº†KSæ£€éªŒã€ç‰¹å¾æ¼‚ç§»åˆ†æå’ŒSHAPå¯è§£é‡Šæ€§åˆ†æçš„æ‰€æœ‰ç»“æœï¼Œ")
            f.write("æä¾›æ›´å…¨é¢ã€æ›´å¯è§£é‡Šçš„ç‰¹å¾æ´å¯Ÿå’Œæ¨¡å‹ä¼˜åŒ–å»ºè®®ã€‚\n\n")

            f.write("## ğŸ“Š æ•°æ®æºæ•´åˆ\n\n")
            f.write("### ä½¿ç”¨çš„æ•°æ®æºï¼š\n")
            f.write("1. **ç‰¹å¾æ¼‚ç§»æ•°æ®**: featuretrain/ks_analysis_*_comparison.csv\n")
            f.write("2. **è¯¯æŠ¥åˆ†ææ•°æ®**: feature_analysis/comprehensive_ks_analysis.xlsx\n")
            f.write("3. **SHAPåˆ†ææ•°æ®**: feature_analysis/shap_*.xlsx\n")
            f.write("4. **æ•´åˆåˆ†æç»“æœ**: feature_analysis/integrated_analysis_results.xlsx\n\n")

            # SHAPåˆ†æç»“æœ
            f.write("## ğŸ” SHAPåˆ†ææ´å¯Ÿ\n\n")
            shap_features_count = len(enhanced_df[enhanced_df['shap_importance'] > 0])
            shap_problematic_count = enhanced_df['shap_problematic'].sum()

            f.write(f"- **æœ‰SHAPæ•°æ®çš„ç‰¹å¾**: {shap_features_count} ä¸ª\n")
            f.write(f"- **SHAPæ ‡è®°çš„é—®é¢˜ç‰¹å¾**: {shap_problematic_count} ä¸ª\n")

            if shap_features_count > 0:
                f.write(f"- **SHAPé‡è¦æ€§æœ€é«˜çš„5ä¸ªç‰¹å¾**:\n")
                top_shap = enhanced_df[enhanced_df['shap_importance'] > 0].nlargest(5, 'shap_importance')
                for i, row in top_shap.iterrows():
                    f.write(f"  {i+1}. `{row['feature']}` (SHAP: {row['shap_importance']:.3f})\n")
            f.write("\n")

            # ç»¼åˆé‡è¦æ€§åˆ†æ
            f.write("## â­ ç»¼åˆé‡è¦æ€§åˆ†æï¼ˆåŸºç¡€+SHAPï¼‰\n\n")
            f.write(f"- **ç»¼åˆé‡è¦æ€§æœ€é«˜çš„5ä¸ªç‰¹å¾**:\n")
            for i, row in enhanced_df.head(5).iterrows():
                f.write(f"  {i+1}. `{row['feature']}` (ç»¼åˆ: {row['combined_importance']:.3f}, ")
                f.write(f"åŸºç¡€: {row['basic_importance']:.3f}, SHAP: {row['shap_importance']:.3f})\n")
            f.write("\n")

            # å¢å¼ºç‰ˆä¼˜åŒ–å»ºè®®
            f.write("## ğŸ’¡ å¢å¼ºç‰ˆæ¨¡å‹ä¼˜åŒ–å»ºè®®\n\n")

            f.write("### ğŸš¨ SHAPé«˜é£é™©ç‰¹å¾\n")
            f.write("ä»¥ä¸‹ç‰¹å¾è¢«SHAPåˆ†ææ ‡è®°ä¸ºé«˜é£é™©ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨:\n")
            for feature in enhanced_recommendations['shap_high_risk_features']:
                f.write(f"- `{feature}`\n")
            f.write("\n")

            f.write("### ğŸ—‘ï¸ å¢å¼ºç‰ˆç§»é™¤å»ºè®®\n")
            f.write("ç»“åˆSHAPåˆ†æï¼Œä»¥ä¸‹ç‰¹å¾å»ºè®®ç§»é™¤:\n")
            for feature in enhanced_recommendations['features_to_remove_enhanced'][:10]:
                f.write(f"- `{feature}`\n")
            f.write("\n")

            f.write("### ğŸ”§ å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹å»ºè®®\n")
            f.write("ç»“åˆSHAPé‡è¦æ€§ï¼Œä»¥ä¸‹ç‰¹å¾å»ºè®®è¿›è¡Œå·¥ç¨‹ä¼˜åŒ–:\n")
            for feature in enhanced_recommendations['features_to_engineer_enhanced'][:10]:
                f.write(f"- `{feature}`\n")
            f.write("\n")

            f.write("### âœ… å¢å¼ºç‰ˆæ ¸å¿ƒä¿ç•™ç‰¹å¾\n")
            f.write("ç»“åˆæ‰€æœ‰åˆ†æï¼Œä»¥ä¸‹ç‰¹å¾å»ºè®®ä½œä¸ºæ ¸å¿ƒä¿ç•™:\n")
            for feature in enhanced_recommendations['features_to_keep_enhanced'][:15]:
                f.write(f"- `{feature}`\n")
            f.write("\n")

            # SHAPç‰¹å®šæ´å¯Ÿ
            f.write("### ğŸ§  SHAPç‰¹å®šæ´å¯Ÿ\n")
            for insight in enhanced_recommendations['shap_specific_insights']:
                f.write(f"- {insight}\n")
            f.write("\n")

            # å¢å¼ºç‰ˆå»ºè®®
            f.write("### ğŸ”„ å¢å¼ºç‰ˆé¢„å¤„ç†å»ºè®®\n")
            for suggestion in enhanced_recommendations['preprocessing_suggestions_enhanced']:
                f.write(f"- {suggestion}\n")
            f.write("\n")

            f.write("### ğŸ¤– å¢å¼ºç‰ˆæ¨¡å‹ç­–ç•¥å»ºè®®\n")
            for suggestion in enhanced_recommendations['model_suggestions_enhanced']:
                f.write(f"- {suggestion}\n")
            f.write("\n")

            f.write("## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶\n\n")
            f.write("- `enhanced_feature_importance_with_shap.csv`: å¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æ\n")
            f.write("- `enhanced_comprehensive_analysis_with_shap.png`: å¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨\n")
            f.write("- `enhanced_comprehensive_analysis_report.md`: æœ¬æŠ¥å‘Š\n")

        print(f"âœ… å¢å¼ºç‰ˆç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def run_enhanced_analysis(self):
        """è¿è¡Œå¢å¼ºç‰ˆç»¼åˆåˆ†æ"""
        print("ğŸ¯ å¢å¼ºç‰ˆç»¼åˆç‰¹å¾åˆ†æï¼ˆæ•´åˆSHAPï¼‰")
        print("=" * 70)

        # 1. åŠ è½½æ‰€æœ‰æ•°æ®æº
        if not self.load_all_data_sources():
            return False

        # 2. åˆ†æSHAPæ´å¯Ÿ
        shap_insights = self.analyze_shap_insights()

        # 3. åˆ›å»ºå¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æ
        enhanced_df = self.create_enhanced_feature_importance(shap_insights)

        # 4. ç”Ÿæˆå¢å¼ºç‰ˆä¼˜åŒ–å»ºè®®
        enhanced_recommendations = self.generate_enhanced_recommendations(enhanced_df, shap_insights)

        # 5. åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–
        self.create_enhanced_visualization(enhanced_df, shap_insights)

        # 6. ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š
        self.generate_enhanced_report(enhanced_df, shap_insights, enhanced_recommendations)

        print(f"\nğŸ‰ å¢å¼ºç‰ˆç»¼åˆåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° featuretrain/ æ–‡ä»¶å¤¹")
        print(f"ğŸ§  SHAPåˆ†æå·²æ•´åˆåˆ°ç‰¹å¾é‡è¦æ€§è¯„ä¼°ä¸­")

        return True

def main():
    """ä¸»å‡½æ•°"""
    analyzer = EnhancedComprehensiveAnalysis()
    success = analyzer.run_enhanced_analysis()

    if success:
        print("\nâœ… å¢å¼ºç‰ˆç»¼åˆåˆ†æå®Œæˆï¼")
        print("ğŸ’¡ ç°åœ¨çš„åˆ†ææ•´åˆäº†KSæ£€éªŒã€ç‰¹å¾æ¼‚ç§»å’ŒSHAPå¯è§£é‡Šæ€§åˆ†æ")
    else:
        print("\nâŒ å¢å¼ºç‰ˆåˆ†æå¤±è´¥ï¼")

if __name__ == "__main__":
    main()