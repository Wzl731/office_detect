#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆç»¼åˆç‰¹å¾åˆ†æ
æ•´åˆKSæ£€éªŒã€ç‰¹å¾æ¼‚ç§»ã€SHAPåˆ†æç­‰æ‰€æœ‰æ•°æ®æº
æä¾›æ›´å…¨é¢çš„ç‰¹å¾æ´å¯Ÿå’Œæ¨¡å‹ä¼˜åŒ–å»ºè®®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

class EnhancedComprehensiveAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆç»¼åˆåˆ†æå™¨"""
        self.output_dir = Path('featuretrain')

        # æ•°æ®å­˜å‚¨
        self.drift_data = {}
        self.misclassification_data = {}
        self.shap_data = {}
        self.feature_insights = {}

    def load_all_data_sources(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®æº"""
        print("ğŸ“Š åŠ è½½æ‰€æœ‰åˆ†ææ•°æ®æº...")

        try:
            # 1. åŠ è½½ç‰¹å¾æ¼‚ç§»æ•°æ®
            print("  ğŸ“– åŠ è½½ç‰¹å¾æ¼‚ç§»æ•°æ®...")
            self.drift_data['white_drift'] = pd.read_csv('featuretrain/ks_analysis_white_comparison.csv')
            self.drift_data['black_drift'] = pd.read_csv('featuretrain/ks_analysis_black_comparison.csv')

            # 2. åŠ è½½è¯¯æŠ¥åˆ†ææ•°æ®
            print("  ğŸ“– åŠ è½½è¯¯æŠ¥åˆ†ææ•°æ®...")
            try:
                self.misclassification_data['comprehensive'] = pd.read_excel('feature_analysis/comprehensive_ks_analysis.xlsx')
            except:
                print("    âš ï¸  comprehensive_ks_analysis.xlsxè¯»å–å¤±è´¥")

            try:
                self.misclassification_data['problematic'] = pd.read_excel('feature_analysis/problematic_features.xlsx')
            except:
                print("    âš ï¸  problematic_features.xlsxè¯»å–å¤±è´¥")

            # 3. åŠ è½½SHAPåˆ†ææ•°æ®
            print("  ğŸ“– åŠ è½½SHAPåˆ†ææ•°æ®...")
            try:
                self.shap_data['pattern_analysis'] = pd.read_excel('feature_analysis/shap_pattern_analysis.xlsx')
                print("    âœ… SHAPæ¨¡å¼åˆ†ææ•°æ®åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  SHAPæ¨¡å¼åˆ†ææ•°æ®è¯»å–å¤±è´¥: {e}")

            try:
                self.shap_data['problematic_features'] = pd.read_excel('feature_analysis/shap_problematic_features.xlsx')
                print("    âœ… SHAPé—®é¢˜ç‰¹å¾æ•°æ®åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  SHAPé—®é¢˜ç‰¹å¾æ•°æ®è¯»å–å¤±è´¥: {e}")

            # 4. å°è¯•åŠ è½½å…¶ä»–å¯èƒ½çš„SHAPç›¸å…³æ–‡ä»¶
            try:
                self.shap_data['integrated_results'] = pd.read_excel('feature_analysis/integrated_analysis_results.xlsx')
                print("    âœ… æ•´åˆåˆ†æç»“æœåŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  æ•´åˆåˆ†æç»“æœè¯»å–å¤±è´¥: {e}")

            print("âœ… æ•°æ®åŠ è½½å®Œæˆ")
            return True

        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False

    def analyze_shap_insights(self):
        """åˆ†æSHAPæ´å¯Ÿ"""
        print("\nğŸ” åˆ†æSHAPç‰¹å¾é‡è¦æ€§æ´å¯Ÿ...")

        shap_insights = {}

        # åˆ†æSHAPæ¨¡å¼æ•°æ®
        if 'pattern_analysis' in self.shap_data and self.shap_data['pattern_analysis'] is not None:
            pattern_data = self.shap_data['pattern_analysis']
            print(f"    ğŸ“‹ SHAPæ¨¡å¼åˆ†ææ•°æ®: {len(pattern_data)} è¡Œ")
            print(f"    ğŸ“‹ åˆ—å: {list(pattern_data.columns)}")

            # å¦‚æœæœ‰SHAPå€¼ç›¸å…³åˆ—ï¼Œè¿›è¡Œåˆ†æ
            shap_columns = [col for col in pattern_data.columns if 'shap' in col.lower()]
            if shap_columns:
                print(f"    ğŸ“Š å‘ç°SHAPç›¸å…³åˆ—: {shap_columns}")
                shap_insights['pattern_analysis'] = pattern_data

        # åˆ†æSHAPé—®é¢˜ç‰¹å¾
        if 'problematic_features' in self.shap_data and self.shap_data['problematic_features'] is not None:
            prob_data = self.shap_data['problematic_features']
            print(f"    ğŸ“‹ SHAPé—®é¢˜ç‰¹å¾æ•°æ®: {len(prob_data)} è¡Œ")
            print(f"    ğŸ“‹ åˆ—å: {list(prob_data.columns)}")
            shap_insights['problematic_features'] = prob_data

        # åˆ†ææ•´åˆç»“æœ
        if 'integrated_results' in self.shap_data and self.shap_data['integrated_results'] is not None:
            integrated_data = self.shap_data['integrated_results']
            print(f"    ğŸ“‹ æ•´åˆåˆ†æç»“æœ: {len(integrated_data)} è¡Œ")
            print(f"    ğŸ“‹ åˆ—å: {list(integrated_data.columns)}")
            shap_insights['integrated_results'] = integrated_data

        return shap_insights

    def create_enhanced_feature_importance(self, shap_insights):
        """åˆ›å»ºå¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆç»“åˆSHAPï¼‰"""
        print("\nâ­ åˆ›å»ºå¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æ...")

        # åŸºç¡€é‡è¦æ€§åˆ†æï¼ˆæ¥è‡ªä¹‹å‰çš„åˆ†æï¼‰
        white_drift = self.drift_data['white_drift']
        black_drift = self.drift_data['black_drift']

        enhanced_importance = []

        for feature in white_drift['feature']:
            white_row = white_drift[white_drift['feature'] == feature].iloc[0]
            black_row = black_drift[black_drift['feature'] == feature].iloc[0]

            # åŸºç¡€æŒ‡æ ‡
            white_mean = white_row['train_white_mean']
            black_mean = black_row['train_black_mean']
            white_std = white_row['train_white_std']
            black_std = black_row['train_black_std']

            # åˆ†ç¦»åº¦å’Œç¨³å®šæ€§
            mean_diff = abs(white_mean - black_mean)
            pooled_std = np.sqrt((white_std**2 + black_std**2) / 2)
            separation_score = mean_diff / pooled_std if pooled_std > 0 else 0
            stability_score = 1 / (1 + white_row['ks_statistic'] + black_row['ks_statistic'])
            basic_importance = separation_score * stability_score

            # åˆå§‹åŒ–SHAPç›¸å…³æŒ‡æ ‡
            shap_importance = 0
            shap_problematic = False
            shap_pattern_score = 0

            # æ•´åˆSHAPåˆ†æç»“æœ
            if 'pattern_analysis' in shap_insights:
                pattern_data = shap_insights['pattern_analysis']
                if 'feature' in pattern_data.columns:
                    feature_shap = pattern_data[pattern_data['feature'] == feature]
                    if not feature_shap.empty:
                        # æŸ¥æ‰¾SHAPå€¼ç›¸å…³åˆ—
                        shap_cols = [col for col in pattern_data.columns if 'shap' in col.lower() and 'value' in col.lower()]
                        if shap_cols:
                            shap_importance = abs(feature_shap[shap_cols[0]].iloc[0])

                        # æŸ¥æ‰¾æ¨¡å¼åˆ†æ•°
                        pattern_cols = [col for col in pattern_data.columns if 'pattern' in col.lower() or 'score' in col.lower()]
                        if pattern_cols:
                            shap_pattern_score = feature_shap[pattern_cols[0]].iloc[0] if not pd.isna(feature_shap[pattern_cols[0]].iloc[0]) else 0

            # æ£€æŸ¥æ˜¯å¦ä¸ºSHAPé—®é¢˜ç‰¹å¾
            if 'problematic_features' in shap_insights:
                prob_data = shap_insights['problematic_features']
                if 'feature' in prob_data.columns:
                    shap_problematic = feature in prob_data['feature'].values

            # ç»¼åˆé‡è¦æ€§åˆ†æ•°ï¼ˆç»“åˆåŸºç¡€åˆ†æå’ŒSHAPï¼‰
            if shap_importance > 0:
                # å¦‚æœæœ‰SHAPæ•°æ®ï¼Œç»“åˆä½¿ç”¨
                combined_importance = (basic_importance * 0.6 + shap_importance * 0.4)
            else:
                # å¦‚æœæ²¡æœ‰SHAPæ•°æ®ï¼Œä½¿ç”¨åŸºç¡€é‡è¦æ€§
                combined_importance = basic_importance

            enhanced_importance.append({
                'feature': feature,
                'basic_importance': basic_importance,
                'shap_importance': shap_importance,
                'combined_importance': combined_importance,
                'separation_score': separation_score,
                'stability_score': stability_score,
                'white_drift_ks': white_row['ks_statistic'],
                'black_drift_ks': black_row['ks_statistic'],
                'shap_problematic': shap_problematic,
                'shap_pattern_score': shap_pattern_score,
                'white_mean': white_mean,
                'black_mean': black_mean,
                'mean_difference': mean_diff
            })

        enhanced_df = pd.DataFrame(enhanced_importance)
        enhanced_df = enhanced_df.sort_values('combined_importance', ascending=False)

        # ä¿å­˜å¢å¼ºç‰ˆé‡è¦æ€§åˆ†æ
        enhanced_file = self.output_dir / 'enhanced_feature_importance_with_shap.csv'
        enhanced_df.to_csv(enhanced_file, index=False, encoding='utf-8')

        print(f"âœ… å¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æå·²ä¿å­˜: {enhanced_file}")
        print(f"   - æœ‰SHAPæ•°æ®çš„ç‰¹å¾: {len(enhanced_df[enhanced_df['shap_importance'] > 0])}")
        print(f"   - SHAPæ ‡è®°çš„é—®é¢˜ç‰¹å¾: {enhanced_df['shap_problematic'].sum()}")

        return enhanced_df

    def generate_enhanced_recommendations(self, enhanced_df, shap_insights):
        """ç”Ÿæˆå¢å¼ºç‰ˆä¼˜åŒ–å»ºè®®ï¼ˆç»“åˆSHAPåˆ†æï¼‰"""
        print("\nğŸ’¡ ç”Ÿæˆå¢å¼ºç‰ˆæ¨¡å‹ä¼˜åŒ–å»ºè®®...")

        enhanced_recommendations = {
            'shap_high_risk_features': [],
            'shap_important_features': [],
            'features_to_remove_enhanced': [],
            'features_to_engineer_enhanced': [],
            'features_to_keep_enhanced': [],
            'shap_specific_insights': [],
            'preprocessing_suggestions_enhanced': [],
            'model_suggestions_enhanced': []
        }

        # 1. SHAPé«˜é£é™©ç‰¹å¾
        shap_high_risk = enhanced_df[enhanced_df['shap_problematic'] == True]
        enhanced_recommendations['shap_high_risk_features'] = shap_high_risk['feature'].tolist()

        # 2. SHAPé‡è¦ç‰¹å¾
        shap_important = enhanced_df[enhanced_df['shap_importance'] > enhanced_df['shap_importance'].quantile(0.8)]
        enhanced_recommendations['shap_important_features'] = shap_important['feature'].tolist()

        # 3. å¢å¼ºç‰ˆç§»é™¤å»ºè®®ï¼ˆç»“åˆSHAPï¼‰
        # é«˜æ¼‚ç§» + ä½ç»¼åˆé‡è¦æ€§ + SHAPé—®é¢˜ç‰¹å¾
        remove_candidates = enhanced_df[
            ((enhanced_df['white_drift_ks'] > 0.4) | (enhanced_df['black_drift_ks'] > 0.4)) &
            (enhanced_df['combined_importance'] < enhanced_df['combined_importance'].quantile(0.3))
        ]
        enhanced_recommendations['features_to_remove_enhanced'] = remove_candidates['feature'].tolist()

        # 4. å¢å¼ºç‰ˆå·¥ç¨‹å»ºè®®
        # ä¸­ç­‰æ¼‚ç§» + é«˜SHAPé‡è¦æ€§
        engineer_candidates = enhanced_df[
            (((enhanced_df['white_drift_ks'] > 0.2) & (enhanced_df['white_drift_ks'] < 0.4)) |
             ((enhanced_df['black_drift_ks'] > 0.2) & (enhanced_df['black_drift_ks'] < 0.4))) &
            (enhanced_df['shap_importance'] > enhanced_df['shap_importance'].quantile(0.6))
        ]
        enhanced_recommendations['features_to_engineer_enhanced'] = engineer_candidates['feature'].tolist()

        # 5. å¢å¼ºç‰ˆä¿ç•™å»ºè®®
        # ä½æ¼‚ç§» + é«˜ç»¼åˆé‡è¦æ€§ + éSHAPé—®é¢˜ç‰¹å¾
        keep_candidates = enhanced_df[
            (enhanced_df['white_drift_ks'] < 0.2) &
            (enhanced_df['black_drift_ks'] < 0.2) &
            (enhanced_df['combined_importance'] > enhanced_df['combined_importance'].quantile(0.7)) &
            (enhanced_df['shap_problematic'] == False)
        ]
        enhanced_recommendations['features_to_keep_enhanced'] = keep_candidates['feature'].tolist()

        # 6. SHAPç‰¹å®šæ´å¯Ÿ
        if shap_insights:
            enhanced_recommendations['shap_specific_insights'] = [
                f"å‘ç° {len(shap_high_risk)} ä¸ªSHAPæ ‡è®°çš„é«˜é£é™©ç‰¹å¾",
                f"å‘ç° {len(shap_important)} ä¸ªSHAPé«˜é‡è¦æ€§ç‰¹å¾",
                "SHAPåˆ†ææä¾›äº†ç‰¹å¾è´¡çŒ®çš„å¯è§£é‡Šæ€§è§†è§’",
                "å»ºè®®é‡ç‚¹å…³æ³¨SHAPå€¼å¼‚å¸¸çš„ç‰¹å¾"
            ]

        # 7. å¢å¼ºç‰ˆé¢„å¤„ç†å»ºè®®
        enhanced_recommendations['preprocessing_suggestions_enhanced'] = [
            "åŸºäºSHAPåˆ†æç»“æœè¿›è¡Œç‰¹å¾é‡è¦æ€§åŠ æƒ",
            "å¯¹SHAPæ ‡è®°çš„é—®é¢˜ç‰¹å¾è¿›è¡Œç‰¹æ®Šå¤„ç†",
            "ä½¿ç”¨SHAPå€¼ä½œä¸ºç‰¹å¾é€‰æ‹©çš„å‚è€ƒæŒ‡æ ‡",
            "ç»“åˆSHAPå¯è§£é‡Šæ€§ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ç­–ç•¥"
        ]

        # 8. å¢å¼ºç‰ˆæ¨¡å‹å»ºè®®
        enhanced_recommendations['model_suggestions_enhanced'] = [
            "é›†æˆSHAPå¯è§£é‡Šæ€§åˆ°æ¨¡å‹ç›‘æ§ä¸­",
            "ä½¿ç”¨SHAPå€¼æ£€æµ‹æ¨¡å‹å†³ç­–çš„å¼‚å¸¸æ¨¡å¼",
            "åŸºäºSHAPåˆ†æå»ºç«‹ç‰¹å¾é‡è¦æ€§åŠ¨æ€è°ƒæ•´æœºåˆ¶",
            "ç»“åˆSHAPå’Œç»Ÿè®¡åˆ†æè¿›è¡Œç‰¹å¾é€‰æ‹©"
        ]

        return enhanced_recommendations