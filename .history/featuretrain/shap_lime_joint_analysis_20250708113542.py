#!/usr/bin/env python3
"""
SHAP-LIMEè”åˆåˆ†æä¼˜åŒ–ç­–ç•¥
ç»“åˆSHAPå…¨å±€å¯è§£é‡Šæ€§å’ŒLIMEå±€éƒ¨å¯è§£é‡Šæ€§
ä¸ºç‰¹å¾ä¼˜åŒ–æä¾›æ›´å…¨é¢ã€æ›´å¯é çš„ç­–ç•¥å»ºè®®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import matplotlib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import shap
import lime
import lime.lime_tabular
from scipy import stats
import json

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class SHAPLIMEJointAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–SHAP-LIMEè”åˆåˆ†æç³»ç»Ÿ"""
        self.output_dir = Path('featuretrain')
        self.output_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.feature_names = None
        
        # æ¨¡å‹å’Œè§£é‡Šå™¨
        self.model = None
        self.shap_explainer = None
        self.lime_explainer = None
        
        # åˆ†æç»“æœ
        self.shap_values = None
        self.lime_explanations = []
        self.joint_insights = {}
        
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        print("ğŸ“Š åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        try:
            # åŠ è½½train.csvæ•°æ®
            self.train_data = pd.read_csv('train.csv')
            
            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            self.feature_names = self.train_data.columns[1:].tolist()  # æ’é™¤æ–‡ä»¶ååˆ—
            X = self.train_data[self.feature_names].values
            
            # åˆ›å»ºæ ‡ç­¾ (å‰2939ä¸ªæ˜¯è‰¯æ€§=0ï¼Œåé¢æ˜¯æ¶æ„=1)
            y = np.concatenate([np.zeros(2939), np.ones(len(self.train_data) - 2939)])
            
            # æ•°æ®åˆ†å‰²
            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            self.X_train = scaler.fit_transform(self.X_train)
            self.X_test = scaler.transform(self.X_test)
            
            print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
            print(f"   - è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
            print(f"   - æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
            print(f"   - ç‰¹å¾æ•°: {len(self.feature_names)}")
            print(f"   - è‰¯æ€§æ ·æœ¬: {int(np.sum(self.y_train == 0))} / æ¶æ„æ ·æœ¬: {int(np.sum(self.y_train == 1))}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return False
    
    def train_model(self):
        """è®­ç»ƒåŸºç¡€æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        try:
            # ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºåŸºç¡€æ¨¡å‹
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            
            self.model.fit(self.X_train, self.y_train)
            
            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            train_score = self.model.score(self.X_train, self.y_train)
            test_score = self.model.score(self.X_test, self.y_test)
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
            print(f"   - è®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
            print(f"   - æµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def perform_shap_analysis(self, sample_size=1000):
        """æ‰§è¡ŒSHAPåˆ†æ"""
        print(f"\nğŸ” æ‰§è¡ŒSHAPå…¨å±€å¯è§£é‡Šæ€§åˆ†æ (æ ·æœ¬æ•°: {sample_size})...")
        
        try:
            # åˆ›å»ºSHAPè§£é‡Šå™¨
            self.shap_explainer = shap.TreeExplainer(self.model)
            
            # é€‰æ‹©æ ·æœ¬è¿›è¡ŒSHAPåˆ†æï¼ˆä¸ºäº†è®¡ç®—æ•ˆç‡ï¼‰
            sample_indices = np.random.choice(len(self.X_test), min(sample_size, len(self.X_test)), replace=False)
            X_sample = self.X_test[sample_indices]
            
            # è®¡ç®—SHAPå€¼
            self.shap_values = self.shap_explainer.shap_values(X_sample)
            
            # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œå–æ­£ç±»çš„SHAPå€¼
            if isinstance(self.shap_values, list):
                self.shap_values = self.shap_values[1]  # æ¶æ„ç±»çš„SHAPå€¼
            
            print(f"âœ… SHAPåˆ†æå®Œæˆ:")
            print(f"   - åˆ†ææ ·æœ¬æ•°: {len(X_sample)}")
            print(f"   - SHAPå€¼å½¢çŠ¶: {self.shap_values.shape}")
            
            return X_sample, sample_indices
            
        except Exception as e:
            print(f"âŒ SHAPåˆ†æå¤±è´¥: {e}")
            return None, None
    
    def perform_lime_analysis(self, sample_indices, num_samples=100):
        """æ‰§è¡ŒLIMEåˆ†æ"""
        print(f"\nğŸ” æ‰§è¡ŒLIMEå±€éƒ¨å¯è§£é‡Šæ€§åˆ†æ (æ ·æœ¬æ•°: {num_samples})...")
        
        try:
            # åˆ›å»ºLIMEè§£é‡Šå™¨
            self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
                self.X_train,
                feature_names=self.feature_names,
                class_names=['Benign', 'Malicious'],
                mode='classification',
                discretize_continuous=True
            )
            
            # é€‰æ‹©æ ·æœ¬è¿›è¡ŒLIMEåˆ†æ
            lime_sample_indices = np.random.choice(sample_indices, min(num_samples, len(sample_indices)), replace=False)
            
            self.lime_explanations = []
            
            for i, idx in enumerate(lime_sample_indices):
                if i % 20 == 0:
                    print(f"    å¤„ç†LIMEæ ·æœ¬: {i+1}/{len(lime_sample_indices)}")
                
                # è·å–LIMEè§£é‡Š
                explanation = self.lime_explainer.explain_instance(
                    self.X_test[idx],
                    self.model.predict_proba,
                    num_features=len(self.feature_names),
                    top_labels=1
                )
                
                # æå–ç‰¹å¾é‡è¦æ€§
                lime_weights = {}
                for feature_idx, weight in explanation.as_list():
                    # è§£æç‰¹å¾åç§°ï¼ˆLIMEè¿”å›çš„æ˜¯ç‰¹å¾åç§°å­—ç¬¦ä¸²ï¼‰
                    feature_name = feature_idx.split('<=')[0].split('>')[0].strip()
                    if feature_name in self.feature_names:
                        lime_weights[feature_name] = weight
                    else:
                        # å¦‚æœæ— æ³•åŒ¹é…ï¼Œä½¿ç”¨ç´¢å¼•
                        try:
                            feat_idx = int(feature_idx.split('_')[-1]) if '_' in feature_idx else 0
                            if feat_idx < len(self.feature_names):
                                lime_weights[self.feature_names[feat_idx]] = weight
                        except:
                            continue
                
                self.lime_explanations.append({
                    'sample_idx': idx,
                    'prediction': self.model.predict_proba([self.X_test[idx]])[0],
                    'true_label': self.y_test[idx],
                    'lime_weights': lime_weights
                })
            
            print(f"âœ… LIMEåˆ†æå®Œæˆ:")
            print(f"   - åˆ†ææ ·æœ¬æ•°: {len(self.lime_explanations)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ LIMEåˆ†æå¤±è´¥: {e}")
            return False
    
    def analyze_shap_lime_consistency(self):
        """åˆ†æSHAPå’ŒLIMEçš„ä¸€è‡´æ€§"""
        print("\nğŸ”„ åˆ†æSHAP-LIMEä¸€è‡´æ€§...")
        
        try:
            # è®¡ç®—SHAPå…¨å±€é‡è¦æ€§
            shap_importance = np.abs(self.shap_values).mean(axis=0)
            shap_feature_importance = dict(zip(self.feature_names, shap_importance))
            
            # è®¡ç®—LIMEå¹³å‡é‡è¦æ€§
            lime_feature_importance = {}
            for feature in self.feature_names:
                weights = []
                for exp in self.lime_explanations:
                    if feature in exp['lime_weights']:
                        weights.append(abs(exp['lime_weights'][feature]))
                lime_feature_importance[feature] = np.mean(weights) if weights else 0
            
            # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
            consistency_analysis = []
            
            for feature in self.feature_names:
                shap_imp = shap_feature_importance.get(feature, 0)
                lime_imp = lime_feature_importance.get(feature, 0)
                
                # å½’ä¸€åŒ–é‡è¦æ€§åˆ†æ•°
                shap_norm = shap_imp / max(shap_feature_importance.values()) if max(shap_feature_importance.values()) > 0 else 0
                lime_norm = lime_imp / max(lime_feature_importance.values()) if max(lime_feature_importance.values()) > 0 else 0
                
                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
                consistency_score = 1 - abs(shap_norm - lime_norm)
                
                # è®¡ç®—é‡è¦æ€§ç­‰çº§
                shap_rank = sorted(shap_feature_importance.values(), reverse=True).index(shap_imp) + 1
                lime_rank = sorted(lime_feature_importance.values(), reverse=True).index(lime_imp) + 1
                rank_diff = abs(shap_rank - lime_rank)
                
                consistency_analysis.append({
                    'feature': feature,
                    'shap_importance': shap_imp,
                    'lime_importance': lime_imp,
                    'shap_normalized': shap_norm,
                    'lime_normalized': lime_norm,
                    'consistency_score': consistency_score,
                    'shap_rank': shap_rank,
                    'lime_rank': lime_rank,
                    'rank_difference': rank_diff,
                    'agreement_level': self.classify_agreement(consistency_score, rank_diff)
                })
            
            self.consistency_df = pd.DataFrame(consistency_analysis)
            self.consistency_df = self.consistency_df.sort_values('consistency_score', ascending=False)
            
            # ä¿å­˜ä¸€è‡´æ€§åˆ†æç»“æœ
            consistency_file = self.output_dir / 'shap_lime_consistency_analysis.csv'
            self.consistency_df.to_csv(consistency_file, index=False, encoding='utf-8')
            
            print(f"âœ… ä¸€è‡´æ€§åˆ†æå®Œæˆ: {consistency_file}")
            print(f"   - é«˜ä¸€è‡´æ€§ç‰¹å¾ (>0.8): {len(self.consistency_df[self.consistency_df['consistency_score'] > 0.8])}")
            print(f"   - ä¸­ç­‰ä¸€è‡´æ€§ç‰¹å¾ (0.6-0.8): {len(self.consistency_df[(self.consistency_df['consistency_score'] > 0.6) & (self.consistency_df['consistency_score'] <= 0.8)])}")
            print(f"   - ä½ä¸€è‡´æ€§ç‰¹å¾ (<0.6): {len(self.consistency_df[self.consistency_df['consistency_score'] <= 0.6])}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¸€è‡´æ€§åˆ†æå¤±è´¥: {e}")
            return False
    
    def classify_agreement(self, consistency_score, rank_diff):
        """åˆ†ç±»ä¸€è‡´æ€§ç­‰çº§"""
        if consistency_score > 0.8 and rank_diff < 10:
            return "é«˜åº¦ä¸€è‡´"
        elif consistency_score > 0.6 and rank_diff < 20:
            return "ä¸­ç­‰ä¸€è‡´"
        elif consistency_score > 0.4:
            return "éƒ¨åˆ†ä¸€è‡´"
        else:
            return "ä¸ä¸€è‡´"
    
    def generate_joint_optimization_strategy(self):
        """ç”ŸæˆSHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥"""
        print("\nğŸ’¡ ç”ŸæˆSHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥...")
        
        try:
            # åŠ è½½ä¹‹å‰çš„ä¸‰ç»´è¯„ä¼°ç»“æœ
            three_dim_scores = pd.read_csv('featuretrain/three_dimensional_feature_scores.csv')
            
            # åˆå¹¶åˆ†æç»“æœ
            joint_analysis = []
            
            for _, row in self.consistency_df.iterrows():
                feature = row['feature']
                
                # è·å–ä¸‰ç»´è¯„åˆ†
                three_dim_row = three_dim_scores[three_dim_scores['feature'] == feature]
                if not three_dim_row.empty:
                    stability = three_dim_row['stability_score'].iloc[0]
                    discriminative = three_dim_row['discriminative_power'].iloc[0]
                    interpretability = three_dim_row['interpretability_score'].iloc[0]
                    overall_score = three_dim_row['overall_score'].iloc[0]
                else:
                    stability = discriminative = interpretability = overall_score = 0
                
                # è®¡ç®—è”åˆå¯ä¿¡åº¦åˆ†æ•°
                # ç»“åˆSHAP-LIMEä¸€è‡´æ€§å’Œä¸‰ç»´è¯„åˆ†
                joint_reliability = (
                    row['consistency_score'] * 0.4 +  # SHAP-LIMEä¸€è‡´æ€§
                    interpretability * 0.3 +          # åŸæœ‰å¯è§£é‡Šæ€§
                    stability * 0.2 +                 # ç¨³å®šæ€§
                    discriminative * 0.1               # åˆ¤åˆ«åŠ›
                )
                
                # ç”Ÿæˆä¼˜åŒ–ç­–ç•¥
                strategy = self.determine_joint_strategy(
                    row['consistency_score'],
                    row['shap_importance'],
                    row['lime_importance'],
                    stability,
                    discriminative,
                    interpretability,
                    joint_reliability
                )
                
                joint_analysis.append({
                    'feature': feature,
                    'shap_importance': row['shap_importance'],
                    'lime_importance': row['lime_importance'],
                    'consistency_score': row['consistency_score'],
                    'agreement_level': row['agreement_level'],
                    'stability_score': stability,
                    'discriminative_power': discriminative,
                    'interpretability_score': interpretability,
                    'joint_reliability': joint_reliability,
                    'optimization_strategy': strategy['action'],
                    'strategy_reason': strategy['reason'],
                    'priority_level': strategy['priority'],
                    'confidence_level': strategy['confidence']
                })
            
            self.joint_strategy_df = pd.DataFrame(joint_analysis)
            self.joint_strategy_df = self.joint_strategy_df.sort_values('joint_reliability', ascending=False)
            
            # ä¿å­˜è”åˆç­–ç•¥
            strategy_file = self.output_dir / 'shap_lime_joint_optimization_strategy.csv'
            self.joint_strategy_df.to_csv(strategy_file, index=False, encoding='utf-8')
            
            print(f"âœ… è”åˆä¼˜åŒ–ç­–ç•¥ç”Ÿæˆå®Œæˆ: {strategy_file}")
            
            # ç»Ÿè®¡ç­–ç•¥åˆ†å¸ƒ
            strategy_counts = self.joint_strategy_df['optimization_strategy'].value_counts()
            print(f"ğŸ“Š ç­–ç•¥åˆ†å¸ƒ:")
            for strategy, count in strategy_counts.items():
                print(f"   - {strategy}: {count} ä¸ªç‰¹å¾")
            
            return True
            
        except Exception as e:
            print(f"âŒ è”åˆç­–ç•¥ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def determine_joint_strategy(self, consistency, shap_imp, lime_imp, stability, discriminative, interpretability, joint_reliability):
        """ç¡®å®šè”åˆä¼˜åŒ–ç­–ç•¥"""
        
        # é«˜å¯ä¿¡åº¦ç‰¹å¾
        if joint_reliability > 0.8 and consistency > 0.7:
            if discriminative > 0.6:
                return {
                    'action': 'æ ¸å¿ƒä¿ç•™',
                    'reason': 'SHAP-LIMEé«˜åº¦ä¸€è‡´ä¸”æ€§èƒ½ä¼˜ç§€',
                    'priority': 'é«˜',
                    'confidence': 'å¾ˆé«˜'
                }
            else:
                return {
                    'action': 'ç¨³å®šä¿ç•™',
                    'reason': 'SHAP-LIMEä¸€è‡´ä½†åˆ¤åˆ«åŠ›ä¸€èˆ¬',
                    'priority': 'ä¸­',
                    'confidence': 'é«˜'
                }
        
        # ä¸ä¸€è‡´ä½†é‡è¦çš„ç‰¹å¾
        elif consistency < 0.5 and (shap_imp > 0.1 or lime_imp > 0.1):
            return {
                'action': 'æ·±åº¦åˆ†æ',
                'reason': 'SHAP-LIMEä¸ä¸€è‡´ä½†æ˜¾ç¤ºé‡è¦æ€§ï¼Œéœ€è¦æ·±å…¥è°ƒæŸ¥',
                'priority': 'é«˜',
                'confidence': 'ä¸­'
            }
        
        # ä¸€è‡´æ€§å¥½ä½†æ€§èƒ½å·®çš„ç‰¹å¾
        elif consistency > 0.7 and discriminative < 0.3:
            return {
                'action': 'ç‰¹å¾å·¥ç¨‹',
                'reason': 'SHAP-LIMEä¸€è‡´ä½†åˆ¤åˆ«åŠ›å¼±ï¼Œé€‚åˆç»„åˆæˆ–å˜æ¢',
                'priority': 'ä¸­',
                'confidence': 'é«˜'
            }
        
        # ç¨³å®šæ€§å·®çš„ç‰¹å¾
        elif stability < 0.5:
            return {
                'action': 'ç›‘æ§æˆ–ç§»é™¤',
                'reason': 'ç¨³å®šæ€§å·®ï¼Œå­˜åœ¨æ¼‚ç§»é£é™©',
                'priority': 'é«˜',
                'confidence': 'é«˜'
            }
        
        # ä½é‡è¦æ€§ç‰¹å¾
        elif shap_imp < 0.01 and lime_imp < 0.01:
            return {
                'action': 'è€ƒè™‘ç§»é™¤',
                'reason': 'SHAP-LIMEå‡æ˜¾ç¤ºä½é‡è¦æ€§',
                'priority': 'ä½',
                'confidence': 'é«˜'
            }
        
        # ä¸­ç­‰è¡¨ç°ç‰¹å¾
        else:
            return {
                'action': 'ä¿æŒè§‚å¯Ÿ',
                'reason': 'è¡¨ç°ä¸­ç­‰ï¼Œç»§ç»­è§‚å¯Ÿ',
                'priority': 'ä½',
                'confidence': 'ä¸­'
            }
    
    def create_joint_visualizations(self):
        """åˆ›å»ºSHAP-LIMEè”åˆå¯è§†åŒ–"""
        print("\nğŸ“ˆ ç”ŸæˆSHAP-LIMEè”åˆå¯è§†åŒ–...")
        
        try:
            fig, axes = plt.subplots(2, 3, figsize=(20, 12))
            
            # 1. SHAP vs LIMEé‡è¦æ€§æ•£ç‚¹å›¾
            ax1 = axes[0, 0]
            scatter = ax1.scatter(self.consistency_df['shap_normalized'], 
                                 self.consistency_df['lime_normalized'],
                                 c=self.consistency_df['consistency_score'],
                                 s=60, alpha=0.7, cmap='RdYlGn')
            ax1.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='å®Œå…¨ä¸€è‡´çº¿')
            ax1.set_xlabel('SHAPé‡è¦æ€§ (å½’ä¸€åŒ–)')
            ax1.set_ylabel('LIMEé‡è¦æ€§ (å½’ä¸€åŒ–)')
            ax1.set_title('SHAP vs LIMEé‡è¦æ€§å¯¹æ¯”')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            plt.colorbar(scatter, ax=ax1, label='ä¸€è‡´æ€§åˆ†æ•°')
            
            # 2. ä¸€è‡´æ€§åˆ†æ•°åˆ†å¸ƒ
            ax2 = axes[0, 1]
            ax2.hist(self.consistency_df['consistency_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            ax2.set_xlabel('ä¸€è‡´æ€§åˆ†æ•°')
            ax2.set_ylabel('ç‰¹å¾æ•°é‡')
            ax2.set_title('SHAP-LIMEä¸€è‡´æ€§åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)
            
            # 3. ä¸€è‡´æ€§ç­‰çº§é¥¼å›¾
            ax3 = axes[0, 2]
            agreement_counts = self.consistency_df['agreement_level'].value_counts()
            colors = ['green', 'yellow', 'orange', 'red']
            ax3.pie(agreement_counts.values, labels=agreement_counts.index, 
                   autopct='%1.1f%%', colors=colors[:len(agreement_counts)], startangle=90)
            ax3.set_title('SHAP-LIMEä¸€è‡´æ€§ç­‰çº§åˆ†å¸ƒ')
            
            # 4. è”åˆå¯ä¿¡åº¦vsä¸€è‡´æ€§
            ax4 = axes[1, 0]
            ax4.scatter(self.joint_strategy_df['consistency_score'],
                       self.joint_strategy_df['joint_reliability'],
                       alpha=0.6, s=50)
            ax4.set_xlabel('SHAP-LIMEä¸€è‡´æ€§')
            ax4.set_ylabel('è”åˆå¯ä¿¡åº¦åˆ†æ•°')
            ax4.set_title('ä¸€è‡´æ€§ vs è”åˆå¯ä¿¡åº¦')
            ax4.grid(True, alpha=0.3)
            
            # 5. ä¼˜åŒ–ç­–ç•¥åˆ†å¸ƒ
            ax5 = axes[1, 1]
            strategy_counts = self.joint_strategy_df['optimization_strategy'].value_counts()
            bars = ax5.bar(range(len(strategy_counts)), strategy_counts.values, alpha=0.7)
            ax5.set_xticks(range(len(strategy_counts)))
            ax5.set_xticklabels(strategy_counts.index, rotation=45, ha='right')
            ax5.set_ylabel('ç‰¹å¾æ•°é‡')
            ax5.set_title('SHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥åˆ†å¸ƒ')
            ax5.grid(True, alpha=0.3)
            
            # ä¸ºä¸åŒç­–ç•¥è®¾ç½®ä¸åŒé¢œè‰²
            colors = ['green', 'blue', 'orange', 'red', 'purple', 'brown']
            for i, bar in enumerate(bars):
                bar.set_color(colors[i % len(colors)])
            
            # 6. å‰15ä¸ªé«˜å¯ä¿¡åº¦ç‰¹å¾
            ax6 = axes[1, 2]
            top_15 = self.joint_strategy_df.head(15)
            y_pos = range(len(top_15))
            
            bars = ax6.barh(y_pos, top_15['joint_reliability'], alpha=0.7)
            ax6.set_yticks(y_pos)
            ax6.set_yticklabels(top_15['feature'], fontsize=9)
            ax6.set_xlabel('è”åˆå¯ä¿¡åº¦åˆ†æ•°')
            ax6.set_title('å‰15ä¸ªé«˜å¯ä¿¡åº¦ç‰¹å¾')
            ax6.grid(True, alpha=0.3)
            ax6.invert_yaxis()
            
            # æ ¹æ®ç­–ç•¥è®¾ç½®é¢œè‰²
            strategy_colors = {'æ ¸å¿ƒä¿ç•™': 'green', 'ç¨³å®šä¿ç•™': 'blue', 'æ·±åº¦åˆ†æ': 'orange', 
                             'ç‰¹å¾å·¥ç¨‹': 'yellow', 'ç›‘æ§æˆ–ç§»é™¤': 'red', 'è€ƒè™‘ç§»é™¤': 'darkred',
                             'ä¿æŒè§‚å¯Ÿ': 'gray'}
            for i, (_, row) in enumerate(top_15.iterrows()):
                bars[i].set_color(strategy_colors.get(row['optimization_strategy'], 'gray'))
            
            plt.tight_layout()
            
            # ä¿å­˜å¯è§†åŒ–
            viz_file = self.output_dir / 'shap_lime_joint_analysis_visualization.png'
            plt.savefig(viz_file, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print(f"âœ… è”åˆå¯è§†åŒ–å·²ä¿å­˜: {viz_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            return False
