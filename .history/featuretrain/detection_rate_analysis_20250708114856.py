#!/usr/bin/env python3
"""
æ£€å‡ºç‡ä½åŸå› åˆ†æ
åˆ†æä¸ºä»€ä¹ˆé»‘æ ·æœ¬æ•°é‡å¤šä½†æ£€å‡ºç‡ä½çš„åŸå› 
åŒ…æ‹¬æ•°æ®è´¨é‡ã€ç‰¹å¾æœ‰æ•ˆæ€§ã€æ¨¡å‹æ€§èƒ½ç­‰å¤šç»´åº¦åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import matplotlib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import json

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class DetectionRateAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–æ£€å‡ºç‡åˆ†æ"""
        self.output_dir = Path('featuretrain')
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.data_data = None
        self.feature_names = None
        
        # åˆ†æç»“æœ
        self.analysis_results = {}
        
    def load_and_analyze_data_distribution(self):
        """åŠ è½½å¹¶åˆ†ææ•°æ®åˆ†å¸ƒ"""
        print("ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒå’Œè´¨é‡...")
        
        try:
            # åŠ è½½è®­ç»ƒæ•°æ®
            self.train_data = pd.read_csv('train.csv')
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            try:
                self.data_data = pd.read_csv('data.csv')
                print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
                print(f"   - è®­ç»ƒæ•°æ®: {len(self.train_data)} æ ·æœ¬")
                print(f"   - æµ‹è¯•æ•°æ®: {len(self.data_data)} æ ·æœ¬")
            except:
                print(f"âš ï¸  data.csvæœªæ‰¾åˆ°ï¼Œä»…åˆ†ætrain.csv")
                self.data_data = None
            
            # åˆ†æè®­ç»ƒæ•°æ®åˆ†å¸ƒ
            print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®åˆ†æ:")
            print(f"   - æ€»æ ·æœ¬æ•°: {len(self.train_data)}")
            
            # æ ¹æ®æ–‡ä»¶ååˆ†ææ ‡ç­¾åˆ†å¸ƒ
            # å‡è®¾å‰2939ä¸ªæ˜¯è‰¯æ€§ï¼Œåé¢æ˜¯æ¶æ„
            benign_count = 2939
            malicious_count = len(self.train_data) - benign_count
            
            print(f"   - è‰¯æ€§æ ·æœ¬: {benign_count} ({benign_count/len(self.train_data)*100:.1f}%)")
            print(f"   - æ¶æ„æ ·æœ¬: {malicious_count} ({malicious_count/len(self.train_data)*100:.1f}%)")
            
            # æ£€æŸ¥æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            imbalance_ratio = malicious_count / benign_count
            print(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f} (æ¶æ„/è‰¯æ€§)")
            
            if imbalance_ratio > 2:
                print(f"   âš ï¸  æ•°æ®ä¸¥é‡ä¸å¹³è¡¡ï¼æ¶æ„æ ·æœ¬è¿‡å¤šå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘")
            
            # åˆ†æç‰¹å¾ç»Ÿè®¡
            self.feature_names = self.train_data.columns[1:].tolist()  # æ’é™¤æ–‡ä»¶å
            print(f"   - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            
            # åˆ†æç‰¹å¾å€¼åˆ†å¸ƒ
            feature_data = self.train_data[self.feature_names]
            
            # æ£€æŸ¥é›¶å€¼ç‰¹å¾
            zero_features = []
            low_variance_features = []
            
            for feature in self.feature_names:
                values = feature_data[feature]
                zero_ratio = (values == 0).sum() / len(values)
                variance = values.var()
                
                if zero_ratio > 0.95:
                    zero_features.append((feature, zero_ratio))
                elif variance < 0.01:
                    low_variance_features.append((feature, variance))
            
            print(f"   - é›¶å€¼ç‰¹å¾ (>95%ä¸º0): {len(zero_features)}")
            print(f"   - ä½æ–¹å·®ç‰¹å¾ (<0.01): {len(low_variance_features)}")
            
            # ä¿å­˜åˆ†æç»“æœ
            self.analysis_results['data_distribution'] = {
                'total_samples': len(self.train_data),
                'benign_count': benign_count,
                'malicious_count': malicious_count,
                'imbalance_ratio': imbalance_ratio,
                'feature_count': len(self.feature_names),
                'zero_features_count': len(zero_features),
                'low_variance_features_count': len(low_variance_features),
                'zero_features': zero_features[:10],  # ä¿å­˜å‰10ä¸ª
                'low_variance_features': low_variance_features[:10]
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
            return False
    
    def analyze_feature_effectiveness(self):
        """åˆ†æç‰¹å¾æœ‰æ•ˆæ€§"""
        print("\nğŸ” åˆ†æç‰¹å¾æœ‰æ•ˆæ€§...")
        
        try:
            # å‡†å¤‡æ•°æ®
            feature_data = self.train_data[self.feature_names].values
            
            # åˆ›å»ºæ ‡ç­¾
            benign_count = 2939
            labels = np.concatenate([np.zeros(benign_count), np.ones(len(self.train_data) - benign_count)])
            
            # åˆ†æè‰¯æ€§å’Œæ¶æ„æ ·æœ¬çš„ç‰¹å¾å·®å¼‚
            benign_features = feature_data[:benign_count]
            malicious_features = feature_data[benign_count:]
            
            feature_effectiveness = []
            
            for i, feature_name in enumerate(self.feature_names):
                benign_values = benign_features[:, i]
                malicious_values = malicious_features[:, i]
                
                # è®¡ç®—ç»Ÿè®¡å·®å¼‚
                benign_mean = np.mean(benign_values)
                malicious_mean = np.mean(malicious_values)
                benign_std = np.std(benign_values)
                malicious_std = np.std(malicious_values)
                
                # è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)
                pooled_std = np.sqrt((benign_std**2 + malicious_std**2) / 2)
                cohens_d = abs(malicious_mean - benign_mean) / pooled_std if pooled_std > 0 else 0
                
                # è®¡ç®—é‡å åº¦
                min_max_benign = (np.min(benign_values), np.max(benign_values))
                min_max_malicious = (np.min(malicious_values), np.max(malicious_values))
                
                # è®¡ç®—åˆ†å¸ƒé‡å 
                overlap_start = max(min_max_benign[0], min_max_malicious[0])
                overlap_end = min(min_max_benign[1], min_max_malicious[1])
                overlap_ratio = max(0, overlap_end - overlap_start) / max(min_max_benign[1] - min_max_benign[0], 
                                                                         min_max_malicious[1] - min_max_malicious[0])
                
                # è®¡ç®—éé›¶æ¯”ä¾‹
                benign_nonzero_ratio = np.count_nonzero(benign_values) / len(benign_values)
                malicious_nonzero_ratio = np.count_nonzero(malicious_values) / len(malicious_values)
                
                feature_effectiveness.append({
                    'feature': feature_name,
                    'benign_mean': benign_mean,
                    'malicious_mean': malicious_mean,
                    'mean_difference': abs(malicious_mean - benign_mean),
                    'cohens_d': cohens_d,
                    'overlap_ratio': overlap_ratio,
                    'benign_nonzero_ratio': benign_nonzero_ratio,
                    'malicious_nonzero_ratio': malicious_nonzero_ratio,
                    'effectiveness_score': cohens_d * (1 - overlap_ratio) * max(benign_nonzero_ratio, malicious_nonzero_ratio)
                })
            
            self.feature_effectiveness_df = pd.DataFrame(feature_effectiveness)
            self.feature_effectiveness_df = self.feature_effectiveness_df.sort_values('effectiveness_score', ascending=False)
            
            # ä¿å­˜ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æ
            effectiveness_file = self.output_dir / 'feature_effectiveness_analysis.csv'
            self.feature_effectiveness_df.to_csv(effectiveness_file, index=False, encoding='utf-8')
            
            # ç»Ÿè®¡æœ‰æ•ˆç‰¹å¾
            highly_effective = len(self.feature_effectiveness_df[self.feature_effectiveness_df['effectiveness_score'] > 0.5])
            moderately_effective = len(self.feature_effectiveness_df[
                (self.feature_effectiveness_df['effectiveness_score'] > 0.1) & 
                (self.feature_effectiveness_df['effectiveness_score'] <= 0.5)
            ])
            low_effective = len(self.feature_effectiveness_df[self.feature_effectiveness_df['effectiveness_score'] <= 0.1])
            
            print(f"âœ… ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æå®Œæˆ:")
            print(f"   - é«˜æ•ˆç‰¹å¾ (>0.5): {highly_effective}")
            print(f"   - ä¸­æ•ˆç‰¹å¾ (0.1-0.5): {moderately_effective}")
            print(f"   - ä½æ•ˆç‰¹å¾ (â‰¤0.1): {low_effective}")
            
            # ä¿å­˜åˆ†æç»“æœ
            self.analysis_results['feature_effectiveness'] = {
                'highly_effective': highly_effective,
                'moderately_effective': moderately_effective,
                'low_effective': low_effective,
                'top_10_features': self.feature_effectiveness_df.head(10).to_dict('records')
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æå¤±è´¥: {e}")
            return False
    
    def train_and_evaluate_model(self):
        """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½"""
        print("\nğŸ¤– è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ£€å‡ºç‡...")
        
        try:
            # å‡†å¤‡æ•°æ®
            feature_data = self.train_data[self.feature_names].values
            benign_count = 2939
            labels = np.concatenate([np.zeros(benign_count), np.ones(len(self.train_data) - benign_count)])
            
            # æ•°æ®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                feature_data, labels, test_size=0.2, random_state=42, stratify=labels
            )
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # è®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
            models = {
                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                'RandomForest_Balanced': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
            }
            
            model_results = {}
            
            for model_name, model in models.items():
                print(f"   è®­ç»ƒ {model_name}...")
                
                # è®­ç»ƒæ¨¡å‹
                if 'Balanced' in model_name:
                    model.fit(X_train_scaled, y_train)
                else:
                    model.fit(X_train, y_train)
                
                # é¢„æµ‹
                if 'Balanced' in model_name:
                    y_pred = model.predict(X_test_scaled)
                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                else:
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # è®¡ç®—æŒ‡æ ‡
                from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
                
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred)
                recall = recall_score(y_test, y_pred)  # è¿™å°±æ˜¯æ£€å‡ºç‡
                f1 = f1_score(y_test, y_pred)
                auc = roc_auc_score(y_test, y_pred_proba)
                
                # æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y_test, y_pred)
                tn, fp, fn, tp = cm.ravel()
                
                # è®¡ç®—æ›´å¤šæŒ‡æ ‡
                false_positive_rate = fp / (fp + tn)
                false_negative_rate = fn / (fn + tp)
                
                model_results[model_name] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,  # æ£€å‡ºç‡
                    'f1_score': f1,
                    'auc': auc,
                    'false_positive_rate': false_positive_rate,
                    'false_negative_rate': false_negative_rate,
                    'confusion_matrix': cm.tolist(),
                    'true_positives': int(tp),
                    'false_positives': int(fp),
                    'true_negatives': int(tn),
                    'false_negatives': int(fn)
                }
                
                print(f"     - å‡†ç¡®ç‡: {accuracy:.3f}")
                print(f"     - æ£€å‡ºç‡ (å¬å›ç‡): {recall:.3f}")
                print(f"     - ç²¾ç¡®ç‡: {precision:.3f}")
                print(f"     - F1åˆ†æ•°: {f1:.3f}")
                print(f"     - AUC: {auc:.3f}")
                print(f"     - è¯¯æŠ¥ç‡: {false_positive_rate:.3f}")
                print(f"     - æ¼æŠ¥ç‡: {false_negative_rate:.3f}")
            
            # ä¿å­˜æ¨¡å‹è¯„ä¼°ç»“æœ
            self.analysis_results['model_performance'] = model_results
            
            return True, models, scaler, X_test, y_test
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒè¯„ä¼°å¤±è´¥: {e}")
            return False

    def create_detection_analysis_visualizations(self):
        """åˆ›å»ºæ£€å‡ºç‡åˆ†æå¯è§†åŒ–"""
        print("\nğŸ“ˆ ç”Ÿæˆæ£€å‡ºç‡åˆ†æå¯è§†åŒ–...")

        try:
            fig, axes = plt.subplots(2, 3, figsize=(20, 12))

            # 1. æ•°æ®åˆ†å¸ƒé¥¼å›¾
            ax1 = axes[0, 0]
            benign_count = self.analysis_results['data_distribution']['benign_count']
            malicious_count = self.analysis_results['data_distribution']['malicious_count']

            ax1.pie([benign_count, malicious_count],
                   labels=['è‰¯æ€§æ ·æœ¬', 'æ¶æ„æ ·æœ¬'],
                   autopct='%1.1f%%',
                   colors=['lightblue', 'lightcoral'],
                   startangle=90)
            ax1.set_title('è®­ç»ƒæ•°æ®åˆ†å¸ƒ')

            # 2. ç‰¹å¾æœ‰æ•ˆæ€§åˆ†å¸ƒ
            ax2 = axes[0, 1]
            effectiveness_scores = self.feature_effectiveness_df['effectiveness_score']
            ax2.hist(effectiveness_scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
            ax2.set_xlabel('ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æ•°')
            ax2.set_ylabel('ç‰¹å¾æ•°é‡')
            ax2.set_title('ç‰¹å¾æœ‰æ•ˆæ€§åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)

            # 3. å‰15ä¸ªæœ€æœ‰æ•ˆç‰¹å¾
            ax3 = axes[0, 2]
            top_15 = self.feature_effectiveness_df.head(15)
            y_pos = range(len(top_15))

            bars = ax3.barh(y_pos, top_15['effectiveness_score'], alpha=0.7, color='green')
            ax3.set_yticks(y_pos)
            ax3.set_yticklabels(top_15['feature'], fontsize=9)
            ax3.set_xlabel('æœ‰æ•ˆæ€§åˆ†æ•°')
            ax3.set_title('å‰15ä¸ªæœ€æœ‰æ•ˆç‰¹å¾')
            ax3.grid(True, alpha=0.3)
            ax3.invert_yaxis()

            # 4. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
            ax4 = axes[1, 0]
            if 'model_performance' in self.analysis_results:
                models = list(self.analysis_results['model_performance'].keys())
                metrics = ['accuracy', 'precision', 'recall', 'f1_score']

                x = np.arange(len(models))
                width = 0.2

                for i, metric in enumerate(metrics):
                    values = [self.analysis_results['model_performance'][model][metric] for model in models]
                    ax4.bar(x + i*width, values, width, label=metric, alpha=0.8)

                ax4.set_xlabel('æ¨¡å‹')
                ax4.set_ylabel('åˆ†æ•°')
                ax4.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
                ax4.set_xticks(x + width * 1.5)
                ax4.set_xticklabels(models, rotation=45)
                ax4.legend()
                ax4.grid(True, alpha=0.3)

            # 5. ç‰¹å¾åˆ†å¸ƒé‡å åˆ†æ
            ax5 = axes[1, 1]
            overlap_ratios = self.feature_effectiveness_df['overlap_ratio']
            ax5.hist(overlap_ratios, bins=20, alpha=0.7, color='orange', edgecolor='black')
            ax5.set_xlabel('é‡å æ¯”ä¾‹')
            ax5.set_ylabel('ç‰¹å¾æ•°é‡')
            ax5.set_title('è‰¯æ€§vsæ¶æ„æ ·æœ¬ç‰¹å¾é‡å åˆ†å¸ƒ')
            ax5.grid(True, alpha=0.3)

            # 6. é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
            ax6 = axes[1, 2]
            if 'detection_issues' in self.analysis_results:
                severity_counts = {}
                for issue in self.analysis_results['detection_issues']:
                    severity = issue['severity']
                    severity_counts[severity] = severity_counts.get(severity, 0) + 1

                colors = {'é«˜': 'red', 'ä¸­': 'orange', 'ä½': 'yellow'}
                severities = list(severity_counts.keys())
                counts = list(severity_counts.values())
                bar_colors = [colors.get(s, 'gray') for s in severities]

                ax6.bar(severities, counts, color=bar_colors, alpha=0.7)
                ax6.set_xlabel('ä¸¥é‡ç¨‹åº¦')
                ax6.set_ylabel('é—®é¢˜æ•°é‡')
                ax6.set_title('æ£€å‡ºç‡é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
                ax6.grid(True, alpha=0.3)

            plt.tight_layout()

            # ä¿å­˜å¯è§†åŒ–
            viz_file = self.output_dir / 'detection_rate_analysis.png'
            plt.savefig(viz_file, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()

            print(f"âœ… æ£€å‡ºç‡åˆ†æå¯è§†åŒ–å·²ä¿å­˜: {viz_file}")

            return True

        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            return False

    def generate_comprehensive_report(self):
        """ç”Ÿæˆæ£€å‡ºç‡åˆ†æç»¼åˆæŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆæ£€å‡ºç‡åˆ†ææŠ¥å‘Š...")

        try:
            report_file = self.output_dir / 'detection_rate_analysis_report.md'

            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# æ£€å‡ºç‡ä½åŸå› åˆ†ææŠ¥å‘Š\n\n")

                f.write("## ğŸ¯ åˆ†æç›®æ ‡\n\n")
                f.write("æœ¬æŠ¥å‘Šæ·±å…¥åˆ†æä¸ºä»€ä¹ˆé»‘æ ·æœ¬æ•°é‡å¤šä½†æ£€å‡ºç‡ä½çš„åŸå› ï¼Œ")
                f.write("ä»æ•°æ®è´¨é‡ã€ç‰¹å¾æœ‰æ•ˆæ€§ã€æ¨¡å‹æ€§èƒ½ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œè¯Šæ–­ã€‚\n\n")

                # æ•°æ®åˆ†å¸ƒåˆ†æ
                f.write("## ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ\n\n")
                dist = self.analysis_results['data_distribution']
                f.write(f"- **æ€»æ ·æœ¬æ•°**: {dist['total_samples']}\n")
                f.write(f"- **è‰¯æ€§æ ·æœ¬**: {dist['benign_count']} ({dist['benign_count']/dist['total_samples']*100:.1f}%)\n")
                f.write(f"- **æ¶æ„æ ·æœ¬**: {dist['malicious_count']} ({dist['malicious_count']/dist['total_samples']*100:.1f}%)\n")
                f.write(f"- **ä¸å¹³è¡¡æ¯”ä¾‹**: {dist['imbalance_ratio']:.2f} (æ¶æ„/è‰¯æ€§)\n")
                f.write(f"- **ç‰¹å¾æ•°é‡**: {dist['feature_count']}\n")
                f.write(f"- **é›¶å€¼ç‰¹å¾**: {dist['zero_features_count']} ({dist['zero_features_count']/dist['feature_count']*100:.1f}%)\n")
                f.write(f"- **ä½æ–¹å·®ç‰¹å¾**: {dist['low_variance_features_count']}\n\n")

                if dist['imbalance_ratio'] > 2:
                    f.write("âš ï¸ **æ•°æ®ä¸å¹³è¡¡ä¸¥é‡**: æ¶æ„æ ·æœ¬è¿‡å¤šå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘ï¼Œå½±å“æ³›åŒ–èƒ½åŠ›\n\n")

                # ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æ
                f.write("## ğŸ” ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æ\n\n")
                eff = self.analysis_results['feature_effectiveness']
                f.write(f"- **é«˜æ•ˆç‰¹å¾** (>0.5): {eff['highly_effective']} ä¸ª\n")
                f.write(f"- **ä¸­æ•ˆç‰¹å¾** (0.1-0.5): {eff['moderately_effective']} ä¸ª\n")
                f.write(f"- **ä½æ•ˆç‰¹å¾** (â‰¤0.1): {eff['low_effective']} ä¸ª\n\n")

                f.write("### æœ€æœ‰æ•ˆçš„10ä¸ªç‰¹å¾:\n")
                for i, feature in enumerate(eff['top_10_features'][:10]):
                    f.write(f"{i+1}. **`{feature['feature']}`** (æœ‰æ•ˆæ€§: {feature['effectiveness_score']:.3f})\n")
                    f.write(f"   - Cohen's d: {feature['cohens_d']:.3f}\n")
                    f.write(f"   - é‡å æ¯”ä¾‹: {feature['overlap_ratio']:.3f}\n")
                    f.write(f"   - è‰¯æ€§å‡å€¼: {feature['benign_mean']:.3f}, æ¶æ„å‡å€¼: {feature['malicious_mean']:.3f}\n\n")

                # æ¨¡å‹æ€§èƒ½åˆ†æ
                if 'model_performance' in self.analysis_results:
                    f.write("## ğŸ¤– æ¨¡å‹æ€§èƒ½åˆ†æ\n\n")

                    for model_name, metrics in self.analysis_results['model_performance'].items():
                        f.write(f"### {model_name}\n")
                        f.write(f"- **æ£€å‡ºç‡ (å¬å›ç‡)**: {metrics['recall']:.3f}\n")
                        f.write(f"- **ç²¾ç¡®ç‡**: {metrics['precision']:.3f}\n")
                        f.write(f"- **å‡†ç¡®ç‡**: {metrics['accuracy']:.3f}\n")
                        f.write(f"- **F1åˆ†æ•°**: {metrics['f1_score']:.3f}\n")
                        f.write(f"- **AUC**: {metrics['auc']:.3f}\n")
                        f.write(f"- **è¯¯æŠ¥ç‡**: {metrics['false_positive_rate']:.3f}\n")
                        f.write(f"- **æ¼æŠ¥ç‡**: {metrics['false_negative_rate']:.3f}\n")
                        f.write(f"- **æ··æ·†çŸ©é˜µ**: TP={metrics['true_positives']}, FP={metrics['false_positives']}, TN={metrics['true_negatives']}, FN={metrics['false_negatives']}\n\n")

                # é—®é¢˜è¯Šæ–­
                f.write("## âš ï¸ æ£€å‡ºç‡ä½çš„ä¸»è¦åŸå› \n\n")

                if 'detection_issues' in self.analysis_results:
                    for i, issue in enumerate(self.analysis_results['detection_issues']):
                        f.write(f"### {i+1}. {issue['issue']} (ä¸¥é‡ç¨‹åº¦: {issue['severity']})\n")
                        f.write(f"**é—®é¢˜æè¿°**: {issue['description']}\n\n")
                        f.write(f"**è§£å†³æ–¹æ¡ˆ**: {issue['solution']}\n\n")

                # æ”¹è¿›å»ºè®®
                f.write("## ğŸ’¡ æ”¹è¿›å»ºè®®\n\n")
                f.write("### ç«‹å³è¡ŒåŠ¨\n")
                f.write("1. **å¤„ç†æ•°æ®ä¸å¹³è¡¡**: ä½¿ç”¨SMOTEã€ä¸‹é‡‡æ ·æˆ–ç±»åˆ«æƒé‡å¹³è¡¡\n")
                f.write("2. **ç§»é™¤æ— æ•ˆç‰¹å¾**: åˆ é™¤é›¶å€¼å’Œä½æ–¹å·®ç‰¹å¾\n")
                f.write("3. **ç‰¹å¾é€‰æ‹©**: ä¿ç•™é«˜æ•ˆç‰¹å¾ï¼Œç§»é™¤ä½æ•ˆç‰¹å¾\n\n")

                f.write("### ä¸­æœŸä¼˜åŒ–\n")
                f.write("1. **ç‰¹å¾å·¥ç¨‹**: åˆ›å»ºç»„åˆç‰¹å¾ã€æ¯”ç‡ç‰¹å¾\n")
                f.write("2. **æ¨¡å‹è°ƒä¼˜**: å°è¯•ä¸åŒç®—æ³•å’Œå‚æ•°\n")
                f.write("3. **é›†æˆå­¦ä¹ **: ä½¿ç”¨å¤šæ¨¡å‹æŠ•ç¥¨æˆ–å †å \n\n")

                f.write("### é•¿æœŸæ”¹è¿›\n")
                f.write("1. **æ•°æ®æ”¶é›†**: æ”¶é›†æ›´å¤šé«˜è´¨é‡çš„åŒºåˆ†æ€§ç‰¹å¾\n")
                f.write("2. **é¢†åŸŸçŸ¥è¯†**: ç»“åˆæ¶æ„è½¯ä»¶åˆ†æä¸“ä¸šçŸ¥è¯†\n")
                f.write("3. **æŒç»­ç›‘æ§**: å»ºç«‹æ¨¡å‹æ€§èƒ½ç›‘æ§æœºåˆ¶\n\n")

                f.write("## ğŸ“ ç”Ÿæˆæ–‡ä»¶\n\n")
                f.write("- `feature_effectiveness_analysis.csv`: ç‰¹å¾æœ‰æ•ˆæ€§è¯¦ç»†åˆ†æ\n")
                f.write("- `detection_rate_analysis.png`: æ£€å‡ºç‡åˆ†æå¯è§†åŒ–\n")
                f.write("- `detection_rate_analysis_report.md`: æœ¬æŠ¥å‘Š\n")

            print(f"âœ… æ£€å‡ºç‡åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

            return True

        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return False

    def run_detection_rate_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ£€å‡ºç‡åˆ†æ"""
        print("ğŸ¯ æ£€å‡ºç‡ä½åŸå› åˆ†æ")
        print("=" * 60)
        print("åˆ†æé»‘æ ·æœ¬å¤šä½†æ£€å‡ºç‡ä½çš„åŸå› ")
        print("=" * 60)

        # 1. æ•°æ®åˆ†å¸ƒåˆ†æ
        if not self.load_and_analyze_data_distribution():
            return False

        # 2. ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æ
        if not self.analyze_feature_effectiveness():
            return False

        # 3. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
        success, models, scaler, X_test, y_test = self.train_and_evaluate_model()
        if not success:
            return False

        # 4. é—®é¢˜è¯Šæ–­
        if not self.analyze_detection_issues():
            return False

        # 5. å¯è§†åŒ–
        if not self.create_detection_analysis_visualizations():
            return False

        # 6. ç”ŸæˆæŠ¥å‘Š
        if not self.generate_comprehensive_report():
            return False

        print(f"\nğŸ‰ æ£€å‡ºç‡åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° featuretrain/ æ–‡ä»¶å¤¹")
        print(f"ğŸ” å·²è¯†åˆ«æ£€å‡ºç‡ä½çš„ä¸»è¦åŸå› å’Œæ”¹è¿›å»ºè®®")

        return True

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DetectionRateAnalysis()
    success = analyzer.run_detection_rate_analysis()

    if success:
        print("\nâœ… æ£€å‡ºç‡åˆ†æå®Œæˆï¼")
        print("ğŸ’¡ è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šäº†è§£æ£€å‡ºç‡ä½çš„å…·ä½“åŸå› å’Œæ”¹è¿›å»ºè®®")
    else:
        print("\nâŒ æ£€å‡ºç‡åˆ†æå¤±è´¥ï¼")

if __name__ == "__main__":
    main(), None, None, None, None
    
    def analyze_detection_issues(self):
        """åˆ†ææ£€å‡ºç‡ä½çš„å…·ä½“åŸå› """
        print("\nğŸ” åˆ†ææ£€å‡ºç‡ä½çš„å…·ä½“åŸå› ...")
        
        try:
            issues = []
            
            # 1. æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            imbalance_ratio = self.analysis_results['data_distribution']['imbalance_ratio']
            if imbalance_ratio > 2:
                issues.append({
                    'issue': 'æ•°æ®ä¸¥é‡ä¸å¹³è¡¡',
                    'description': f'æ¶æ„æ ·æœ¬æ˜¯è‰¯æ€§æ ·æœ¬çš„{imbalance_ratio:.1f}å€ï¼Œæ¨¡å‹å¯èƒ½è¿‡åº¦æ‹Ÿåˆæ¶æ„æ ·æœ¬',
                    'severity': 'é«˜',
                    'solution': 'ä½¿ç”¨ç±»åˆ«å¹³è¡¡æŠ€æœ¯ã€é‡é‡‡æ ·ã€æˆ–è°ƒæ•´ç±»åˆ«æƒé‡'
                })
            
            # 2. æ— æ•ˆç‰¹å¾è¿‡å¤š
            zero_features_count = self.analysis_results['data_distribution']['zero_features_count']
            total_features = self.analysis_results['data_distribution']['feature_count']
            if zero_features_count > total_features * 0.3:
                issues.append({
                    'issue': 'æ— æ•ˆç‰¹å¾è¿‡å¤š',
                    'description': f'{zero_features_count}/{total_features} ä¸ªç‰¹å¾å‡ ä¹å…¨ä¸ºé›¶å€¼',
                    'severity': 'ä¸­',
                    'solution': 'ç§»é™¤é›¶å€¼ç‰¹å¾ï¼Œè¿›è¡Œç‰¹å¾é€‰æ‹©'
                })
            
            # 3. ç‰¹å¾åŒºåˆ†åº¦ä¸è¶³
            highly_effective = self.analysis_results['feature_effectiveness']['highly_effective']
            if highly_effective < 10:
                issues.append({
                    'issue': 'é«˜æ•ˆç‰¹å¾ä¸è¶³',
                    'description': f'åªæœ‰{highly_effective}ä¸ªé«˜æ•ˆç‰¹å¾ï¼ŒåŒºåˆ†èƒ½åŠ›ä¸è¶³',
                    'severity': 'é«˜',
                    'solution': 'ç‰¹å¾å·¥ç¨‹ã€ç‰¹å¾ç»„åˆã€æˆ–æ”¶é›†æ›´å¤šæœ‰æ•ˆç‰¹å¾'
                })
            
            # 4. æ¨¡å‹æ€§èƒ½é—®é¢˜
            if 'model_performance' in self.analysis_results:
                best_recall = max([result['recall'] for result in self.analysis_results['model_performance'].values()])
                if best_recall < 0.7:
                    issues.append({
                        'issue': 'æ¨¡å‹æ£€å‡ºç‡è¿‡ä½',
                        'description': f'æœ€ä½³æ¨¡å‹æ£€å‡ºç‡ä»…ä¸º{best_recall:.3f}',
                        'severity': 'é«˜',
                        'solution': 'å°è¯•å…¶ä»–ç®—æ³•ã€è°ƒå‚ã€æˆ–é›†æˆå­¦ä¹ '
                    })
            
            # 5. ç‰¹å¾é‡å åº¦é«˜
            avg_overlap = np.mean([f['overlap_ratio'] for f in self.analysis_results['feature_effectiveness']['top_10_features']])
            if avg_overlap > 0.8:
                issues.append({
                    'issue': 'ç‰¹å¾åˆ†å¸ƒé‡å ä¸¥é‡',
                    'description': f'è‰¯æ€§å’Œæ¶æ„æ ·æœ¬ç‰¹å¾é‡å åº¦è¾¾{avg_overlap:.3f}',
                    'severity': 'é«˜',
                    'solution': 'å¯»æ‰¾æ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ï¼Œæˆ–ä½¿ç”¨éçº¿æ€§æ¨¡å‹'
                })
            
            print(f"âœ… å‘ç° {len(issues)} ä¸ªä¸»è¦é—®é¢˜:")
            for i, issue in enumerate(issues):
                print(f"   {i+1}. {issue['issue']} (ä¸¥é‡ç¨‹åº¦: {issue['severity']})")
                print(f"      {issue['description']}")
                print(f"      å»ºè®®: {issue['solution']}")
            
            self.analysis_results['detection_issues'] = issues
            
            return True
            
        except Exception as e:
            print(f"âŒ é—®é¢˜åˆ†æå¤±è´¥: {e}")
            return False
