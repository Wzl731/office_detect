#!/usr/bin/env python3
"""
æ£€å‡ºç‡ä½åŸå› åˆ†æ
åˆ†æä¸ºä»€ä¹ˆé»‘æ ·æœ¬æ•°é‡å¤šä½†æ£€å‡ºç‡ä½çš„åŸå› 
åŒ…æ‹¬æ•°æ®è´¨é‡ã€ç‰¹å¾æœ‰æ•ˆæ€§ã€æ¨¡å‹æ€§èƒ½ç­‰å¤šç»´åº¦åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import matplotlib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import json

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class DetectionRateAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–æ£€å‡ºç‡åˆ†æ"""
        self.output_dir = Path('featuretrain')
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.data_data = None
        self.feature_names = None
        
        # åˆ†æç»“æœ
        self.analysis_results = {}
        
    def load_and_analyze_data_distribution(self):
        """åŠ è½½å¹¶åˆ†ææ•°æ®åˆ†å¸ƒ"""
        print("ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒå’Œè´¨é‡...")
        
        try:
            # åŠ è½½è®­ç»ƒæ•°æ®
            self.train_data = pd.read_csv('train.csv')
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            try:
                self.data_data = pd.read_csv('data.csv')
                print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
                print(f"   - è®­ç»ƒæ•°æ®: {len(self.train_data)} æ ·æœ¬")
                print(f"   - æµ‹è¯•æ•°æ®: {len(self.data_data)} æ ·æœ¬")
            except:
                print(f"âš ï¸  data.csvæœªæ‰¾åˆ°ï¼Œä»…åˆ†ætrain.csv")
                self.data_data = None
            
            # åˆ†æè®­ç»ƒæ•°æ®åˆ†å¸ƒ
            print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®åˆ†æ:")
            print(f"   - æ€»æ ·æœ¬æ•°: {len(self.train_data)}")
            
            # æ ¹æ®æ–‡ä»¶ååˆ†ææ ‡ç­¾åˆ†å¸ƒ
            # å‡è®¾å‰2939ä¸ªæ˜¯è‰¯æ€§ï¼Œåé¢æ˜¯æ¶æ„
            benign_count = 2939
            malicious_count = len(self.train_data) - benign_count
            
            print(f"   - è‰¯æ€§æ ·æœ¬: {benign_count} ({benign_count/len(self.train_data)*100:.1f}%)")
            print(f"   - æ¶æ„æ ·æœ¬: {malicious_count} ({malicious_count/len(self.train_data)*100:.1f}%)")
            
            # æ£€æŸ¥æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            imbalance_ratio = malicious_count / benign_count
            print(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f} (æ¶æ„/è‰¯æ€§)")
            
            if imbalance_ratio > 2:
                print(f"   âš ï¸  æ•°æ®ä¸¥é‡ä¸å¹³è¡¡ï¼æ¶æ„æ ·æœ¬è¿‡å¤šå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘")
            
            # åˆ†æç‰¹å¾ç»Ÿè®¡
            self.feature_names = self.train_data.columns[1:].tolist()  # æ’é™¤æ–‡ä»¶å
            print(f"   - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            
            # åˆ†æç‰¹å¾å€¼åˆ†å¸ƒ
            feature_data = self.train_data[self.feature_names]
            
            # æ£€æŸ¥é›¶å€¼ç‰¹å¾
            zero_features = []
            low_variance_features = []
            
            for feature in self.feature_names:
                values = feature_data[feature]
                zero_ratio = (values == 0).sum() / len(values)
                variance = values.var()
                
                if zero_ratio > 0.95:
                    zero_features.append((feature, zero_ratio))
                elif variance < 0.01:
                    low_variance_features.append((feature, variance))
            
            print(f"   - é›¶å€¼ç‰¹å¾ (>95%ä¸º0): {len(zero_features)}")
            print(f"   - ä½æ–¹å·®ç‰¹å¾ (<0.01): {len(low_variance_features)}")
            
            # ä¿å­˜åˆ†æç»“æœ
            self.analysis_results['data_distribution'] = {
                'total_samples': len(self.train_data),
                'benign_count': benign_count,
                'malicious_count': malicious_count,
                'imbalance_ratio': imbalance_ratio,
                'feature_count': len(self.feature_names),
                'zero_features_count': len(zero_features),
                'low_variance_features_count': len(low_variance_features),
                'zero_features': zero_features[:10],  # ä¿å­˜å‰10ä¸ª
                'low_variance_features': low_variance_features[:10]
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
            return False
    
    def analyze_feature_effectiveness(self):
        """åˆ†æç‰¹å¾æœ‰æ•ˆæ€§"""
        print("\nğŸ” åˆ†æç‰¹å¾æœ‰æ•ˆæ€§...")
        
        try:
            # å‡†å¤‡æ•°æ®
            feature_data = self.train_data[self.feature_names].values
            
            # åˆ›å»ºæ ‡ç­¾
            benign_count = 2939
            labels = np.concatenate([np.zeros(benign_count), np.ones(len(self.train_data) - benign_count)])
            
            # åˆ†æè‰¯æ€§å’Œæ¶æ„æ ·æœ¬çš„ç‰¹å¾å·®å¼‚
            benign_features = feature_data[:benign_count]
            malicious_features = feature_data[benign_count:]
            
            feature_effectiveness = []
            
            for i, feature_name in enumerate(self.feature_names):
                benign_values = benign_features[:, i]
                malicious_values = malicious_features[:, i]
                
                # è®¡ç®—ç»Ÿè®¡å·®å¼‚
                benign_mean = np.mean(benign_values)
                malicious_mean = np.mean(malicious_values)
                benign_std = np.std(benign_values)
                malicious_std = np.std(malicious_values)
                
                # è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)
                pooled_std = np.sqrt((benign_std**2 + malicious_std**2) / 2)
                cohens_d = abs(malicious_mean - benign_mean) / pooled_std if pooled_std > 0 else 0
                
                # è®¡ç®—é‡å åº¦
                min_max_benign = (np.min(benign_values), np.max(benign_values))
                min_max_malicious = (np.min(malicious_values), np.max(malicious_values))
                
                # è®¡ç®—åˆ†å¸ƒé‡å 
                overlap_start = max(min_max_benign[0], min_max_malicious[0])
                overlap_end = min(min_max_benign[1], min_max_malicious[1])
                overlap_ratio = max(0, overlap_end - overlap_start) / max(min_max_benign[1] - min_max_benign[0], 
                                                                         min_max_malicious[1] - min_max_malicious[0])
                
                # è®¡ç®—éé›¶æ¯”ä¾‹
                benign_nonzero_ratio = np.count_nonzero(benign_values) / len(benign_values)
                malicious_nonzero_ratio = np.count_nonzero(malicious_values) / len(malicious_values)
                
                feature_effectiveness.append({
                    'feature': feature_name,
                    'benign_mean': benign_mean,
                    'malicious_mean': malicious_mean,
                    'mean_difference': abs(malicious_mean - benign_mean),
                    'cohens_d': cohens_d,
                    'overlap_ratio': overlap_ratio,
                    'benign_nonzero_ratio': benign_nonzero_ratio,
                    'malicious_nonzero_ratio': malicious_nonzero_ratio,
                    'effectiveness_score': cohens_d * (1 - overlap_ratio) * max(benign_nonzero_ratio, malicious_nonzero_ratio)
                })
            
            self.feature_effectiveness_df = pd.DataFrame(feature_effectiveness)
            self.feature_effectiveness_df = self.feature_effectiveness_df.sort_values('effectiveness_score', ascending=False)
            
            # ä¿å­˜ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æ
            effectiveness_file = self.output_dir / 'feature_effectiveness_analysis.csv'
            self.feature_effectiveness_df.to_csv(effectiveness_file, index=False, encoding='utf-8')
            
            # ç»Ÿè®¡æœ‰æ•ˆç‰¹å¾
            highly_effective = len(self.feature_effectiveness_df[self.feature_effectiveness_df['effectiveness_score'] > 0.5])
            moderately_effective = len(self.feature_effectiveness_df[
                (self.feature_effectiveness_df['effectiveness_score'] > 0.1) & 
                (self.feature_effectiveness_df['effectiveness_score'] <= 0.5)
            ])
            low_effective = len(self.feature_effectiveness_df[self.feature_effectiveness_df['effectiveness_score'] <= 0.1])
            
            print(f"âœ… ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æå®Œæˆ:")
            print(f"   - é«˜æ•ˆç‰¹å¾ (>0.5): {highly_effective}")
            print(f"   - ä¸­æ•ˆç‰¹å¾ (0.1-0.5): {moderately_effective}")
            print(f"   - ä½æ•ˆç‰¹å¾ (â‰¤0.1): {low_effective}")
            
            # ä¿å­˜åˆ†æç»“æœ
            self.analysis_results['feature_effectiveness'] = {
                'highly_effective': highly_effective,
                'moderately_effective': moderately_effective,
                'low_effective': low_effective,
                'top_10_features': self.feature_effectiveness_df.head(10).to_dict('records')
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾æœ‰æ•ˆæ€§åˆ†æå¤±è´¥: {e}")
            return False
    
    def train_and_evaluate_model(self):
        """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½"""
        print("\nğŸ¤– è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ£€å‡ºç‡...")
        
        try:
            # å‡†å¤‡æ•°æ®
            feature_data = self.train_data[self.feature_names].values
            benign_count = 2939
            labels = np.concatenate([np.zeros(benign_count), np.ones(len(self.train_data) - benign_count)])
            
            # æ•°æ®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                feature_data, labels, test_size=0.2, random_state=42, stratify=labels
            )
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # è®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
            models = {
                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                'RandomForest_Balanced': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
            }
            
            model_results = {}
            
            for model_name, model in models.items():
                print(f"   è®­ç»ƒ {model_name}...")
                
                # è®­ç»ƒæ¨¡å‹
                if 'Balanced' in model_name:
                    model.fit(X_train_scaled, y_train)
                else:
                    model.fit(X_train, y_train)
                
                # é¢„æµ‹
                if 'Balanced' in model_name:
                    y_pred = model.predict(X_test_scaled)
                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                else:
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # è®¡ç®—æŒ‡æ ‡
                from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
                
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred)
                recall = recall_score(y_test, y_pred)  # è¿™å°±æ˜¯æ£€å‡ºç‡
                f1 = f1_score(y_test, y_pred)
                auc = roc_auc_score(y_test, y_pred_proba)
                
                # æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y_test, y_pred)
                tn, fp, fn, tp = cm.ravel()
                
                # è®¡ç®—æ›´å¤šæŒ‡æ ‡
                false_positive_rate = fp / (fp + tn)
                false_negative_rate = fn / (fn + tp)
                
                model_results[model_name] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,  # æ£€å‡ºç‡
                    'f1_score': f1,
                    'auc': auc,
                    'false_positive_rate': false_positive_rate,
                    'false_negative_rate': false_negative_rate,
                    'confusion_matrix': cm.tolist(),
                    'true_positives': int(tp),
                    'false_positives': int(fp),
                    'true_negatives': int(tn),
                    'false_negatives': int(fn)
                }
                
                print(f"     - å‡†ç¡®ç‡: {accuracy:.3f}")
                print(f"     - æ£€å‡ºç‡ (å¬å›ç‡): {recall:.3f}")
                print(f"     - ç²¾ç¡®ç‡: {precision:.3f}")
                print(f"     - F1åˆ†æ•°: {f1:.3f}")
                print(f"     - AUC: {auc:.3f}")
                print(f"     - è¯¯æŠ¥ç‡: {false_positive_rate:.3f}")
                print(f"     - æ¼æŠ¥ç‡: {false_negative_rate:.3f}")
            
            # ä¿å­˜æ¨¡å‹è¯„ä¼°ç»“æœ
            self.analysis_results['model_performance'] = model_results
            
            return True, models, scaler, X_test, y_test
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒè¯„ä¼°å¤±è´¥: {e}")
            return False, None, None, None, None
    
    def analyze_detection_issues(self):
        """åˆ†ææ£€å‡ºç‡ä½çš„å…·ä½“åŸå› """
        print("\nğŸ” åˆ†ææ£€å‡ºç‡ä½çš„å…·ä½“åŸå› ...")
        
        try:
            issues = []
            
            # 1. æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            imbalance_ratio = self.analysis_results['data_distribution']['imbalance_ratio']
            if imbalance_ratio > 2:
                issues.append({
                    'issue': 'æ•°æ®ä¸¥é‡ä¸å¹³è¡¡',
                    'description': f'æ¶æ„æ ·æœ¬æ˜¯è‰¯æ€§æ ·æœ¬çš„{imbalance_ratio:.1f}å€ï¼Œæ¨¡å‹å¯èƒ½è¿‡åº¦æ‹Ÿåˆæ¶æ„æ ·æœ¬',
                    'severity': 'é«˜',
                    'solution': 'ä½¿ç”¨ç±»åˆ«å¹³è¡¡æŠ€æœ¯ã€é‡é‡‡æ ·ã€æˆ–è°ƒæ•´ç±»åˆ«æƒé‡'
                })
            
            # 2. æ— æ•ˆç‰¹å¾è¿‡å¤š
            zero_features_count = self.analysis_results['data_distribution']['zero_features_count']
            total_features = self.analysis_results['data_distribution']['feature_count']
            if zero_features_count > total_features * 0.3:
                issues.append({
                    'issue': 'æ— æ•ˆç‰¹å¾è¿‡å¤š',
                    'description': f'{zero_features_count}/{total_features} ä¸ªç‰¹å¾å‡ ä¹å…¨ä¸ºé›¶å€¼',
                    'severity': 'ä¸­',
                    'solution': 'ç§»é™¤é›¶å€¼ç‰¹å¾ï¼Œè¿›è¡Œç‰¹å¾é€‰æ‹©'
                })
            
            # 3. ç‰¹å¾åŒºåˆ†åº¦ä¸è¶³
            highly_effective = self.analysis_results['feature_effectiveness']['highly_effective']
            if highly_effective < 10:
                issues.append({
                    'issue': 'é«˜æ•ˆç‰¹å¾ä¸è¶³',
                    'description': f'åªæœ‰{highly_effective}ä¸ªé«˜æ•ˆç‰¹å¾ï¼ŒåŒºåˆ†èƒ½åŠ›ä¸è¶³',
                    'severity': 'é«˜',
                    'solution': 'ç‰¹å¾å·¥ç¨‹ã€ç‰¹å¾ç»„åˆã€æˆ–æ”¶é›†æ›´å¤šæœ‰æ•ˆç‰¹å¾'
                })
            
            # 4. æ¨¡å‹æ€§èƒ½é—®é¢˜
            if 'model_performance' in self.analysis_results:
                best_recall = max([result['recall'] for result in self.analysis_results['model_performance'].values()])
                if best_recall < 0.7:
                    issues.append({
                        'issue': 'æ¨¡å‹æ£€å‡ºç‡è¿‡ä½',
                        'description': f'æœ€ä½³æ¨¡å‹æ£€å‡ºç‡ä»…ä¸º{best_recall:.3f}',
                        'severity': 'é«˜',
                        'solution': 'å°è¯•å…¶ä»–ç®—æ³•ã€è°ƒå‚ã€æˆ–é›†æˆå­¦ä¹ '
                    })
            
            # 5. ç‰¹å¾é‡å åº¦é«˜
            avg_overlap = np.mean([f['overlap_ratio'] for f in self.analysis_results['feature_effectiveness']['top_10_features']])
            if avg_overlap > 0.8:
                issues.append({
                    'issue': 'ç‰¹å¾åˆ†å¸ƒé‡å ä¸¥é‡',
                    'description': f'è‰¯æ€§å’Œæ¶æ„æ ·æœ¬ç‰¹å¾é‡å åº¦è¾¾{avg_overlap:.3f}',
                    'severity': 'é«˜',
                    'solution': 'å¯»æ‰¾æ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ï¼Œæˆ–ä½¿ç”¨éçº¿æ€§æ¨¡å‹'
                })
            
            print(f"âœ… å‘ç° {len(issues)} ä¸ªä¸»è¦é—®é¢˜:")
            for i, issue in enumerate(issues):
                print(f"   {i+1}. {issue['issue']} (ä¸¥é‡ç¨‹åº¦: {issue['severity']})")
                print(f"      {issue['description']}")
                print(f"      å»ºè®®: {issue['solution']}")
            
            self.analysis_results['detection_issues'] = issues
            
            return True
            
        except Exception as e:
            print(f"âŒ é—®é¢˜åˆ†æå¤±è´¥: {e}")
            return False
