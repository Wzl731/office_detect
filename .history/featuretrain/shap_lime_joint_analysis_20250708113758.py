#!/usr/bin/env python3
"""
SHAP-LIMEè”åˆåˆ†æä¼˜åŒ–ç­–ç•¥
ç»“åˆSHAPå…¨å±€å¯è§£é‡Šæ€§å’ŒLIMEå±€éƒ¨å¯è§£é‡Šæ€§
ä¸ºç‰¹å¾ä¼˜åŒ–æä¾›æ›´å…¨é¢ã€æ›´å¯é çš„ç­–ç•¥å»ºè®®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import matplotlib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import shap
import lime
import lime.lime_tabular
from scipy import stats
import json

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class SHAPLIMEJointAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–SHAP-LIMEè”åˆåˆ†æç³»ç»Ÿ"""
        self.output_dir = Path('featuretrain')
        self.output_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.feature_names = None
        
        # æ¨¡å‹å’Œè§£é‡Šå™¨
        self.model = None
        self.shap_explainer = None
        self.lime_explainer = None
        
        # åˆ†æç»“æœ
        self.shap_values = None
        self.lime_explanations = []
        self.joint_insights = {}
        
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        print("ğŸ“Š åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        try:
            # åŠ è½½train.csvæ•°æ®
            self.train_data = pd.read_csv('train.csv')
            
            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            self.feature_names = self.train_data.columns[1:].tolist()  # æ’é™¤æ–‡ä»¶ååˆ—
            X = self.train_data[self.feature_names].values
            
            # åˆ›å»ºæ ‡ç­¾ (å‰2939ä¸ªæ˜¯è‰¯æ€§=0ï¼Œåé¢æ˜¯æ¶æ„=1)
            y = np.concatenate([np.zeros(2939), np.ones(len(self.train_data) - 2939)])
            
            # æ•°æ®åˆ†å‰²
            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            self.X_train = scaler.fit_transform(self.X_train)
            self.X_test = scaler.transform(self.X_test)
            
            print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
            print(f"   - è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
            print(f"   - æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
            print(f"   - ç‰¹å¾æ•°: {len(self.feature_names)}")
            print(f"   - è‰¯æ€§æ ·æœ¬: {int(np.sum(self.y_train == 0))} / æ¶æ„æ ·æœ¬: {int(np.sum(self.y_train == 1))}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return False
    
    def train_model(self):
        """è®­ç»ƒåŸºç¡€æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        try:
            # ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºåŸºç¡€æ¨¡å‹
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            
            self.model.fit(self.X_train, self.y_train)
            
            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            train_score = self.model.score(self.X_train, self.y_train)
            test_score = self.model.score(self.X_test, self.y_test)
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
            print(f"   - è®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
            print(f"   - æµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def perform_shap_analysis(self, sample_size=1000):
        """æ‰§è¡ŒSHAPåˆ†æ"""
        print(f"\nğŸ” æ‰§è¡ŒSHAPå…¨å±€å¯è§£é‡Šæ€§åˆ†æ (æ ·æœ¬æ•°: {sample_size})...")
        
        try:
            # åˆ›å»ºSHAPè§£é‡Šå™¨
            self.shap_explainer = shap.TreeExplainer(self.model)
            
            # é€‰æ‹©æ ·æœ¬è¿›è¡ŒSHAPåˆ†æï¼ˆä¸ºäº†è®¡ç®—æ•ˆç‡ï¼‰
            sample_indices = np.random.choice(len(self.X_test), min(sample_size, len(self.X_test)), replace=False)
            X_sample = self.X_test[sample_indices]
            
            # è®¡ç®—SHAPå€¼
            self.shap_values = self.shap_explainer.shap_values(X_sample)
            
            # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œå–æ­£ç±»çš„SHAPå€¼
            if isinstance(self.shap_values, list):
                self.shap_values = self.shap_values[1]  # æ¶æ„ç±»çš„SHAPå€¼
            
            print(f"âœ… SHAPåˆ†æå®Œæˆ:")
            print(f"   - åˆ†ææ ·æœ¬æ•°: {len(X_sample)}")
            print(f"   - SHAPå€¼å½¢çŠ¶: {self.shap_values.shape}")
            
            return X_sample, sample_indices
            
        except Exception as e:
            print(f"âŒ SHAPåˆ†æå¤±è´¥: {e}")
            return None, None
    
    def perform_lime_analysis(self, sample_indices, num_samples=100):
        """æ‰§è¡ŒLIMEåˆ†æ"""
        print(f"\nğŸ” æ‰§è¡ŒLIMEå±€éƒ¨å¯è§£é‡Šæ€§åˆ†æ (æ ·æœ¬æ•°: {num_samples})...")
        
        try:
            # åˆ›å»ºLIMEè§£é‡Šå™¨
            self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
                self.X_train,
                feature_names=self.feature_names,
                class_names=['Benign', 'Malicious'],
                mode='classification',
                discretize_continuous=True
            )
            
            # é€‰æ‹©æ ·æœ¬è¿›è¡ŒLIMEåˆ†æ
            lime_sample_indices = np.random.choice(sample_indices, min(num_samples, len(sample_indices)), replace=False)
            
            self.lime_explanations = []
            
            for i, idx in enumerate(lime_sample_indices):
                if i % 20 == 0:
                    print(f"    å¤„ç†LIMEæ ·æœ¬: {i+1}/{len(lime_sample_indices)}")
                
                # è·å–LIMEè§£é‡Š
                explanation = self.lime_explainer.explain_instance(
                    self.X_test[idx],
                    self.model.predict_proba,
                    num_features=len(self.feature_names),
                    top_labels=1
                )
                
                # æå–ç‰¹å¾é‡è¦æ€§
                lime_weights = {}
                for feature_idx, weight in explanation.as_list():
                    # è§£æç‰¹å¾åç§°ï¼ˆLIMEè¿”å›çš„æ˜¯ç‰¹å¾åç§°å­—ç¬¦ä¸²ï¼‰
                    feature_name = feature_idx.split('<=')[0].split('>')[0].strip()
                    if feature_name in self.feature_names:
                        lime_weights[feature_name] = weight
                    else:
                        # å¦‚æœæ— æ³•åŒ¹é…ï¼Œä½¿ç”¨ç´¢å¼•
                        try:
                            feat_idx = int(feature_idx.split('_')[-1]) if '_' in feature_idx else 0
                            if feat_idx < len(self.feature_names):
                                lime_weights[self.feature_names[feat_idx]] = weight
                        except:
                            continue
                
                self.lime_explanations.append({
                    'sample_idx': idx,
                    'prediction': self.model.predict_proba([self.X_test[idx]])[0],
                    'true_label': self.y_test[idx],
                    'lime_weights': lime_weights
                })
            
            print(f"âœ… LIMEåˆ†æå®Œæˆ:")
            print(f"   - åˆ†ææ ·æœ¬æ•°: {len(self.lime_explanations)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ LIMEåˆ†æå¤±è´¥: {e}")
            return False
    
    def analyze_shap_lime_consistency(self):
        """åˆ†æSHAPå’ŒLIMEçš„ä¸€è‡´æ€§"""
        print("\nğŸ”„ åˆ†æSHAP-LIMEä¸€è‡´æ€§...")
        
        try:
            # è®¡ç®—SHAPå…¨å±€é‡è¦æ€§
            shap_importance = np.abs(self.shap_values).mean(axis=0)
            shap_feature_importance = dict(zip(self.feature_names, shap_importance))
            
            # è®¡ç®—LIMEå¹³å‡é‡è¦æ€§
            lime_feature_importance = {}
            for feature in self.feature_names:
                weights = []
                for exp in self.lime_explanations:
                    if feature in exp['lime_weights']:
                        weights.append(abs(exp['lime_weights'][feature]))
                lime_feature_importance[feature] = np.mean(weights) if weights else 0
            
            # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
            consistency_analysis = []
            
            for feature in self.feature_names:
                shap_imp = shap_feature_importance.get(feature, 0)
                lime_imp = lime_feature_importance.get(feature, 0)
                
                # å½’ä¸€åŒ–é‡è¦æ€§åˆ†æ•°
                shap_norm = shap_imp / max(shap_feature_importance.values()) if max(shap_feature_importance.values()) > 0 else 0
                lime_norm = lime_imp / max(lime_feature_importance.values()) if max(lime_feature_importance.values()) > 0 else 0
                
                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
                consistency_score = 1 - abs(shap_norm - lime_norm)
                
                # è®¡ç®—é‡è¦æ€§ç­‰çº§
                shap_rank = sorted(shap_feature_importance.values(), reverse=True).index(shap_imp) + 1
                lime_rank = sorted(lime_feature_importance.values(), reverse=True).index(lime_imp) + 1
                rank_diff = abs(shap_rank - lime_rank)
                
                consistency_analysis.append({
                    'feature': feature,
                    'shap_importance': shap_imp,
                    'lime_importance': lime_imp,
                    'shap_normalized': shap_norm,
                    'lime_normalized': lime_norm,
                    'consistency_score': consistency_score,
                    'shap_rank': shap_rank,
                    'lime_rank': lime_rank,
                    'rank_difference': rank_diff,
                    'agreement_level': self.classify_agreement(consistency_score, rank_diff)
                })
            
            self.consistency_df = pd.DataFrame(consistency_analysis)
            self.consistency_df = self.consistency_df.sort_values('consistency_score', ascending=False)
            
            # ä¿å­˜ä¸€è‡´æ€§åˆ†æç»“æœ
            consistency_file = self.output_dir / 'shap_lime_consistency_analysis.csv'
            self.consistency_df.to_csv(consistency_file, index=False, encoding='utf-8')
            
            print(f"âœ… ä¸€è‡´æ€§åˆ†æå®Œæˆ: {consistency_file}")
            print(f"   - é«˜ä¸€è‡´æ€§ç‰¹å¾ (>0.8): {len(self.consistency_df[self.consistency_df['consistency_score'] > 0.8])}")
            print(f"   - ä¸­ç­‰ä¸€è‡´æ€§ç‰¹å¾ (0.6-0.8): {len(self.consistency_df[(self.consistency_df['consistency_score'] > 0.6) & (self.consistency_df['consistency_score'] <= 0.8)])}")
            print(f"   - ä½ä¸€è‡´æ€§ç‰¹å¾ (<0.6): {len(self.consistency_df[self.consistency_df['consistency_score'] <= 0.6])}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¸€è‡´æ€§åˆ†æå¤±è´¥: {e}")
            return False
    
    def classify_agreement(self, consistency_score, rank_diff):
        """åˆ†ç±»ä¸€è‡´æ€§ç­‰çº§"""
        if consistency_score > 0.8 and rank_diff < 10:
            return "é«˜åº¦ä¸€è‡´"
        elif consistency_score > 0.6 and rank_diff < 20:
            return "ä¸­ç­‰ä¸€è‡´"
        elif consistency_score > 0.4:
            return "éƒ¨åˆ†ä¸€è‡´"
        else:
            return "ä¸ä¸€è‡´"
    
    def generate_joint_optimization_strategy(self):
        """ç”ŸæˆSHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥"""
        print("\nğŸ’¡ ç”ŸæˆSHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥...")
        
        try:
            # åŠ è½½ä¹‹å‰çš„ä¸‰ç»´è¯„ä¼°ç»“æœ
            three_dim_scores = pd.read_csv('featuretrain/three_dimensional_feature_scores.csv')
            
            # åˆå¹¶åˆ†æç»“æœ
            joint_analysis = []
            
            for _, row in self.consistency_df.iterrows():
                feature = row['feature']
                
                # è·å–ä¸‰ç»´è¯„åˆ†
                three_dim_row = three_dim_scores[three_dim_scores['feature'] == feature]
                if not three_dim_row.empty:
                    stability = three_dim_row['stability_score'].iloc[0]
                    discriminative = three_dim_row['discriminative_power'].iloc[0]
                    interpretability = three_dim_row['interpretability_score'].iloc[0]
                    overall_score = three_dim_row['overall_score'].iloc[0]
                else:
                    stability = discriminative = interpretability = overall_score = 0
                
                # è®¡ç®—è”åˆå¯ä¿¡åº¦åˆ†æ•°
                # ç»“åˆSHAP-LIMEä¸€è‡´æ€§å’Œä¸‰ç»´è¯„åˆ†
                joint_reliability = (
                    row['consistency_score'] * 0.4 +  # SHAP-LIMEä¸€è‡´æ€§
                    interpretability * 0.3 +          # åŸæœ‰å¯è§£é‡Šæ€§
                    stability * 0.2 +                 # ç¨³å®šæ€§
                    discriminative * 0.1               # åˆ¤åˆ«åŠ›
                )
                
                # ç”Ÿæˆä¼˜åŒ–ç­–ç•¥
                strategy = self.determine_joint_strategy(
                    row['consistency_score'],
                    row['shap_importance'],
                    row['lime_importance'],
                    stability,
                    discriminative,
                    interpretability,
                    joint_reliability
                )
                
                joint_analysis.append({
                    'feature': feature,
                    'shap_importance': row['shap_importance'],
                    'lime_importance': row['lime_importance'],
                    'consistency_score': row['consistency_score'],
                    'agreement_level': row['agreement_level'],
                    'stability_score': stability,
                    'discriminative_power': discriminative,
                    'interpretability_score': interpretability,
                    'joint_reliability': joint_reliability,
                    'optimization_strategy': strategy['action'],
                    'strategy_reason': strategy['reason'],
                    'priority_level': strategy['priority'],
                    'confidence_level': strategy['confidence']
                })
            
            self.joint_strategy_df = pd.DataFrame(joint_analysis)
            self.joint_strategy_df = self.joint_strategy_df.sort_values('joint_reliability', ascending=False)
            
            # ä¿å­˜è”åˆç­–ç•¥
            strategy_file = self.output_dir / 'shap_lime_joint_optimization_strategy.csv'
            self.joint_strategy_df.to_csv(strategy_file, index=False, encoding='utf-8')
            
            print(f"âœ… è”åˆä¼˜åŒ–ç­–ç•¥ç”Ÿæˆå®Œæˆ: {strategy_file}")
            
            # ç»Ÿè®¡ç­–ç•¥åˆ†å¸ƒ
            strategy_counts = self.joint_strategy_df['optimization_strategy'].value_counts()
            print(f"ğŸ“Š ç­–ç•¥åˆ†å¸ƒ:")
            for strategy, count in strategy_counts.items():
                print(f"   - {strategy}: {count} ä¸ªç‰¹å¾")
            
            return True
            
        except Exception as e:
            print(f"âŒ è”åˆç­–ç•¥ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def determine_joint_strategy(self, consistency, shap_imp, lime_imp, stability, discriminative, interpretability, joint_reliability):
        """ç¡®å®šè”åˆä¼˜åŒ–ç­–ç•¥"""
        
        # é«˜å¯ä¿¡åº¦ç‰¹å¾
        if joint_reliability > 0.8 and consistency > 0.7:
            if discriminative > 0.6:
                return {
                    'action': 'æ ¸å¿ƒä¿ç•™',
                    'reason': 'SHAP-LIMEé«˜åº¦ä¸€è‡´ä¸”æ€§èƒ½ä¼˜ç§€',
                    'priority': 'é«˜',
                    'confidence': 'å¾ˆé«˜'
                }
            else:
                return {
                    'action': 'ç¨³å®šä¿ç•™',
                    'reason': 'SHAP-LIMEä¸€è‡´ä½†åˆ¤åˆ«åŠ›ä¸€èˆ¬',
                    'priority': 'ä¸­',
                    'confidence': 'é«˜'
                }
        
        # ä¸ä¸€è‡´ä½†é‡è¦çš„ç‰¹å¾
        elif consistency < 0.5 and (shap_imp > 0.1 or lime_imp > 0.1):
            return {
                'action': 'æ·±åº¦åˆ†æ',
                'reason': 'SHAP-LIMEä¸ä¸€è‡´ä½†æ˜¾ç¤ºé‡è¦æ€§ï¼Œéœ€è¦æ·±å…¥è°ƒæŸ¥',
                'priority': 'é«˜',
                'confidence': 'ä¸­'
            }
        
        # ä¸€è‡´æ€§å¥½ä½†æ€§èƒ½å·®çš„ç‰¹å¾
        elif consistency > 0.7 and discriminative < 0.3:
            return {
                'action': 'ç‰¹å¾å·¥ç¨‹',
                'reason': 'SHAP-LIMEä¸€è‡´ä½†åˆ¤åˆ«åŠ›å¼±ï¼Œé€‚åˆç»„åˆæˆ–å˜æ¢',
                'priority': 'ä¸­',
                'confidence': 'é«˜'
            }
        
        # ç¨³å®šæ€§å·®çš„ç‰¹å¾
        elif stability < 0.5:
            return {
                'action': 'ç›‘æ§æˆ–ç§»é™¤',
                'reason': 'ç¨³å®šæ€§å·®ï¼Œå­˜åœ¨æ¼‚ç§»é£é™©',
                'priority': 'é«˜',
                'confidence': 'é«˜'
            }
        
        # ä½é‡è¦æ€§ç‰¹å¾
        elif shap_imp < 0.01 and lime_imp < 0.01:
            return {
                'action': 'è€ƒè™‘ç§»é™¤',
                'reason': 'SHAP-LIMEå‡æ˜¾ç¤ºä½é‡è¦æ€§',
                'priority': 'ä½',
                'confidence': 'é«˜'
            }
        
        # ä¸­ç­‰è¡¨ç°ç‰¹å¾
        else:
            return {
                'action': 'ä¿æŒè§‚å¯Ÿ',
                'reason': 'è¡¨ç°ä¸­ç­‰ï¼Œç»§ç»­è§‚å¯Ÿ',
                'priority': 'ä½',
                'confidence': 'ä¸­'
            }
    
    def create_joint_visualizations(self):
        """åˆ›å»ºSHAP-LIMEè”åˆå¯è§†åŒ–"""
        print("\nğŸ“ˆ ç”ŸæˆSHAP-LIMEè”åˆå¯è§†åŒ–...")
        
        try:
            fig, axes = plt.subplots(2, 3, figsize=(20, 12))
            
            # 1. SHAP vs LIMEé‡è¦æ€§æ•£ç‚¹å›¾
            ax1 = axes[0, 0]
            scatter = ax1.scatter(self.consistency_df['shap_normalized'], 
                                 self.consistency_df['lime_normalized'],
                                 c=self.consistency_df['consistency_score'],
                                 s=60, alpha=0.7, cmap='RdYlGn')
            ax1.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='å®Œå…¨ä¸€è‡´çº¿')
            ax1.set_xlabel('SHAPé‡è¦æ€§ (å½’ä¸€åŒ–)')
            ax1.set_ylabel('LIMEé‡è¦æ€§ (å½’ä¸€åŒ–)')
            ax1.set_title('SHAP vs LIMEé‡è¦æ€§å¯¹æ¯”')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            plt.colorbar(scatter, ax=ax1, label='ä¸€è‡´æ€§åˆ†æ•°')
            
            # 2. ä¸€è‡´æ€§åˆ†æ•°åˆ†å¸ƒ
            ax2 = axes[0, 1]
            ax2.hist(self.consistency_df['consistency_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            ax2.set_xlabel('ä¸€è‡´æ€§åˆ†æ•°')
            ax2.set_ylabel('ç‰¹å¾æ•°é‡')
            ax2.set_title('SHAP-LIMEä¸€è‡´æ€§åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)
            
            # 3. ä¸€è‡´æ€§ç­‰çº§é¥¼å›¾
            ax3 = axes[0, 2]
            agreement_counts = self.consistency_df['agreement_level'].value_counts()
            colors = ['green', 'yellow', 'orange', 'red']
            ax3.pie(agreement_counts.values, labels=agreement_counts.index, 
                   autopct='%1.1f%%', colors=colors[:len(agreement_counts)], startangle=90)
            ax3.set_title('SHAP-LIMEä¸€è‡´æ€§ç­‰çº§åˆ†å¸ƒ')
            
            # 4. è”åˆå¯ä¿¡åº¦vsä¸€è‡´æ€§
            ax4 = axes[1, 0]
            ax4.scatter(self.joint_strategy_df['consistency_score'],
                       self.joint_strategy_df['joint_reliability'],
                       alpha=0.6, s=50)
            ax4.set_xlabel('SHAP-LIMEä¸€è‡´æ€§')
            ax4.set_ylabel('è”åˆå¯ä¿¡åº¦åˆ†æ•°')
            ax4.set_title('ä¸€è‡´æ€§ vs è”åˆå¯ä¿¡åº¦')
            ax4.grid(True, alpha=0.3)
            
            # 5. ä¼˜åŒ–ç­–ç•¥åˆ†å¸ƒ
            ax5 = axes[1, 1]
            strategy_counts = self.joint_strategy_df['optimization_strategy'].value_counts()
            bars = ax5.bar(range(len(strategy_counts)), strategy_counts.values, alpha=0.7)
            ax5.set_xticks(range(len(strategy_counts)))
            ax5.set_xticklabels(strategy_counts.index, rotation=45, ha='right')
            ax5.set_ylabel('ç‰¹å¾æ•°é‡')
            ax5.set_title('SHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥åˆ†å¸ƒ')
            ax5.grid(True, alpha=0.3)
            
            # ä¸ºä¸åŒç­–ç•¥è®¾ç½®ä¸åŒé¢œè‰²
            colors = ['green', 'blue', 'orange', 'red', 'purple', 'brown']
            for i, bar in enumerate(bars):
                bar.set_color(colors[i % len(colors)])
            
            # 6. å‰15ä¸ªé«˜å¯ä¿¡åº¦ç‰¹å¾
            ax6 = axes[1, 2]
            top_15 = self.joint_strategy_df.head(15)
            y_pos = range(len(top_15))
            
            bars = ax6.barh(y_pos, top_15['joint_reliability'], alpha=0.7)
            ax6.set_yticks(y_pos)
            ax6.set_yticklabels(top_15['feature'], fontsize=9)
            ax6.set_xlabel('è”åˆå¯ä¿¡åº¦åˆ†æ•°')
            ax6.set_title('å‰15ä¸ªé«˜å¯ä¿¡åº¦ç‰¹å¾')
            ax6.grid(True, alpha=0.3)
            ax6.invert_yaxis()
            
            # æ ¹æ®ç­–ç•¥è®¾ç½®é¢œè‰²
            strategy_colors = {'æ ¸å¿ƒä¿ç•™': 'green', 'ç¨³å®šä¿ç•™': 'blue', 'æ·±åº¦åˆ†æ': 'orange', 
                             'ç‰¹å¾å·¥ç¨‹': 'yellow', 'ç›‘æ§æˆ–ç§»é™¤': 'red', 'è€ƒè™‘ç§»é™¤': 'darkred',
                             'ä¿æŒè§‚å¯Ÿ': 'gray'}
            for i, (_, row) in enumerate(top_15.iterrows()):
                bars[i].set_color(strategy_colors.get(row['optimization_strategy'], 'gray'))
            
            plt.tight_layout()
            
            # ä¿å­˜å¯è§†åŒ–
            viz_file = self.output_dir / 'shap_lime_joint_analysis_visualization.png'
            plt.savefig(viz_file, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print(f"âœ… è”åˆå¯è§†åŒ–å·²ä¿å­˜: {viz_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            return False

    def create_shap_summary_plots(self):
        """åˆ›å»ºSHAPæ‘˜è¦å›¾"""
        print("\nğŸ“Š ç”ŸæˆSHAPæ‘˜è¦å¯è§†åŒ–...")

        try:
            fig, axes = plt.subplots(1, 2, figsize=(16, 8))

            # 1. SHAPç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
            shap_importance = np.abs(self.shap_values).mean(axis=0)
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': shap_importance
            }).sort_values('importance', ascending=False)

            top_20 = feature_importance_df.head(20)

            ax1 = axes[0]
            bars = ax1.barh(range(len(top_20)), top_20['importance'], alpha=0.7, color='skyblue')
            ax1.set_yticks(range(len(top_20)))
            ax1.set_yticklabels(top_20['feature'], fontsize=10)
            ax1.set_xlabel('SHAPé‡è¦æ€§åˆ†æ•°')
            ax1.set_title('SHAPç‰¹å¾é‡è¦æ€§æ’å (Top 20)')
            ax1.grid(True, alpha=0.3)
            ax1.invert_yaxis()

            # 2. SHAPå€¼åˆ†å¸ƒå°æç´å›¾
            ax2 = axes[1]

            # é€‰æ‹©å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾è¿›è¡Œå¯è§†åŒ–
            top_10_features = top_20.head(10)['feature'].tolist()
            top_10_indices = [self.feature_names.index(f) for f in top_10_features]

            shap_data_for_violin = []
            labels_for_violin = []

            for i, feature_idx in enumerate(top_10_indices):
                shap_data_for_violin.append(self.shap_values[:, feature_idx])
                labels_for_violin.append(top_10_features[i][:15])  # æˆªæ–­é•¿ç‰¹å¾å

            parts = ax2.violinplot(shap_data_for_violin, positions=range(len(shap_data_for_violin)),
                                  showmeans=True, showmedians=True)

            ax2.set_xticks(range(len(labels_for_violin)))
            ax2.set_xticklabels(labels_for_violin, rotation=45, ha='right')
            ax2.set_ylabel('SHAPå€¼åˆ†å¸ƒ')
            ax2.set_title('å‰10ä¸ªç‰¹å¾çš„SHAPå€¼åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)
            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)

            plt.tight_layout()

            # ä¿å­˜SHAPæ‘˜è¦å›¾
            shap_file = self.output_dir / 'shap_summary_plots.png'
            plt.savefig(shap_file, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()

            print(f"âœ… SHAPæ‘˜è¦å›¾å·²ä¿å­˜: {shap_file}")

            return True

        except Exception as e:
            print(f"âŒ SHAPæ‘˜è¦å›¾ç”Ÿæˆå¤±è´¥: {e}")
            return False

    def generate_comprehensive_report(self):
        """ç”ŸæˆSHAP-LIMEè”åˆåˆ†æç»¼åˆæŠ¥å‘Š"""
        print("\nğŸ“„ ç”ŸæˆSHAP-LIMEè”åˆåˆ†ææŠ¥å‘Š...")

        try:
            report_file = self.output_dir / 'shap_lime_joint_analysis_report.md'

            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# SHAP-LIMEè”åˆåˆ†æä¼˜åŒ–ç­–ç•¥æŠ¥å‘Š\n\n")

                f.write("## ğŸ¯ åˆ†æç›®æ ‡\n\n")
                f.write("æœ¬æŠ¥å‘Šç»“åˆSHAPå…¨å±€å¯è§£é‡Šæ€§å’ŒLIMEå±€éƒ¨å¯è§£é‡Šæ€§åˆ†æï¼Œ")
                f.write("é€šè¿‡ä¸¤ç§æ–¹æ³•çš„ä¸€è‡´æ€§éªŒè¯ï¼Œä¸ºç‰¹å¾ä¼˜åŒ–æä¾›æ›´å¯é çš„ç­–ç•¥å»ºè®®ã€‚\n\n")

                f.write("## ğŸ“Š æ–¹æ³•è®º\n\n")
                f.write("### SHAP (SHapley Additive exPlanations)\n")
                f.write("- **ä¼˜åŠ¿**: æä¾›å…¨å±€ä¸€è‡´çš„ç‰¹å¾é‡è¦æ€§ï¼Œç†è®ºåŸºç¡€æ‰å®\n")
                f.write("- **åº”ç”¨**: åˆ†æç‰¹å¾å¯¹æ¨¡å‹æ•´ä½“å†³ç­–çš„è´¡çŒ®\n")
                f.write("- **æ ·æœ¬æ•°**: 1000ä¸ªæµ‹è¯•æ ·æœ¬\n\n")

                f.write("### LIME (Local Interpretable Model-agnostic Explanations)\n")
                f.write("- **ä¼˜åŠ¿**: æä¾›å±€éƒ¨å¯è§£é‡Šæ€§ï¼Œå…³æ³¨ä¸ªä½“æ ·æœ¬çš„å†³ç­–è¿‡ç¨‹\n")
                f.write("- **åº”ç”¨**: éªŒè¯SHAPç»“æœçš„å±€éƒ¨ä¸€è‡´æ€§\n")
                f.write("- **æ ·æœ¬æ•°**: 100ä¸ªä»£è¡¨æ€§æ ·æœ¬\n\n")

                f.write("### è”åˆåˆ†æç­–ç•¥\n")
                f.write("- **ä¸€è‡´æ€§éªŒè¯**: æ¯”è¾ƒSHAPå’ŒLIMEçš„ç‰¹å¾é‡è¦æ€§æ’å\n")
                f.write("- **å¯ä¿¡åº¦è¯„ä¼°**: ç»“åˆä¸€è‡´æ€§å’Œä¸‰ç»´ç‰¹å¾è¯„åˆ†\n")
                f.write("- **ç­–ç•¥ç”Ÿæˆ**: åŸºäºè”åˆåˆ†æç»“æœåˆ¶å®šä¼˜åŒ–ç­–ç•¥\n\n")

                # åˆ†æç»“æœç»Ÿè®¡
                f.write("## ğŸ“ˆ åˆ†æç»“æœç»Ÿè®¡\n\n")
                f.write(f"- **æ€»ç‰¹å¾æ•°**: {len(self.consistency_df)}\n")

                high_consistency = len(self.consistency_df[self.consistency_df['consistency_score'] > 0.8])
                medium_consistency = len(self.consistency_df[(self.consistency_df['consistency_score'] > 0.6) &
                                                           (self.consistency_df['consistency_score'] <= 0.8)])
                low_consistency = len(self.consistency_df[self.consistency_df['consistency_score'] <= 0.6])

                f.write(f"- **é«˜ä¸€è‡´æ€§ç‰¹å¾** (>0.8): {high_consistency} ({high_consistency/len(self.consistency_df)*100:.1f}%)\n")
                f.write(f"- **ä¸­ç­‰ä¸€è‡´æ€§ç‰¹å¾** (0.6-0.8): {medium_consistency} ({medium_consistency/len(self.consistency_df)*100:.1f}%)\n")
                f.write(f"- **ä½ä¸€è‡´æ€§ç‰¹å¾** (<0.6): {low_consistency} ({low_consistency/len(self.consistency_df)*100:.1f}%)\n\n")

                # ä¸€è‡´æ€§æœ€é«˜çš„ç‰¹å¾
                f.write("## ğŸ† SHAP-LIMEé«˜åº¦ä¸€è‡´ç‰¹å¾ (Top 10)\n\n")
                top_consistent = self.consistency_df.head(10)
                for i, (_, row) in enumerate(top_consistent.iterrows()):
                    f.write(f"{i+1}. **`{row['feature']}`** (ä¸€è‡´æ€§: {row['consistency_score']:.3f})\n")
                    f.write(f"   - SHAPé‡è¦æ€§: {row['shap_importance']:.4f} (æ’å: {row['shap_rank']})\n")
                    f.write(f"   - LIMEé‡è¦æ€§: {row['lime_importance']:.4f} (æ’å: {row['lime_rank']})\n")
                    f.write(f"   - ä¸€è‡´æ€§ç­‰çº§: {row['agreement_level']}\n\n")

                # è”åˆä¼˜åŒ–ç­–ç•¥
                f.write("## ğŸ’¡ SHAP-LIMEè”åˆä¼˜åŒ–ç­–ç•¥\n\n")

                strategy_counts = self.joint_strategy_df['optimization_strategy'].value_counts()

                for strategy, count in strategy_counts.items():
                    f.write(f"### {strategy} ({count}ä¸ªç‰¹å¾)\n\n")

                    strategy_features = self.joint_strategy_df[
                        self.joint_strategy_df['optimization_strategy'] == strategy
                    ].head(5)  # æ˜¾ç¤ºå‰5ä¸ª

                    for _, row in strategy_features.iterrows():
                        f.write(f"- **`{row['feature']}`**: {row['strategy_reason']}\n")
                        f.write(f"  - è”åˆå¯ä¿¡åº¦: {row['joint_reliability']:.3f}\n")
                        f.write(f"  - ä¸€è‡´æ€§: {row['consistency_score']:.3f}\n\n")

                f.write("## ğŸ“ ç”Ÿæˆæ–‡ä»¶\n\n")
                f.write("- `shap_lime_consistency_analysis.csv`: SHAP-LIMEä¸€è‡´æ€§è¯¦ç»†åˆ†æ\n")
                f.write("- `shap_lime_joint_optimization_strategy.csv`: è”åˆä¼˜åŒ–ç­–ç•¥\n")
                f.write("- `shap_lime_joint_analysis_visualization.png`: è”åˆåˆ†æå¯è§†åŒ–\n")
                f.write("- `shap_summary_plots.png`: SHAPæ‘˜è¦å›¾\n")
                f.write("- `shap_lime_joint_analysis_report.md`: æœ¬æŠ¥å‘Š\n")

            print(f"âœ… SHAP-LIMEè”åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

            return True

        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return False

    def run_joint_analysis(self):
        """è¿è¡Œå®Œæ•´çš„SHAP-LIMEè”åˆåˆ†æ"""
        print("ğŸ¯ SHAP-LIMEè”åˆåˆ†æä¼˜åŒ–ç­–ç•¥")
        print("=" * 70)
        print("ç»“åˆå…¨å±€å¯è§£é‡Šæ€§(SHAP)å’Œå±€éƒ¨å¯è§£é‡Šæ€§(LIME)")
        print("=" * 70)

        # 1. æ•°æ®å‡†å¤‡
        if not self.load_and_prepare_data():
            return False

        # 2. æ¨¡å‹è®­ç»ƒ
        if not self.train_model():
            return False

        # 3. SHAPåˆ†æ
        X_sample, sample_indices = self.perform_shap_analysis()
        if X_sample is None:
            return False

        # 4. LIMEåˆ†æ
        if not self.perform_lime_analysis(sample_indices):
            return False

        # 5. ä¸€è‡´æ€§åˆ†æ
        if not self.analyze_shap_lime_consistency():
            return False

        # 6. è”åˆä¼˜åŒ–ç­–ç•¥
        if not self.generate_joint_optimization_strategy():
            return False

        # 7. å¯è§†åŒ–
        if not self.create_joint_visualizations():
            return False

        # 8. SHAPæ‘˜è¦å›¾
        if not self.create_shap_summary_plots():
            return False

        # 9. ç»¼åˆæŠ¥å‘Š
        if not self.generate_comprehensive_report():
            return False

        print(f"\nğŸ‰ SHAP-LIMEè”åˆåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° featuretrain/ æ–‡ä»¶å¤¹")
        print(f"ğŸ” è”åˆåˆ†ææä¾›äº†æ›´å¯é çš„ç‰¹å¾ä¼˜åŒ–ç­–ç•¥")

        return True

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SHAPLIMEJointAnalysis()
    success = analyzer.run_joint_analysis()

    if success:
        print("\nâœ… SHAP-LIMEè”åˆåˆ†æå®Œæˆï¼")
        print("ğŸ’¡ è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šå’Œå¯è§†åŒ–æ–‡ä»¶è·å–è¯¦ç»†çš„ä¼˜åŒ–å»ºè®®")
    else:
        print("\nâŒ SHAP-LIMEè”åˆåˆ†æå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
